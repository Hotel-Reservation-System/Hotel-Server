1. DATABASE: CONCEPTS

This file will discuss database concepts that you need to set up and work with databases.
The next document, '2.4 Databases - Practical Applications', will discuss the 
installation and configuration of a database, followed by a section on how to connect a 
program to a database.

In this file, Section 2 will explain what databases are, and list some key things that 
you need to know about them. Section 3 is concerned with program-database interfacing: 
How does a program talk to a database? What role does an ORM library play in facilitating
this communication? 


    2.    Introduction to Database Concepts and Terms
    2.1   What is a Database?
    2.2   Relational Database Management Systems (RDBMS)
    2.3   Other Database Concepts and Terms to Learn Before Proceeding to Section 3
    
    3.    Application-To-Database Communication 
    3.1   Entity Framework Core: An Object-Relational Mapper (ORM)
    3.2   How Does Entity Framework Core Translate Data To and From the Database?
    3.3   Entity Framework Core Tooling: Nuget Packages and CLI Tools
    3.4   Entity Framework Core: Concepts and Components
    3.4.1 Database Providers
    3.4.2 The Program-side Representation of Database Entities: The Data Model vs. The Schema
    3.4.3 Migrations
    

****************************************************************************************************
****************************************************************************************************


2. INTRODUCTION TO DATABASE CONCEPTS AND TERMS

2.1 WHAT IS A DATABASE?

A database is an software system that stores data. A snippet I found on the internet 
describes databases like this: "A database is a computerised system that makes it easy to 
search, select and store information." Databases are used in institutions and businesses 
to store organizational data like customer records and profiles, sales transactions, 
product information etc. Computers, web browsers and phones have many built-in databases
for tracking information. However, storing data is not worthwhile unless the database is 
secure and well-organized, yet easily accessible to those who have authorization.

This is why a database program is rarely marketed as a standalone piece of software; 
instead most databases come with an ecosystem of additional tools. This collection of 
database and tooling is called a Database Management System (DBMS). A DBMS comes with 
tools to facilitate things like security rules and user access management, user 
authentication, search optimization, data management (Data integrity, encryption, 
compression, de-duplication, backup and recovery etc.), report generation etc. 

In 'Databases: A Beginner's Guide', Andy Oppel provides some definitions for terms 
related to databases[1]:
 
 
    1. Database Object (Page 4): "A database object is a named data structure that is 
       stored in a database. The specific types of database objects supported in a 
       database vary from vendor to vendor and from one database model to another."
       
    2. Database Model (Page 4): "Database model refers to the way in which a database 
       organizes its data to pattern the real world." 
       
       Historical database models include Flat File, Hierarchical, Network and the 
       Inverted file model. Some modern niche database models: Object-oriented, 
       Object-Relational, Graph, Document-oriented, and NewSQL.
    
       The most common modern database models are Relational and NoSQL.
    
    3. File (Page 4): A file is a collection of related records that are stored as a 
       single unit by an operating system. Given the unfortunately similar definitions of 
       files and databases, how can we make a distinction?… The answer lies in an 
       understanding of certain characteristics or properties that databases possess 
       which are not found in ordinary files, including the following:
    
           * Management by a database management system (DBMS)
           * Layers of data abstraction
           * Physical data independence
           * Logical data independence
    
    4. Database Management System (DBMS) (Page 5): The database management system (DBMS) 
       is software provided by the database vendor. Software products such as Microsoft 
       Access, Oracle, Microsoft SQL Server, Sybase ASE,DB2, Ingres, and MySQL are all 
       DBMSs…
    
       The DBMS provides all the basic services required to organize and maintain the 
       database, including the following:
    
       * Moves data to and from the physical data files as needed.
       * Manages concurrent data access by multiple users, including provisions to prevent 
         simultaneous updates from conflicting with one another.
       * Manages transactions so that each transaction’s database changes are an 
         all-or-nothing unit of work. In other words, if the transaction succeeds, all 
         database changes made by it are recorded in the database; if the transaction 
         fails, none of the changes it made are recorded in the database.
       * Supports a query language, which is a system of commands that a database user 
         employs to retrieve data from the database.
       * Provides provisions for backing up the database and recovering from failures.
       * Provides security mechanisms to prevent unauthorized data access and modification.


2.2 RELATIONAL DATABASE MANAGEMENT SYSTEMS (RDBMS)

Andy Oppel explains the Relational Model on pages 17-19[1]:


    In addition to complexity, the network and hierarchical database models share another 
    common problem—they are inflexible. You must follow the preconceived paths through 
    the data to process the data efficiently. Ad hoc queries, such as finding all the 
    orders shipped in a particular month, require scanning the entire database to locate 
    them all. Computer scientists were still looking for a better way…
    
    The relational model is based on the notion that any preconceived path through a 
    data structure is too restrictive a solution, especially in light of ever-increasing 
    demands to support ad hoc requests for information. Database users simply cannot 
    think of every possible use of the data before the database is created; therefore, 
    imposing predefined paths through the data merely creates a “data jail.” The 
    relational model allows users to relate records as needed rather than as predefined 
    when the records are first stored in the database. Moreover, the relational model 
    is constructed such that queries work with sets of data (for example, all the 
    customers who have an outstanding balance) rather than one record at a time, as 
    with the network and hierarchical models.
    
    The relational model presents data in familiar two-dimensional tables, much like 
    a spreadsheet does. Unlike a spreadsheet, the data is not necessarily stored in 
    tabular form and the model also permits combining (joining in relational terminology) 
    tables to form views, which are also presented as two-dimensional tables. In short, 
    it follows the ANSI/SPARC model and therefore provides healthy doses of physical and 
    logical data independence. Instead of linking related records together with physical 
    address pointers, as is done in the hierarchical and network models, a common data 
    item is stored in each table, just as was done in flat file systems…
    
    The elegant simplicity of the relational model and the ease with which people can 
    learn and understand it has been the main factor in its universal acceptance. The 
    relational model is the main focus of this book because it is ubiquitous in today’s 
    information technology systems and will likely remain so for many years to come.


In the Relational Model, two pieces of data are linked to each other with respect to the 
relationship they have to another. A table is a physical representation of a relation.

This article will only consider the most common kind of databases, the Relational 
Database Management System. Here is an explanation of RDBMSes[2]: 


    1. Relational Databases: Relational database management systems (RDBMS) support the 
       relational (=table-oriented) data model. The schema of a table (=relation schema) 
       is defined by the table name and a fixed number of attributes with fixed data 
       types. A record (=entity) corresponds to a row in the table and consists of the 
       values of each attribute. A relation thus consists of a set of uniform records.
    
       The table schemas are generated by normalization in the process of data modeling.
    
       Certain basic operations are defined on the relations:
    
       * classical set operations (union, intersection and difference)
       * Selection (selection of a subset of records according to certain filter criteria 
         for the attribute values)
       * Projection (selecting a subset of attributes / columns of the table)
       * Join: special conjunction of multiple tables as a combination of the Cartesian 
         product with selection and projection.
         
      These basic operations, as well as operations for creation, modification and 
      deletion of table schemas, operations for controlling transactions and user 
      management are performed by means of database languages​​, with SQL being a well 
      established standard for such languages.
    
      The first relational database management systems appeared on the market at the 
      beginning of the 1980s and since have been the most commonly used DBMS type.
    
      Over the years, many RDBMS have been expanded with non-relational concepts such as 
      user-defined data types, not atomic attributes, inheritance and hierarchies, which 
      is why they are sometimes referred to as object-relational DBMS.
      
      
Here is a list of well known RDBMSes:


    Oracle
    MySQL
    Microsoft SQL Server
    PostgreSQL
    DB2
    Microsoft Access
    SQLite
    Teradata
    MariaDB


The most popular open source databases as of October 2018 are: PostgreSQL and SQLite. 
Both are robust and capable. 

SQLite is designed to be a light-weight, server-less single file database. It's used in 
electronic devices like phones, PDAs, MP3 players as well as web browsers. If you need a
low footprint, high-quality database, go with SQLite.

PostgreSQL is a open source, fully featured RDBMS. It's more powerful than MySQL/MariDB,
and it has better data integrity. It will serve well as the database for web applications
and for heavier duty software.   
 
 
2.3 OTHER DATABASE CONCEPTS AND TERMS TO LEARN BEFORE PROCEEDING TO SECTION 3

There is much more to learn about databases before proceeding to Section 3. A full 
explanation of all these concepts is outside the scope of this document. For this reason, 
this section will merely list these concepts. You have to study them and familiarize 
yourself with them before proceeding to following sections. The following lists are mostly 
taken from Caleb Curry's website.[3]


1. SQL

You should know the basics of SQL, because this is the language of databases.
 

2. DATA & DATABASES

    * Data
    * Database
    * Relational Database
    * Database Management System
    * Relational Database Management System
    * Null
    * Anomalies
    * Referential Integrity 


3. DATABASE COMPONENTS

    * Entity
    * Attribute
    * Relation
    * Tuple
    * Table
    * Row
    * Column
    * File
    * Record
    * Field
    * Value
    * Entry
    * Database Design
    * Schema
    * Naming Conventions
    * Keys
    

4. CONCEPTUAL DATABASE DESIGN CONCEPTS

Andy Oppel explains 'Conceptual Database Design' on page 30: [1]

    … involves studying and modeling the data in a technology-independent manner. The 
    conceptual data model that results can be theoretically implemented on any database 
    or even on a flat file system. The person who performs conceptual database design 
    is often called a data modeler.

These are the concepts related to 'Conceptual Database Design':

    * Entity vs. Entity-Type
    * External Entities
    * Attributes
    * Cardinal Relationships (Inter-Table Relationships):
        * One-to-One Relationships
        * One-to-Many Relationships
        * Many-to-Many Relationship
    * Recursive Relationships 
    * Business Rules
    
    
5. LOGICAL & PHYSICAL DESIGN CONCEPTS

Andy Oppel explains these concepts on page 30: [1]

Logical Database Design:


    …is the process of translating, or mapping, the conceptual design into a logical 
    design that fits the chosen database model (relational, object-oriented, 
    object-relational, and so on). A specialist who performs logical database design is 
    called a database designer, but often the database administrator (DBA) performs all 
    or part of this design step.


Physical Database Design:

    The final design step is physical database design, which involves mapping the logical 
    design to one or more physical designs, each tailored to the particular DBMS that 
    will manage the database and the particular computer system on which the database 
    will run. The person who performs physical database design is usually the DBA.


These are the concepts related to Logical and Physical Database Design:

    * Tables
    * Columns and Data Types
    * Constraints
    * Primary Key Constraints
    * Foreign Key Constraints
    * Intersection Tables
    * NOT NULL Constraints
    * CHECK Constraints
    * Constraint Enforcement Using Triggers
    * Views
    * Database Normalization


6. ENTITY-RELATIONSHIP MODEL

In the context of a database, an entity–relationship model (ER model for short) describes 
relationships between instances of entities. These relationships are expressed using 
keys.


    * Keys (Primary, Foreign, Compound, Alternate etc.)
    
    
Learn and know the distinction between various types of keys.
 
 
****************************************************************************************************
****************************************************************************************************
   

3. APPLICATION-TO-DATABASE COMMUNICATION

3.1 ENTITY FRAMEWORK CORE: AN OBJECT-RELATIONAL MAPPER (ORM)

When you start up a program that stores data in a discrete database, as this one does, 
one of the first things the program will do is try and talk to the database. A database 
usually understands only the the SQL language. When you connect a database to a program 
written in C# or another general purpose programming language, you need an translator 
program/library to translate from SQL to C# and vice versa. A program that translates SQL 
to a general purpose programming language like Java, C# or JavaScript and vice versa is 
called an OBJECT-RELATIONAL MAPPER (ORM).

When the program runs, the ORM sits between the program and the database, translating C#
instructions into SQL and the reverse, as illustrated in this diagram:


                                          +---------+
                                          |         |
                                          | PROGRAM |
                                          |         |
                                          +----+----+
                                               ^
                                               |
                                               v
                                         +-----+-----+
                                         |           |
                                         | ENTITY    |
                                         | FRAMEWORK |
                                         | CORE      |
                                         |           |
                                         +-----+-----+
                                               ^
                                               |
                                               v
                                         +------------+
                                         |            |
                                         |  DATABASE  |
                                         |            |
                                         +------------+


Entity Framework Core (from here on in referred to as EFC or EF Core) is an ORM that sits 
between a app server like Hotel-Server and its database. When you hook it up to a data 
source, such as a database, an ORM reads in raw SQL data and turns it into collections of 
C# objects for use in the program. By doing this, EF Core is translating data as it is 
represented in the database into a data representation that C# programs would understand.
Entity Framework Core is the standard ORM solution in the .NET world, provided by 
Microsoft for .NET Core projects. 


3.2 HOW DOES ENTITY FRAMEWORK CORE TRANSLATE INFORMATION TO AND FROM THE DATABASE?

Consider this example table from a database, called Contacts:

+----+-----------+----------+----------------+
| Id | FirstName | LastName |  PhoneNumber   |
+----+-----------+----------+----------------+
|  1 | Bonnie    | Tsing    | (123) 456-7890 |
|  2 | Barry     | Forg     | (198) 928-9834 |
|  3 | Linda     | Blom     | (234) 239-9231 |
+----+-----------+----------+----------------+

The table contains 3 records. Each record has 4 fields. Each record contains Contact 
information for one person. This is how data is stored and represented in a database. 

There are other ways to represent this data. Take a look at the C# model class below: 


    public class Contact
    {
        [Required]
        public int Id { get; set; }
        
        [MinLength(2)] 
        [MaxLength(50)]
        public string FirstName { get; set; }
        
        [MinLength(2), MaxLength(50)]
        public string LastName { get; set; }

        [Phone]
        public string PhoneNumber { get; set; }
    }
    
    
The Contact class has a name similar to the table and it has four properties, but what is 
most interesting is that the property names - Id, FirstName, LastName and PhoneNumber - 
are the same as the column names in the Contacts table above. Do you see where this is 
going?

Put simply, records in a database can be converted into C# objects. The structure of a 
table in a database, such as the Contacts table above, can be represented in a C# program 
in the form of a C# POCO (Plain Old Class Object) model class. Columns in a table can be 
mapped to properties in a POCO class. The information in a record from the Contacts table 
can be represented in the form of a Contact object. To say it again, data contained in a 
record from a database table can transmuted into a form recognizable and usable in a C# 
program - a C# object.  

This is Entity Framework Core's job: to map records from tables in a database to a 
collection of C# objects and and the reverse. More specifically, EF Core acts as a 
translation layer between the Hotel Reservation System project and its database, 
translating C# code into SQL commands and C# objects into records in the database and 
vice versa, without writing any SQL or any database access code.


3.3 ENTITY FRAMEWORK CORE TOOLING: NUGET PACKAGES & CLI TOOLS

You'll need several nuget packages to add EF Core functionality to your project: 


    Microsoft.EntityFrameworkCore, 
    Microsoft.EntityFrameworkCore.Relational, and 
    Microsoft.EntityFrameworkCore.Tools (For Migrations. Tools for Package Manager Console.)
    
    
Fortunately, all these packages come with the Microsoft.AspNetCore.All metapackage, which 
appears to be mandatory for all ASP.NET Core projects. 

If you want to use the command line interface, which you will need while working with 
migrations, note that there are two sets of tools available:


    1. Cross Platform .NET Core Command-Line Interface (CLI) Tools
    2. Visual Studio's Package Manager Console Tools  


Due to the Visual Studio centric nature of the VSPMC tools, I will only provide commands
for the cross-platform tooling. .NET Core CLI tools come with the .NET Core SDK (for 
ASP.NET Core 2.1+ projects). With this set of CLI tools, most commands begin with 
'dotnet ef'.


3.4 ENTITY FRAMEWORK CORE: CONCEPTS AND COMPONENTS

Entity Framework Core has several major concepts and components you have to understand. 
You have to set up the first three things in this list to get EF Core to act as an ORM:


    1. DATABASE PROVIDERS: "Entity Framework Core can access many different databases 
       through plug-in libraries called database providers." By adding a database 
       provider designed to be consumed by Entity Framework Core to the App Server 
       project in your application, you will enable EF Core to talk to that particular 
       vendor's database.
    
    2. DATA MODEL: is the collection of Model classes in a program. A Model Class is a 
       POCO class: a class that contains only properties. Model classes define business 
       entities, which are objects on which the program performs operations. Business 
       entities are usually stored in the database, which is why EF Core needs model 
       classes: To understand the C# representation of a business entity when it talks to
       the database. 
         
    3. THE SCHEMA: (aka the Context/DbContext class) A Context class defines the schema 
       for a database by extending the Data Model. In it, the programmer declares all the 
       tables in a database as DbSet properties. Then, these tables have their columns 
       declared, followed by constraints associated with each column. Finally, the schema 
       declares inter-table relationships (One-to-One, One-to-Many, Many-to-Many). 
       
       It's worth noting that the DbSet data structure used in the Schema almost qualifies
       as a separate concept. Make sure you understand how they work. 
    
    4. MIGRATIONS: are version control for the schema. They track changes to the 
       database's topology during the lifetime of the software project. 
     

Here they are listed again, this time with information on where in the program they are
defined:    


     1) Database Provider,          (Defined in Data/MyDbContextFactory.cs)
     2) Data Model,                 (Defined in the Models folder of the Common project)
     3) Database Schema,            (Defined in Data/Context.cs)
     4) Migrations                  (Generated into the Data/Migrations Folder)


The next three sections explain these concepts in greater detail.


3.4.1 DATABASE PROVIDERS
 
A Database Provider is a "a software library consisting of classes that provide data 
access services such as connecting to a data source, executing commands at a data source 
and fetching data from a data source with support to execute commands within 
transactions. It resides as a lightweight layer between data source and code, providing 
data access services with increased performance."[4] 

A DATABASE PROVIDER is a small library that helps EF Core to speak to databases from a 
certain vendor. For instance, there are Database Providers for SQL Server, SQLite, 
PostgreSQL and other databases. EF Core can talk to any database if it has the database 
provider library for it. When Hotel-Server is run, Entity Framework will be brought 
online to talk to the database. The first thing EFC will do is search for the Database 
Provider. 

Therefore, you need to find the right database provider for the database in your project 
and add it to the program via a Nuget package. The Database Provider for this project is 
the 'Npgsql.EntityFrameworkCore.PostgreSQL' package. By using this package, EF Core will 
come to know that this project uses a PostgreSQL database. Through it, EFC can interface 
with the database and carry out operations on it. 

Once it can talk to the database, EF Core will will use the Data Model to transform a 
SQL query into an object model representation of a query (which is called a canonical 
command tree) and vice versa. .NET Database Providers can consume canonical command trees 
to talk the database.

The Data Provider, connection string and other related features can be configured in: 


    * Data/MyDbContextFactory.cs, OR
    * ConfigureServices() method of Startup.cs 
    

To learn more about database providers, go to: https://docs.microsoft.com/en-us/ef/core/providers/


3.4.2 THE PROGRAM-SIDE REPRESENTATION OF DATABASE ENTITIES: THE DATA MODEL vs. THE SCHEMA

Once EF Core confirms that it can talk to the database via the database provider, it 
will need the Data Model and Schema of the database. What are those things and why does 
your database need them? Recall that it is EFC's job to do two-way translations between 
your program and the database. This involves mapping tables and other databases entities 
to C# objects and vice versa. To do this, EF Core must have a program-side representation 
of entities your program will want to store in the database. This includes entity and 
attribute names, and their relationships expressed in C# code. The Data Model and the
Schema are precisely that: representations of database entities and their relationships. 

A short discussion is in order on when and how the data model and schema is to be 
written. When the programmer first starts writing an ASP.NET Core program, she typically
starts by defining the entities in that project. These entities are usually defined as
POCO classes in the 'Models' folder of your solution. In the eyes of EFC, these model 
classes are the Data Model. When preparing a greenfield database for your program, you 
have to write the schema for it. To write the schema, the programmer will have to 
consult the Data Model.

Let's move on to studying these two things. The Data Model is distinct from a Database 
Schema; see this StackOverflow Answer from richik jaiswal to learn the difference[5]:


     A schema is a blueprint of the database which specifies what fields will be present 
     and what their types will be. For example an employee table will have an employee_ID 
     column represented by a string of 10 digits and an employee_Name column with a 
     string of 45 characters.
     
     Data model is a high level design implementation which decides what can be present 
     in the schema. It provides a database user with a conceptual framework in which we 
     specify the database requirements of the database user and the structure of the 
     database to fulfill these requirements.
     
     A data model can, for example, be a relational model where the data will be 
     organised in tables whereas the schema for this model would be the set of attributes 
     and their corresponding domains.


A Data Model is an abstract formalization of a database's entities. The basic properties 
of database entities like tables and columns are defined as C# model classes. The model 
classes in the 'Models' or 'Entities' folder of your project form the DATA MODEL for your 
database. 

For an example, let's take another look at the Contact class from section 3.2:


    public class Contact
    {
        [Required]
        public int Id { get; set; }
        
        [MinLength(2)] 
        [MaxLength(50)]
        public string FirstName { get; set; }
        
        [MinLength(2), MaxLength(50)]
        public string LastName { get; set; }

        [Phone]
        public string PhoneNumber { get; set; }
    }


The Contact class is a data model class: it's serves as very abstract, high-level C# 
representation of the Contacts table in your database. When EFC tries to talk to the 
Contacts table in your database it will first consult its C# counterpart, the Contacts 
data model class, to learn the topology of this entity. 

Note that the properties of this class have data annotation attributes on them. These 
annotations mandate values for the 'Id' property and establish value minimums and 
maximums on the 'FirstName' and 'LastName' properties. As the data model informs the 
schema, when the schema is written, these restrictions will carry over to it. In fact,
when the programmer first writes the schema for a new database, she usually derives it 
from the data model. 

Go to this location to see the model classes for this project.


    /Common/Models/Tables


Let's move on to discussing the schema. The DATABASE SCHEMA is a more specific definition 
of a database's entities and their attributes. It serves as a snapshot of the database's 
topology at the current point in time. The schema takes entities defined in the data 
model, plus any data annotation attributes defined therein, and adds other things to it. 
This may include restrictions on character length for a field, marking certain fields as 
mandatory when a record is created, and establishing the nature of relationships between 
entities. 

For new databases, the schema has to be written by the programmer. However, if a database 
already exists, you can join it to an ASP.NET Core project and then generate a schema 
from it. 

Let's take a look at what a schema looks like. First, here's the Hotel model class (from 
the Models folder): 


    public class Hotel : IObjectWithState
    {
        // ORMs (Object-Relational Mapper) map properties in an object to a table. The 
        // ObjectState property is not a table column; it is used to track changes in an 
        // entity. This will likely happen in the endpoints of this project.
        [NotMapped]
        public ObjectState State { get; set; }
        
        
        // The primary key of the Hotel table in the Database is the Room Number.
        [Key]
        public long Id { get; set; } 
        public string Name { get; set; }
        public string Address { get; set; }
        public string PhoneNumber { get; set; }

        // Hotel Objects have a HotelRooms property to track the list of hotelrooms at a
        // hotel without needing to query the database.
        public virtual IEnumerable<HotelRoom> HotelRooms { get; set; }
    }
    

There are schemas for at least three levels of a database: physical schema (how data 
blocks are stored at the lowest level; this is where database designers work), logical 
schema (how records get stored in data structures; this level is where programmers and DB 
administrators work) and view schemas (which are schemas for end-user interaction). 

Taken from Context.cs, here is the corresponding Schema for a Hotel table:


    public class Context : DbContext
    {
        public virtual DbSet<Hotel> Hotels { get; set; }

        protected override void OnModelCreating(ModelBuilder modelBuilder)
        {
            modelBuilder.Entity<Hotel>(entity =>
            {
                entity.HasKey(e => e.Id);
                entity.Property(e => e.Id).IsRequired().ValueGeneratedOnAdd();
                entity.Property(e => e.Name).IsRequired().HasMaxLength(100);
                entity.Property(e => e.Address).IsRequired().HasMaxLength(200);
                entity.Property(e => e.PhoneNumber).IsRequired().HasMaxLength(100);
            });
        }
    }
        

When any ASP.NET Core program runs normally, EF Core will be sitting between the program 
and its database, translating SQL and database records into C# commands and objects. 
Provided it has the data model and the schema, EF Core has a map to the layout of any 
particular database.

Let's talk about the schema for the Hotel table; EF Core is going to use it to 
translate entities from the database into C# objects. When EF Core converts information 
from a data source into C# objects it needs two things: a pre-defined C# model class that 
can represent data from each table and a collection to hold these model objects. 

Let's talk about the Hotels DbSet<Hotel> data structure first. DbSet<Hotel> is a generic 
data structure that can contain only Hotel objects. This is the first thing that is 
declared in the schema: a data structure to hold Hotel objects as they get converted from 
records, called Hotels.

The purpose of a Schema is to define the specific shape and form of the entities in a 
table. Following the declaration of Hotels DbSet, comes the OnModelCreating() method. 
This method is overridden and the schema is defined within; it lays out primary keys for 
tables and Navigation properties (foreign keys) between entities. Lastly, we can also 
define field constraints in this method, which are specifications and limitations for 
every column in the Hotel Table. For instance, HasKey() denotes the Id field as the 
Primary Key of a table. The other properties are defined with the Property() method. The 
Id, Name, Address and PhoneNumber fields are marked as required fields. The character 
lengths of each field are also specified. 

As EF Core converts every record into a new object in a DbSet<T> collection or vice 
versa, it makes sure that all these requirements are met. The fields of each record in 
the table are copied and assigned to the properties of a new C# object. Once a new Hotel 
object is populated with data from a record, it gets stored in the Hotels collection.

In case it was not clear, the schema for an ASP.NET Core solution can usually be found 
in:

    /Data/Context.cs


3.4.3 MIGRATIONS

3.4.3.1 WHAT ARE MIGRATION? WHY DO WE NEED THEM?

During the course of developing a software solution, your program and its database will
grow and change. This section will focus on managing change in databases.

Recall that the Schema is a more specific map to the database than the Data Model. The 
schema of a database defines tables, specifies columns and their types, and the 
relationships between tables, i.e. it concretely describes the structure and topology of
the database. It also imposes constraints on data that seeks to enter the database.

However, a database is not a static thing; it changes with its program. In other words, 
during the lifetime of a database, its topology is in constant flux. In order to make any 
changes to fundamental entities in a database or their relationships, the programmer 
will have to modify the schema. Tables and attributes can be modified, added or deleted
by making changes to the schema. Corresponding changes will also have to be made to the
Data model classes.  

Changes to a database are complicated by the fact that organizations usually have 
multiple instances of a database. For instance, a company may have a development 
environment where software is developed, a production environment where stable software 
runs and perhaps a demo environment for demonstrating products to clients. If the 
development team makes changes to the master database, how are they going to roll out 
changes to all instances of the database? Databases change often enough that their Schema 
needs version control. 

A schema defines the topology of a database at one point in time. Usually, the schema 
only contains the current topology of the database. However, it would be very useful to 
have snapshots of the schema every time it was changed. This is where SCHEMA MIGRATIONS 
(often called MIGRATIONS) come in. Schema snapshots are called MIGRATIONS and they serve 
as version control for a database's schema. 

To track changes to the schema, you can generate a migrations file. Migration files 
contain only delta values, i.e. only changes from the previous migration file. Your 
program usually has one active schema file, but many migrations files. You can use 
migrations to update to or rollback from a version of the database's schema. A migration 
file will  have a way to revert its own changes. It may also contain scripts to enact 
transformations and to insert default data into the database by populating tables and 
columns.

To ensure data integrity, it is a best practice to take a snapshot of a database's 
topology every time a change is made to the schema. When you first write the schema for a 
new database, you should generate a migration file that contains the entire schema. 
Subsequently, every time you change entities or their relationships by editing the 
schema, you will want to generate a new migration file that contains the new, updated 
topology of the database. Once again, do note that the new migration file only contains 
delta values, i.e. only changes from the previous migration file.

Migrations are constantly generated in the development stage of an application, because 
that's when the schema is changing the most. Large architectural changes and feature 
churn necessitate frequent alterations to the database's schema. Migrations are also 
needed when upgrading a database, switching to another database vendor, or moving a 
database into the cloud. Having a version controlled history of the database's schema 
(which captures the changes made to the database over time) gives you the ability to 
migrate the database to older or newer versions of the schema. Migrations are a 
convenient way to track your database's schema over time in a predictable and 
consistent way. By using migrations, you can update all instances of a database 
automatically and reliably to the schema version of your choice.
 
Typical location of migration files: Data/Migrations/ 


3.4.3.2 THE CONTENTS OF A MIGRATIONS FILE

Every migration file will contain an Up() and Down() method. The Up() method contains 
transformations to the schema of the Database. The Down() method just rolls back 
transformations in the Up() method i.e. it returns the schema to the state in the Up() 
method of the previous migration. The database schema should be unchanged if a call to 
Up() is followed by a Down(). If another, new migration file has been generated and 
needs to be applied to the database, call its Up() method. To revert these 
transformations, call its Down() method. 

The final set of transformations in a long chain of such changes is described in the 
Up() method of the last migration. The current state of a database's schema is the 
result of calling the Up() methods of all previous Migrations. 

Here is a answer from StackOverflow that expands on migrations[6]:

    DB Migrations make changes to your database to reflect the changes made to the Entity 
    Framework Model. These changes are added to the database with the Up method.
    
    When you want to rollback a change (e.g. by rolling back a changeset in TFS or Git), 
    the changes in the database have to be rolled back also because otherwise your Entity 
    Framework model is out-of-sync with the database. This is what the Down method is 
    for. It undo's all changes to the database that were done when the Up method was run 
    against the database.
    
    The Seed method gives you the ability to Insert, Update or Delete data which is 
    sometimes needed when you change the model of the database. So the Seed method is 
    optional and only necessary when you need to modify existing data or add new data to 
    make the model working.
    
    - Ric .Net    
    
Leaving aside the Seed method, which appears to from another framework, let's take a look 
at a migrations file. Taken from the file called, 
'20180128032156_CreateHotelHotelRoomRoomTypeBedTypeRoomReservationTables.cs', this is the 
Logical schema for the Hotel table: 


    public partial class CreateHotelHotelRoomRoomTypeBedTypeRoomReservationTables : Migration
    {
        protected override void Up(MigrationBuilder migrationBuilder)
        {
            migrationBuilder.CreateTable(
                name: "Hotels",
                columns: table => new
                {
                    Id = table.Column<long>(type: "int8", nullable: false)
                        .Annotation("Npgsql:ValueGenerationStrategy",
                                     NpgsqlValueGenerationStrategy.SerialColumn),
                    Address = table.Column<string>(type: "varchar(200)",
                                                   maxLength: 200,
                                                   nullable: false),
                    Name = table.Column<string>(type: "varchar(100)",
                                                maxLength: 100,
                                                nullable: false),
                    PhoneNumber = table.Column<string>(type: "varchar(100)",
                                                       maxLength: 100,
                                                       nullable: false)
                },
                constraints: table =>
                {
                    table.PrimaryKey("PK_Hotels", x => x.Id);
                });
        }
    }
    

3.4.3.3 GENERATING, APPLYING AND REMOVING MIGRATIONS
    
What happens when a migration is generated? When the command is given, it appears that 
Entity Framework Core combines the existing Schema with data from Model classes to 
generate a migration file. Generating a Migration will create a new file in the 
Migrations folder. It will contain a a user-named class that is a subclass of Entity 
Framework Core's Migration class. You may need to manually remove things from either 
method if it added more than the changes you made to the Schema. Make sure you read the 
migration file before applying it.

Migrations are generated or applied with CLI commands. Note that there are two sets of 
CLI tools provided by Microsoft, one that is Visual-Studio only and the other is cross-
platform tooling that comes with every installation of the .NET Core SDK. The

The commands given below are for the cross-platform CLI tools only. You can issue these
commands by switching to the 'Terminal' tab in JetBrains Rider. Check the Entity 
Framework Core docs for more commands.


    1. GENERATING MIGRATIONS: Creates a new migration. The migration name should describe 
       the changes you made to the data model in pascal case (E.g. "PascalCaseExample"). 
    
       Command: dotnet ef migrations add <Migration-Name>
       
       E.g. 1: dotnet ef migrations add InitialMigration
       E.g. 2: dotnet ef migrations add AddUserUserPreferenceUserInformationRoleUserRoleTables
    
       According to Microsoft Docs' article on Migrations[7], the following things happen
       when you execute this command: 
       
           Three files are added to your project under the Migrations directory:
           
           * 00000000000000_InitialCreate.cs--The main migrations file. Contains the 
             operations necessary to apply the migration (in Up()) and to revert it 
             (in Down()).
             
           * 00000000000000_InitialCreate.Designer.cs--The migrations metadata file. 
             Contains information used by EF.
             
           * MyContextModelSnapshot.cs--A snapshot of your current model. Used to 
             determine what changed when adding the next migration.
    
    
    2. UPDATING DATABASE TO THE LATEST MIGRATION: Runs the Up() method of all migrations. 
    
       Command: dotnet ef database update 


       VARIATION 1: UPDATE DATABASE TO A SPECIFIED MIGRATION
       
       Command: dotnet ef database update <Migration-Name>
       
       E.g. 1: dotnet ef database update 20180904195021_InitialCreate
       
    
    3. REVERT THE LAST MIGRATION: Rolls back the last applied migration.
    
       Command: dotnet ef migrations remove [-f]  [--force]
       
    4. LIST AVAILABLE MIGRATIONS.
    
       Command: dotnet ef migrations list
        

There are many more commands available. These commands are not restricted to migrations
either. There are commands to drop databases, query for DbContext information, scaffold a
DbContext from an existing database and more. 

    
3.4.3.4 MIGRATIONS: THE WORKFLOW 

This is the workflow for changing the schema and generating migrations:


In the Development Environment:

    1. When you need to add tables or modify relations in the database, edit the Schema 
       in Context.cs.
    2. Because the Schema has been modified, you need to generate a new migrations file.
    3. You run the migration file in the development environment to verify that it works 
       as intended. 


In the Production, Demo and other Environments:
    
    4. Deploy and run the migrations file in other environments to update these instances 
       of the database to the latest version of the schema.
    5. Verify that migration has gone smoothly in these environments as well.
    

****************************************************************************************************
****************************************************************************************************
    
4. DATABASE NORMALIZATION

4.1 THE CONSEQUENCES OF POOR DATABASE DESIGN 

To keep it short, our natural instincts for creating tables and storing data in a 
database typically lead to terrible database designs. Poorly designed databases are prone 
to data redundancies or database anomalies. Redundant data is self-explanatory, so what 
are database anomalies? An anomaly is an aberrant behaviour in a database that occurs due 
to poor design. There are three kinds of database anomalies:


    * INSERTION ANOMALY: is a situation that occurs when you cannot insert a new record 
      into a table because the table requires a piece of data that is unavailable to you. 
      
      E.g.: A database that cannot add a customer information to its Customers table 
      until the customer actually makes a purchase.
      
      
    * DELETION ANOMALY: is the opposite of the Insert anomaly. This is a situation where
      the deletion of one entity/record causes unintended deletion of another entity or
      record. Deletion anomalies result in unexpected data loss.
      
      E.g.: Using the previous example, a deletion anomaly occurs if we delete a customer 
      invoice, which due to poor design, causes the deletion of that customer's 
      information. As in the insertion example, this happened because information about 
      two entities, namely customers and invoices, was incorrectly combined into a single
      table. 
      
      
    * UPDATE ANOMALY: arises when the same information can be expressed on multiple rows 
      of a table. This makes it possible for the same information to exist in multiple
      records. If a table design permits this error, then over time, it will almost 
      certainly lead to data inconsistencies when one record is updated but the other one 
      is not.
      
      E.g.: Suppose you have a Student table where its possible to have a student's 
      information sstored in more than one record. If one of these records is updated, 
      say the address is changed, you now have two records for one student, with 
      conflicting addresses. This is a clear sign of data redundancy design error in your 
      table.
      
      Your tables should have only a single source of truth for an attribute. Attributes 
      should be captured once and stored once in one field only. If needed, you can make 
      references to it in other tables. All references must point to this single, 
      original source.


The best way to avoid anomalies is to normalize your database. This is a reference to a 
practice called 'Database Normalization'. 


4.2 NORMALIZATION

Here is an introductory description of Normalization from Wikipedia's article[8]:


    Database normalization is the process of restructuring a relational database in 
    accordance with a series of so-called normal forms in order to reduce data redundancy 
    and improve data integrity. It was first proposed by Edgar F. Codd as an integral 
    part of his relational model.
    
    Normalization entails organizing the columns (attributes) and tables (relations) of a 
    database to ensure that their dependencies are properly enforced by database 
    integrity constraints. It is accomplished by applying some formal rules either by a 
    process of synthesis (creating a new database design) or decomposition (improving an 
    existing database design).


Here's an alternate definition[9]:


    Database Normalization is a technique of organizing the data in the database. 
    Normalization is a systematic approach of decomposing tables to eliminate data 
    redundancy (repetition) and undesirable characteristics like Insertion, Update and 
    Deletion Anomalies. It is a multi-step process that puts data into tabular form, 
    [and] removing duplicated data from the relation tables.
    
    Normalization is used for mainly two purposes,
    
    * Eliminating redundant data, AND
    * Ensuring data dependencies make sense i.e data is logically stored.
    
    
When you design your database, you will have to do something called 'Entity-Relationship 
Modelling'. This is the process of planning out tables in your database and their
relationships to other tables. ER Modelling is dominated by "big picture" consideration 
of your program's data needs. Based on your conclusions, you should iteratively identify 
the relevant entities (tables) in your database, their attributes (columns) and their 
relationships. 

Once you produced a tentative database design, you should have some idea of what tables 
you will have in your database, their names, their attributes and their relationships to 
other tables. Designing inter-table relationships means that you should have decided on 
primary keys for most of your tables.

Following the ER Modelling process, you will then apply the process of 'Database 
Normalization'. Normalization is applied at the "micro level"; you will focus on one 
entity (table) and consider its attributes and other characteristics. The general idea
behind normalization is that a table is about a specific topic and that only columns
related to the topic must be included in the table. Our natural instincts at table design 
typically lead to tables that contain duplicate data. The other big problem is that poor
table design results in tables that fail to take advantage of the database's ability to
check and impose data integrity constraints. This can lead to wrongly typed data, 
multi-field values and other types of data integrity problems.

You should normalizae every database you design. You should do normalization as part of
the initial design and whenever you revisit and tweak your database's schema. The process
of normalizing your database often results in the creation of new tables.

Normalization will make your database easier and more reliable to work with. 
Normalization will reduce unwanted duplicate data in the database, thereby decreasing its
size and increasing integrity and robustness of the data stored therein. This will make 
it harder to create invalid states in the database or enter poor quality data into it. 
This should make it easier to maintain, edit and modify your database.  


4.2.1 FUNCTIONAL DEPENDENCY

To normalize tables, you have to understand the concept of 'Functional Dependency'. 
A functional dependency is a relationship between two attributes. Usually, this is 
referring to the relationship between a Primary Key (PK) attribute and a second non-key 
attribute within a table.  

For two arbitrary attributes, A and B, Attribute B is said to be functionally dependent 
on Attribute A, if at any moment in time there is exactly one instance of B for any given 
instance of A. Phrased another way: If Attribute A functionally determines Attribute B, 
the value of an instance of A uniquely determines a value for B. Therefore, A is a 
determinant (i.e. a unique identifier) of B and B is a dependent of A. This relationship 
is visually represented like this:

    
    A -> B 


It is read like this: "A (functionally) determines B."

Let's take a concrete example[10]: In a table, suppose there are two columns with a 
functionally dependent relationship: 'Social Security Number' and 'Employee Name'. The 
'Employee Name' attribute is functionally dependent on the 'SSN' attribute because an SSN 
uniquely identifies an employee. Note that the reverse does not hold true; names do not 
uniquely identify a SSN. If there were other non-key attributes in the table, for example
'Employee Address' and 'Birth Date', the 'SSN' attribute could functionally determine 
them too.

Why is this important? In a StackOverflow answer, NealB explains[11]:


    Sets of functional dependencies may be used to synthesize relations (tables). The 
    definition of the first 3 normal forms, including Boyce Codd Normal Form (BCNF) is 
    stated in terms of how a given set of relations represent functional dependencies. 
    Fourth and fifth normal forms involve Multi-Valued dependencies (another kettle of 
    fish).
 
 
Normalization applies what are called 'Normal Forms' to tables. As NealB notes, the first 
three forms require understanding of functional dependencies. Functional dependency acts 
as a constraint between two attributes. 


4.2.2 THE NORMAL FORMS

The Normal Forms were developed by Edgar Codd, the father of relational databases. 
Normalization theory defines 6 Normal Forms, of which the first three are treated like an 
essential block. The first three forms should be applied to all databases. The fourth 
form is not necessary in many cases, and the fifth form comes into play only very rarely.
The sixth form seems to be treated like a theoretical oddity.  

    
4.2.2.1 FIRST NORMAL FORM (1NF)
   
First Normal Form is a property of a relation (table). These are 1NF criteria:


    * To eliminate anomalies and data redundancy, create a separate table for each set of
      related data. Each table should have a primary key to uniquely identify records. 
      
    * All values in a column must be of the same domain (data type). 
      
    * A field (the intersection of a row and a column) must, at most, contain only one 
      piece of data. This piece of data must be atomic, i.e. it cannot be divided into 
      smaller pieces. This applies to all fields in all tables. 1NF prohibits multi-value 
      fields. Do not use commas, semi-colons or other delimiters to put multiple values 
      into a field.
    
    * Eliminate repeating groups from tables. Repeating groups are columns with the same
      name, but different numbers at the end. They are a special case of multi-value 
      fields. They usually occur when you try to get rid of multi-value fields by 
      creating new columns. The best way to eliminate repeating groups is to study the 
      table, find and identify the set of data related to repeating groups and move them 
      into a new table. 
          
    
This principle is hard to understand in the abstract, so consider the table below. It is
an 'Employee' table, which contains a list of employees, their Ids, names, emails etc. 
Each employee may be issued corporate devices like laptops and phones, whose serials need 
to be tracked. This will be done in the last column, which is called 'DeviceSerial'.


    +------------+-----------+----------+-----------------------+--------------+
    | EmployeeId | FirstName | LastName |         Email         | DeviceSerial |
    +------------+-----------+----------+-----------------------+--------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D |
    |       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 |
    |       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8 |
    +------------+-----------+----------+-----------------------+--------------+


So far, this table lets the company track the devices they given to employees and as a
bonus, complies with 1NF. However, what happens if employees are given more than one 
device? The easiest solution to the problem is to add additional devices to the 
DeviceSerial column, separated by commas and other delimiters. This is what it would look
like:


+------------+-----------+----------+-----------------------+--------------------------------------------+
| EmployeeId | FirstName | LastName |         Email         |                DeviceSerial                |
+------------+-----------+----------+-----------------------+--------------------------------------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D, SD8GD8DFHFH0                 |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 | RIQO98L239DS | 0J78G8J8G7GJ |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8                               |
+------------+-----------+----------+-----------------------+--------------------------------------------+


However, this is something you should NEVER do in a database. It is a violation of 1NF. 
Relational Databases can handle many things, but they are not equipped to handle 
multi-value fields. Multiple values in a field get treated like a single value. Searching 
for data becomes extremely difficult in a database that fails 1NF compliance. The bigger 
problem is that this practice retards a database's full abilities to categorize and 
manage data. If you use multi-value fields, you are now responsible for enforcing data 
types, and other data integrity constraints for potentially millions of records. So, 
don't go there. Let your database manage the data and enforce types and constraints. All 
you have to do to offload this responsibility to the database is put a maximum of one 
value in a field.  

However, we still have multiple values to track. How else can you track them? The next 
obvious solution is to add additional columns for devices, so that you can track one 
device per column. See the table below:


+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
| EmployeeId | FirstName | LastName |         Email         | DeviceSerial1 | DeviceSerial2 | DeviceSerial3 |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D  | SD8GD8DFHFH0  |               |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4  | RIQO98L239DS  | 0J78G8J8G7GJ  |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8  |               |               |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+


Unfortunately, this is also something you should NEVER do, because this too violates 
First Normal Form. Creating multiple columns in the vein of DeviceSerial1, DeviceSerial2,
DeviceSerial3 etc., is called a REPEATING GROUP. The existence of repeating groups also
violates First Normal Form. The classic sign of a repeating group is identical column
names with different numbers tacked on the end of it to make unique names. 

Repeating groups are a sign of inflexible design. What happens when employees from the 
quality control department need to be given 10 different devices each? Repeating groups 
do not scale well. Giving an employee a new device should not necessitate changes to a 
database's schema. 

No, the correct response to such a problem is to create a new table that tracks devices,
with EmployeeId as foreign key. You can move the 'DeviceSerial' column to this new table
and add new ones, like 'DeviceType' and 'Description' to help the business better track 
its devices. After doing this, you can establish a one-to-many relationship between the 
'Employee' and 'Devices' table, where the 'One' side is the 'Employee' table and the 
'Many' side being the 'Devices' table. This is the modified 'Employees' table may look 
like:

    
    +------------+-----------+----------+-----------------------+
    | EmployeeId | FirstName | LastName |         Email         |
    +------------+-----------+----------+-----------------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com |
    |       0002 | Jill      | Hill     | jhill@gmail.com       |
    |       0003 | Simon     | Says     | ss@outlook.com        |
    +------------+-----------+----------+-----------------------+


This is what the 'Devices' table might look like: 

    
    +--------------+------------+------------+-----------------------+
    | DeviceSerial | EmployeeId | DeviceType |      Description      |
    +--------------+------------+------------+-----------------------+
    | AD89AFLL35L4 |       0001 | Iphone     | 5th gen Apple iPhone. |
    | AD9F8AFAF98D |       0003 | Printer    | HP Printer #2839      |
    | SDF90AF87GG8 |       0003 | Laptop     | Dell Latitude 910     |
    | 0J78G8J8G7GJ |       0002 | Camera     | Canon XTS Pro 510     |
    +--------------+------------+------------+-----------------------+
    
    
You can use 'EmployeeId' to query the 'Devices' table, and find the devices that any 
given employee has been loaned. With this design modification, there are no repeating 
groups in tables or repeating values in any field.

Note that the trouble with the original table was that table's design was too lenient:
You are to only put a set of related data in a table, but the original design allowed 
extraneous data in. Often, as was the case in this example, the typical solution to a 
normalization problem is the creation of a new table.
  
    
4.2.2.2 SECOND NORMAL FORM (2NF)
      
    * The first requirement for Second Normal Form is that the table is in First Normal
      Form. Full compliance with 1NF is a mandatory requirement for full compliance with 
      2NF.
    
    * Second Normal Form (and Third Normal Form) is concerned with the relationship 
      between Key columns and Non-key columns. More specifically, 2NF is only a problem 
      for tables that use a composite primary key. This is a primary key that is composed 
      of values from two or more columns. If you use a composite key, you must pay close
      attention to its relationship to other non-key columns in that table. 
    
      If your table does use a composite key, 2NF demands that all non-key attributes 
      must be functionally dependent on the entirety of the composite key. As a composite 
      key is composed of two or more keys, it is possible for a non-key attribute to be 
      dependent on only one of the component keys of the composite key. 2NF will not 
      accept partial dependence; it demands that all non-key attributes must be fully 
      dependent on all constituent attributes of the composite primary key. 
    
    
Once again, this is hard to understand without a actual example, so consider the example
table below. This is a 'Shipments Received' table. The 'Shipment#' and 'Part#' columns 
together form a composite key for the table. This has been done because the 'Shipment#' 
column alone cannot uniquely identify a part; a Cooling Gasket can be part of multiple 
shipments. However, when 'Shipment#' is combined with 'Part#', you can use that to 
uniquely identify the other non-key columns. Or at least, that's what should happen if
2NF were in effect. However, while this table complies with 1NF, it fails to comply with 
Second Normal Form.   


    +-----------+--------+---------------+-----------------+------------+-------------+
    | Shipment# | Part#  | DateOfArrival |    PartName     | Quantity   | Price (CAD) |
    +-----------+--------+---------------+-----------------+------------+-------------+
    |       532 | 183490 | 2018/08/23    | Cooling Gasket  |         43 |         125 |
    |       329 | 109834 | 2018/03/11    | Sprocket Wrench |        118 |          35 |
    |       623 | 958943 | 2018/09/02    | Fizzy Whizzler  |         12 |        1500 |
    |       497 | 183490 | 2018/06/17    | Cooling Gasket  |         11 |         125 |
    |       648 | 958943 | 2018/11/13    | Fizzy Whizzler  |         25 |        1500 |
    +-----------+--------+---------------+-----------------+------------+-------------+


Why? Because two columns, 'PartName' and 'Price', are not functionally dependent on the
composite key. In fact, these columns fail to have a dependency on either 'Part#' or 
'Shipment#', so they are not even partially dependent on the composite key. You'll notice 
that multiple entries exist for 'Cooling Gasket' and 'Fizzy Whizzler' and their 
associated prices. Neither 'PartName' nor 'Price' can be uniquely determined by either of 
the component keys of the composite key, or the composite key as a whole. Thus, this 
table fails to be 2NF-compliant. If the offending non-key columns were dependent on 
either 'Part#' or 'Shipment#', they would be partially dependent on the composite key.
Partial dependence also fails 2NF: all non-key columns must depend on the entirety of the
composite key.  

Why is any of this important? This is important because at least a few columns, 'Part#', 
'PartName' and 'Price' respectively, are linked to each other, but that link is not being
managed by the database. What if someone updates a part number in this table, but fails 
to change its associated part name or its price? Now you face the question: Did you get a 
shipment of 'Sprocket Wrenches' or part number 34910 on a given date? Which is the 
incorrect value? Chaos will rule in the database, as part numbers and names change and 
start conflicting with each other. You do not want error-prone humans in charge of making 
sure values do not conflict in a database. A database could have millions of records! The 
point of implementing 2NF is to offload that responsibility on to the database itself. 

How can you fix this problem and bring this table into compliance with 2NF? Once again, 
the solution is to create a new table. You have to create a new 'Parts' table and move 
the 'PartName' and 'Prices' columns into it:


    +--------+-----------------+-------+
    | Part#  |    PartName     | Price |
    +--------+-----------------+-------+
    | 183490 | Cooling Gasket  |   125 |
    | 109834 | Sprocket Wrench |    35 |
    | 958943 | Fizzy Whizzler  |  1500 |
    +--------+-----------------+-------+
    
    
The 'Part#' attribute will act as a primary key for this field. In the 'Shipments 
Received' table, you can remove these fields and make the table smaller, like this: 


    +-----------+--------+---------------+----------+
    | Shipment# | Part#  | DateOfArrival | Quantity |
    +-----------+--------+---------------+----------+
    |       532 | 183490 | 2018/08/23    |       43 |
    |       329 | 109834 | 2018/03/11    |      118 |
    |       623 | 958943 | 2018/09/02    |       12 |
    |       497 | 183490 | 2018/06/17    |       11 |
    |       648 | 958943 | 2018/11/13    |       25 |
    +-----------+--------+---------------+----------+


Then you have to connect these two tables with a One-to-Many relationship. 'Shipments 
Received' would be 'Many' side and the 'Parts' table would be the 'One' side. This brings
the table into compliance with 1NF and as well as the requirements peculiar to 2NF, 
making it fully compliant with Second Normal Form.
    
    
4.2.2.3 THIRD NORMAL FORM



    

****************************************************************************************************
****************************************************************************************************


SOURCES

01: Databases - A Beginner's Guide (Andy Oppel, 2009)
02: https://db-engines.com/en/article/RDBMS
03: https://www.calebcurry.com/beginner-database-terms/
04: https://www.techopedia.com/definition/25227/net-data-provider
05: https://stackoverflow.com/questions/25093452/difference-between-data-model-and-database-schema-in-dbms
06: https://stackoverflow.com/questions/36650268/the-difference-between-the-up-and-down-methods-in-the-migration-file
07: https://docs.microsoft.com/en-us/ef/core/managing-schemas/migrations/
08: https://en.wikipedia.org/wiki/Database_normalization
09: https://www.studytonight.com/dbms/database-normalization.php
10: https://www.techopedia.com/definition/19504/functional-dependency
11: https://stackoverflow.com/questions/4199444/functional-dependency-and-normalization