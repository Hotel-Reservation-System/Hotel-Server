1. DATABASE: CONCEPTS

This file will discuss database concepts that you need to set up and work with databases.
The next document, '2.4 Databases - Practical Applications', will discuss the 
installation and configuration of a database, followed by a section on how to connect a 
program to a database.

In this file, Section 2 will explain what databases are, and list some key things that 
you need to know about them. Section 3 is concerned with program-database interfacing: 
How does a program talk to a database? What role does an ORM library play in facilitating
this communication? 


    2.    Introduction to Database Concepts and Terms
    2.1   What is a Database?
    2.2   Relational Database Management Systems (RDBMS)
    2.3   Other Database Concepts and Terms to Learn Before Proceeding to Section 3
    
    3.    Application-To-Database Communication 
    3.1   Entity Framework Core: An Object-Relational Mapper (ORM)
    3.2   How Does Entity Framework Core Translate Data To and From the Database?
    3.3   Entity Framework Core Tooling: Nuget Packages and CLI Tools
    3.4   Entity Framework Core: Concepts and Components
    3.4.1 Database Providers
    3.4.2 The Program-side Representation of Database Entities: The Data Model vs. The Schema
    3.4.3 Migrations
    

****************************************************************************************************
****************************************************************************************************


2. INTRODUCTION TO DATABASE CONCEPTS AND TERMS

2.1 WHAT IS A DATABASE?

A database is an software system that stores data. A snippet I found on the internet 
describes databases like this: "A database is a computerised system that makes it easy to 
search, select and store information." Databases are used in institutions and businesses 
to store organizational data like customer records and profiles, sales transactions, 
product information etc. Computers, web browsers and phones have many built-in databases
for tracking information. However, storing data is not worthwhile unless the database is 
secure and well-organized, yet easily accessible to those who have authorization.

This is why a database program is rarely marketed as a standalone piece of software; 
instead most databases come with an ecosystem of additional tools. This collection of 
database and tooling is called a Database Management System (DBMS). A DBMS comes with 
tools to facilitate things like security rules and user access management, user 
authentication, search optimization, data management (Data integrity, encryption, 
compression, de-duplication, backup and recovery etc.), report generation etc. 

In 'Databases: A Beginner's Guide', Andy Oppel provides some definitions for terms 
related to databases[1]:
 
 
    1. Database Object (Page 4): "A database object is a named data structure that is 
       stored in a database. The specific types of database objects supported in a 
       database vary from vendor to vendor and from one database model to another."
       
    2. Database Model (Page 4): "Database model refers to the way in which a database 
       organizes its data to pattern the real world." 
       
       Historical database models include Flat File, Hierarchical, Network and the 
       Inverted file model. Some modern niche database models: Object-oriented, 
       Object-Relational, Graph, Document-oriented, and NewSQL.
    
       The most common modern database models are Relational and NoSQL.
    
    3. File (Page 4): A file is a collection of related records that are stored as a 
       single unit by an operating system. Given the unfortunately similar definitions of 
       files and databases, how can we make a distinction?… The answer lies in an 
       understanding of certain characteristics or properties that databases possess 
       which are not found in ordinary files, including the following:
    
           * Management by a database management system (DBMS)
           * Layers of data abstraction
           * Physical data independence
           * Logical data independence
    
    4. Database Management System (DBMS) (Page 5): The database management system (DBMS) 
       is software provided by the database vendor. Software products such as Microsoft 
       Access, Oracle, Microsoft SQL Server, Sybase ASE,DB2, Ingres, and MySQL are all 
       DBMSs…
    
       The DBMS provides all the basic services required to organize and maintain the 
       database, including the following:
    
       * Moves data to and from the physical data files as needed.
       * Manages concurrent data access by multiple users, including provisions to prevent 
         simultaneous updates from conflicting with one another.
       * Manages transactions so that each transaction’s database changes are an 
         all-or-nothing unit of work. In other words, if the transaction succeeds, all 
         database changes made by it are recorded in the database; if the transaction 
         fails, none of the changes it made are recorded in the database.
       * Supports a query language, which is a system of commands that a database user 
         employs to retrieve data from the database.
       * Provides provisions for backing up the database and recovering from failures.
       * Provides security mechanisms to prevent unauthorized data access and modification.


2.2 RELATIONAL DATABASE MANAGEMENT SYSTEMS (RDBMS)

Andy Oppel explains the Relational Model on pages 17-19[1]:


    In addition to complexity, the network and hierarchical database models share another 
    common problem—they are inflexible. You must follow the preconceived paths through 
    the data to process the data efficiently. Ad hoc queries, such as finding all the 
    orders shipped in a particular month, require scanning the entire database to locate 
    them all. Computer scientists were still looking for a better way…
    
    The relational model is based on the notion that any preconceived path through a 
    data structure is too restrictive a solution, especially in light of ever-increasing 
    demands to support ad hoc requests for information. Database users simply cannot 
    think of every possible use of the data before the database is created; therefore, 
    imposing predefined paths through the data merely creates a “data jail.” The 
    relational model allows users to relate records as needed rather than as predefined 
    when the records are first stored in the database. Moreover, the relational model 
    is constructed such that queries work with sets of data (for example, all the 
    customers who have an outstanding balance) rather than one record at a time, as 
    with the network and hierarchical models.
    
    The relational model presents data in familiar two-dimensional tables, much like 
    a spreadsheet does. Unlike a spreadsheet, the data is not necessarily stored in 
    tabular form and the model also permits combining (joining in relational terminology) 
    tables to form views, which are also presented as two-dimensional tables. In short, 
    it follows the ANSI/SPARC model and therefore provides healthy doses of physical and 
    logical data independence. Instead of linking related records together with physical 
    address pointers, as is done in the hierarchical and network models, a common data 
    item is stored in each table, just as was done in flat file systems…
    
    The elegant simplicity of the relational model and the ease with which people can 
    learn and understand it has been the main factor in its universal acceptance. The 
    relational model is the main focus of this book because it is ubiquitous in today’s 
    information technology systems and will likely remain so for many years to come.


In the Relational Model, two pieces of data are linked to each other with respect to the 
relationship they have to another. A table is a physical representation of a relation.

This article will only consider the most common kind of databases, the Relational 
Database Management System. Here is an explanation of RDBMSes[2]: 


    1. Relational Databases: Relational database management systems (RDBMS) support the 
       relational (=table-oriented) data model. The schema of a table (=relation schema) 
       is defined by the table name and a fixed number of attributes with fixed data 
       types. A record (=entity) corresponds to a row in the table and consists of the 
       values of each attribute. A relation thus consists of a set of uniform records.
    
       The table schemas are generated by normalization in the process of data modeling.
    
       Certain basic operations are defined on the relations:
    
       * classical set operations (union, intersection and difference)
       * Selection (selection of a subset of records according to certain filter criteria 
         for the attribute values)
       * Projection (selecting a subset of attributes / columns of the table)
       * Join: special conjunction of multiple tables as a combination of the Cartesian 
         product with selection and projection.
         
      These basic operations, as well as operations for creation, modification and 
      deletion of table schemas, operations for controlling transactions and user 
      management are performed by means of database languages​​, with SQL being a well 
      established standard for such languages.
    
      The first relational database management systems appeared on the market at the 
      beginning of the 1980s and since have been the most commonly used DBMS type.
    
      Over the years, many RDBMS have been expanded with non-relational concepts such as 
      user-defined data types, not atomic attributes, inheritance and hierarchies, which 
      is why they are sometimes referred to as object-relational DBMS.
      
      
Here is a list of well known RDBMSes:


    Oracle
    MySQL
    Microsoft SQL Server
    PostgreSQL
    DB2
    Microsoft Access
    SQLite
    Teradata
    MariaDB


The most popular open source databases as of October 2018 are: PostgreSQL and SQLite. 
Both are robust and capable. 

SQLite is designed to be a light-weight, server-less single file database. It's used in 
electronic devices like phones, PDAs, MP3 players as well as web browsers. If you need a
low footprint, high-quality database, go with SQLite.

PostgreSQL is a open source, fully featured RDBMS. It's more powerful than MySQL/MariDB,
and it has better data integrity. It will serve well as the database for web applications
and for heavier duty software.   
 
 
2.3 OTHER DATABASE CONCEPTS AND TERMS TO LEARN BEFORE PROCEEDING TO SECTION 3

There is much more to learn about databases before proceeding to Section 3. A full 
explanation of all these concepts is outside the scope of this document. For this reason, 
this section will merely list these concepts. You have to study them and familiarize 
yourself with them before proceeding to following sections. The following lists are mostly 
taken from Caleb Curry's website.[3]


1. SQL

You should know the basics of SQL, because this is the language of databases.
 

2. DATA & DATABASES

    * Data
    * Database
    * Relational Database
    * Database Management System
    * Relational Database Management System
    * Null
    * Anomalies
    * Referential Integrity 


3. DATABASE COMPONENTS

    * Entity
    * Attribute
    * Relation
    * Tuple
    * Table
    * Row
    * Column
    * File
    * Record
    * Field
    * Value
    * Entry
    * Database Design
    * Schema
    * Naming Conventions
    * Keys
    

4. CONCEPTUAL DATABASE DESIGN CONCEPTS

Andy Oppel explains 'Conceptual Database Design' on page 30: [1]

    … involves studying and modeling the data in a technology-independent manner. The 
    conceptual data model that results can be theoretically implemented on any database 
    or even on a flat file system. The person who performs conceptual database design 
    is often called a data modeler.

These are the concepts related to 'Conceptual Database Design':

    * Entity vs. Entity-Type
    * External Entities
    * Attributes
    * Cardinal Relationships (Inter-Table Relationships):
        * One-to-One Relationships
        * One-to-Many Relationships
        * Many-to-Many Relationship
    * Recursive Relationships 
    * Business Rules
    
    
5. LOGICAL & PHYSICAL DESIGN CONCEPTS

Andy Oppel explains these concepts on page 30: [1]

Logical Database Design:


    …is the process of translating, or mapping, the conceptual design into a logical 
    design that fits the chosen database model (relational, object-oriented, 
    object-relational, and so on). A specialist who performs logical database design is 
    called a database designer, but often the database administrator (DBA) performs all 
    or part of this design step.


Physical Database Design:

    The final design step is physical database design, which involves mapping the logical 
    design to one or more physical designs, each tailored to the particular DBMS that 
    will manage the database and the particular computer system on which the database 
    will run. The person who performs physical database design is usually the DBA.


These are the concepts related to Logical and Physical Database Design:

    * Tables
    * Columns and Data Types
    * Constraints
    * Primary Key Constraints
    * Foreign Key Constraints
    * Intersection Tables
    * NOT NULL Constraints
    * CHECK Constraints
    * Constraint Enforcement Using Triggers
    * Views
    * Database Normalization


6. ENTITY-RELATIONSHIP MODEL

In the context of a database, an entity–relationship model (ER model for short) describes 
relationships between instances of entities. These relationships are expressed using 
keys.


    * Keys (Primary, Foreign, Compound, Alternate etc.)
    
    
Learn and know the distinction between various types of keys.
 
 
****************************************************************************************************
****************************************************************************************************
   

3. APPLICATION-TO-DATABASE COMMUNICATION

3.1 ENTITY FRAMEWORK CORE: AN OBJECT-RELATIONAL MAPPER (ORM)

When you start up a program that stores data in a discrete database, as this one does, 
one of the first things the program will do is try and talk to the database. A database 
usually understands only the the SQL language. When you connect a database to a program 
written in C# or another general purpose programming language, you need an translator 
program/library to translate from SQL to C# and vice versa. A program that translates SQL 
to a general purpose programming language like Java, C# or JavaScript and vice versa is 
called an OBJECT-RELATIONAL MAPPER (ORM).

When the program runs, the ORM sits between the program and the database, translating C#
instructions into SQL and the reverse, as illustrated in this diagram:


                                          +---------+
                                          |         |
                                          | PROGRAM |
                                          |         |
                                          +----+----+
                                               ^
                                               |
                                               v
                                         +-----+-----+
                                         |           |
                                         | ENTITY    |
                                         | FRAMEWORK |
                                         | CORE      |
                                         |           |
                                         +-----+-----+
                                               ^
                                               |
                                               v
                                         +------------+
                                         |            |
                                         |  DATABASE  |
                                         |            |
                                         +------------+


Entity Framework Core (from here on in referred to as EFC or EF Core) is an ORM that sits 
between a app server like Hotel-Server and its database. When you hook it up to a data 
source, such as a database, an ORM reads in raw SQL data and turns it into collections of 
C# objects for use in the program. By doing this, EF Core is translating data as it is 
represented in the database into a data representation that C# programs would understand.
Entity Framework Core is the standard ORM solution in the .NET world, provided by 
Microsoft for .NET Core projects. 


3.2 HOW DOES ENTITY FRAMEWORK CORE TRANSLATE INFORMATION TO AND FROM THE DATABASE?

Consider this example table from a database, called Contacts:

+----+-----------+----------+----------------+
| Id | FirstName | LastName |  PhoneNumber   |
+----+-----------+----------+----------------+
|  1 | Bonnie    | Tsing    | (123) 456-7890 |
|  2 | Barry     | Forg     | (198) 928-9834 |
|  3 | Linda     | Blom     | (234) 239-9231 |
+----+-----------+----------+----------------+

The table contains 3 records. Each record has 4 fields. Each record contains Contact 
information for one person. This is how data is stored and represented in a database. 

There are other ways to represent this data. Take a look at the C# model class below: 


    public class Contact
    {
        [Required]
        public int Id { get; set; }
        
        [MinLength(2)] 
        [MaxLength(50)]
        public string FirstName { get; set; }
        
        [MinLength(2), MaxLength(50)]
        public string LastName { get; set; }

        [Phone]
        public string PhoneNumber { get; set; }
    }
    
    
The Contact class has a name similar to the table and it has four properties, but what is 
most interesting is that the property names - Id, FirstName, LastName and PhoneNumber - 
are the same as the column names in the Contacts table above. Do you see where this is 
going?

Put simply, records in a database can be converted into C# objects. The structure of a 
table in a database, such as the Contacts table above, can be represented in a C# program 
in the form of a C# POCO (Plain Old Class Object) model class. Columns in a table can be 
mapped to properties in a POCO class. The information in a record from the Contacts table 
can be represented in the form of a Contact object. To say it again, data contained in a 
record from a database table can transmuted into a form recognizable and usable in a C# 
program - a C# object.  

This is Entity Framework Core's job: to map records from tables in a database to a 
collection of C# objects and and the reverse. More specifically, EF Core acts as a 
translation layer between the Hotel Reservation System project and its database, 
translating C# code into SQL commands and C# objects into records in the database and 
vice versa, without writing any SQL or any database access code.


3.3 ENTITY FRAMEWORK CORE TOOLING: NUGET PACKAGES & CLI TOOLS

You'll need several nuget packages to add EF Core functionality to your project: 


    Microsoft.EntityFrameworkCore, 
    Microsoft.EntityFrameworkCore.Relational, and 
    Microsoft.EntityFrameworkCore.Tools (For Migrations. Tools for Package Manager Console.)
    
    
Fortunately, all these packages come with the Microsoft.AspNetCore.All metapackage, which 
appears to be mandatory for all ASP.NET Core projects. 

If you want to use the command line interface, which you will need while working with 
migrations, note that there are two sets of tools available:


    1. Cross Platform .NET Core Command-Line Interface (CLI) Tools
    2. Visual Studio's Package Manager Console Tools  


Due to the Visual Studio centric nature of the VSPMC tools, I will only provide commands
for the cross-platform tooling. .NET Core CLI tools come with the .NET Core SDK (for 
ASP.NET Core 2.1+ projects). With this set of CLI tools, most commands begin with 
'dotnet ef'.


3.4 ENTITY FRAMEWORK CORE: CONCEPTS AND COMPONENTS

Entity Framework Core has several major concepts and components you have to understand. 
You have to set up the first three things in this list to get EF Core to act as an ORM:


    1. DATABASE PROVIDERS: "Entity Framework Core can access many different databases 
       through plug-in libraries called database providers." By adding a database 
       provider designed to be consumed by Entity Framework Core to the App Server 
       project in your application, you will enable EF Core to talk to that particular 
       vendor's database.
    
    2. DATA MODEL: is the collection of Model classes in a program. A Model Class is a 
       POCO class: a class that contains only properties. Model classes define business 
       entities, which are objects on which the program performs operations. Business 
       entities are usually stored in the database, which is why EF Core needs model 
       classes: To understand the C# representation of a business entity when it talks to
       the database. 
         
    3. THE SCHEMA: (aka the Context/DbContext class) A Context class defines the schema 
       for a database by extending the Data Model. In it, the programmer declares all the 
       tables in a database as DbSet properties. Then, these tables have their columns 
       declared, followed by constraints associated with each column. Finally, the schema 
       declares inter-table relationships (One-to-One, One-to-Many, Many-to-Many). 
       
       It's worth noting that the DbSet data structure used in the Schema almost qualifies
       as a separate concept. Make sure you understand how they work. 
    
    4. MIGRATIONS: are version control for the schema. They track changes to the 
       database's topology during the lifetime of the software project. 
     

Here they are listed again, this time with information on where in the program they are
defined:    


     1) Database Provider,          (Defined in Data/MyDbContextFactory.cs)
     2) Data Model,                 (Defined in the Models folder of the Common project)
     3) Database Schema,            (Defined in Data/Context.cs)
     4) Migrations                  (Generated into the Data/Migrations Folder)


The next three sections explain these concepts in greater detail.


3.4.1 DATABASE PROVIDERS
 
A Database Provider is a "a software library consisting of classes that provide data 
access services such as connecting to a data source, executing commands at a data source 
and fetching data from a data source with support to execute commands within 
transactions. It resides as a lightweight layer between data source and code, providing 
data access services with increased performance."[4] 

A DATABASE PROVIDER is a small library that helps EF Core to speak to databases from a 
certain vendor. For instance, there are Database Providers for SQL Server, SQLite, 
PostgreSQL and other databases. EF Core can talk to any database if it has the database 
provider library for it. When Hotel-Server is run, Entity Framework will be brought 
online to talk to the database. The first thing EFC will do is search for the Database 
Provider. 

Therefore, you need to find the right database provider for the database in your project 
and add it to the program via a Nuget package. The Database Provider for this project is 
the 'Npgsql.EntityFrameworkCore.PostgreSQL' package. By using this package, EF Core will 
come to know that this project uses a PostgreSQL database. Through it, EFC can interface 
with the database and carry out operations on it. 

Once it can talk to the database, EF Core will will use the Data Model to transform a 
SQL query into an object model representation of a query (which is called a canonical 
command tree) and vice versa. .NET Database Providers can consume canonical command trees 
to talk the database.

The Data Provider, connection string and other related features can be configured in: 


    * Data/MyDbContextFactory.cs, OR
    * ConfigureServices() method of Startup.cs 
    

To learn more about database providers, go to: https://docs.microsoft.com/en-us/ef/core/providers/


3.4.2 THE PROGRAM-SIDE REPRESENTATION OF DATABASE ENTITIES: THE DATA MODEL vs. THE SCHEMA

Once EF Core confirms that it can talk to the database via the database provider, it 
will need the Data Model and Schema of the database. What are those things and why does 
your database need them? Recall that it is EFC's job to do two-way translations between 
your program and the database. This involves mapping tables and other databases entities 
to C# objects and vice versa. To do this, EF Core must have a program-side representation 
of entities your program will want to store in the database. This includes entity and 
attribute names, and their relationships expressed in C# code. The Data Model and the
Schema are precisely that: representations of database entities and their relationships. 

A short discussion is in order on when and how the data model and schema is to be 
written. When the programmer first starts writing an ASP.NET Core program, she typically
starts by defining the entities in that project. These entities are usually defined as
POCO classes in the 'Models' folder of your solution. In the eyes of EFC, these model 
classes are the Data Model. When preparing a greenfield database for your program, you 
have to write the schema for it. To write the schema, the programmer will have to 
consult the Data Model.

Let's move on to studying these two things. The Data Model is distinct from a Database 
Schema; see this StackOverflow Answer from richik jaiswal to learn the difference[5]:


     A schema is a blueprint of the database which specifies what fields will be present 
     and what their types will be. For example an employee table will have an employee_ID 
     column represented by a string of 10 digits and an employee_Name column with a 
     string of 45 characters.
     
     Data model is a high level design implementation which decides what can be present 
     in the schema. It provides a database user with a conceptual framework in which we 
     specify the database requirements of the database user and the structure of the 
     database to fulfill these requirements.
     
     A data model can, for example, be a relational model where the data will be 
     organised in tables whereas the schema for this model would be the set of attributes 
     and their corresponding domains.


A Data Model is an abstract formalization of a database's entities. The basic properties 
of database entities like tables and columns are defined as C# model classes. The model 
classes in the 'Models' or 'Entities' folder of your project form the DATA MODEL for your 
database. 

For an example, let's take another look at the Contact class from section 3.2:


    public class Contact
    {
        [Required]
        public int Id { get; set; }
        
        [MinLength(2)] 
        [MaxLength(50)]
        public string FirstName { get; set; }
        
        [MinLength(2), MaxLength(50)]
        public string LastName { get; set; }

        [Phone]
        public string PhoneNumber { get; set; }
    }


The Contact class is a data model class: it's serves as very abstract, high-level C# 
representation of the Contacts table in your database. When EFC tries to talk to the 
Contacts table in your database it will first consult its C# counterpart, the Contacts 
data model class, to learn the topology of this entity. 

Note that the properties of this class have data annotation attributes on them. These 
annotations mandate values for the 'Id' property and establish value minimums and 
maximums on the 'FirstName' and 'LastName' properties. As the data model informs the 
schema, when the schema is written, these restrictions will carry over to it. In fact,
when the programmer first writes the schema for a new database, she usually derives it 
from the data model. 

Go to this location to see the model classes for this project.


    /Common/Models/Tables


Let's move on to discussing the schema. The DATABASE SCHEMA is a more specific definition 
of a database's entities and their attributes. It serves as a snapshot of the database's 
topology at the current point in time. The schema takes entities defined in the data 
model, plus any data annotation attributes defined therein, and adds other things to it. 
This may include restrictions on character length for a field, marking certain fields as 
mandatory when a record is created, and establishing the nature of relationships between 
entities. 

For new databases, the schema has to be written by the programmer. However, if a database 
already exists, you can join it to an ASP.NET Core project and then generate a schema 
from it. 

Let's take a look at what a schema looks like. First, here's the Hotel model class (from 
the Models folder): 


    public class Hotel : IObjectWithState
    {
        // ORMs (Object-Relational Mapper) map properties in an object to a table. The 
        // ObjectState property is not a table column; it is used to track changes in an 
        // entity. This will likely happen in the endpoints of this project.
        [NotMapped]
        public ObjectState State { get; set; }
        
        
        // The primary key of the Hotel table in the Database is the Room Number.
        [Key]
        public long Id { get; set; } 
        public string Name { get; set; }
        public string Address { get; set; }
        public string PhoneNumber { get; set; }

        // Hotel Objects have a HotelRooms property to track the list of hotelrooms at a
        // hotel without needing to query the database.
        public virtual IEnumerable<HotelRoom> HotelRooms { get; set; }
    }
    

There are schemas for at least three levels of a database: physical schema (how data 
blocks are stored at the lowest level; this is where database designers work), logical 
schema (how records get stored in data structures; this level is where programmers and DB 
administrators work) and view schemas (which are schemas for end-user interaction). 

Taken from Context.cs, here is the corresponding Schema for a Hotel table:


    public class Context : DbContext
    {
        public virtual DbSet<Hotel> Hotels { get; set; }

        protected override void OnModelCreating(ModelBuilder modelBuilder)
        {
            modelBuilder.Entity<Hotel>(entity =>
            {
                entity.HasKey(e => e.Id);
                entity.Property(e => e.Id).IsRequired().ValueGeneratedOnAdd();
                entity.Property(e => e.Name).IsRequired().HasMaxLength(100);
                entity.Property(e => e.Address).IsRequired().HasMaxLength(200);
                entity.Property(e => e.PhoneNumber).IsRequired().HasMaxLength(100);
            });
        }
    }
        

When any ASP.NET Core program runs normally, EF Core will be sitting between the program 
and its database, translating SQL and database records into C# commands and objects. 
Provided it has the data model and the schema, EF Core has a map to the layout of any 
particular database.

Let's talk about the schema for the Hotel table; EF Core is going to use it to 
translate entities from the database into C# objects. When EF Core converts information 
from a data source into C# objects it needs two things: a pre-defined C# model class that 
can represent data from each table and a collection to hold these model objects. 

Let's talk about the Hotels DbSet<Hotel> data structure first. DbSet<Hotel> is a generic 
data structure that can contain only Hotel objects. This is the first thing that is 
declared in the schema: a data structure to hold Hotel objects as they get converted from 
records, called Hotels.

The purpose of a Schema is to define the specific shape and form of the entities in a 
table. Following the declaration of Hotels DbSet, comes the OnModelCreating() method. 
This method is overridden and the schema is defined within; it lays out primary keys for 
tables and Navigation properties (foreign keys) between entities. Lastly, we can also 
define field constraints in this method, which are specifications and limitations for 
every column in the Hotel Table. For instance, HasKey() denotes the Id field as the 
Primary Key of a table. The other properties are defined with the Property() method. The 
Id, Name, Address and PhoneNumber fields are marked as required fields. The character 
lengths of each field are also specified. 

As EF Core converts every record into a new object in a DbSet<T> collection or vice 
versa, it makes sure that all these requirements are met. The fields of each record in 
the table are copied and assigned to the properties of a new C# object. Once a new Hotel 
object is populated with data from a record, it gets stored in the Hotels collection.

In case it was not clear, the schema for an ASP.NET Core solution can usually be found 
in:

    /Data/Context.cs


3.4.3 MIGRATIONS

3.4.3.1 WHAT ARE MIGRATION? WHY DO WE NEED THEM?

During the course of developing a software solution, your program and its database will
grow and change. This section will focus on managing change in databases.

Recall that the Schema is a more specific map to the database than the Data Model. The 
schema of a database defines tables, specifies columns and their types, and the 
relationships between tables, i.e. it concretely describes the structure and topology of
the database. It also imposes constraints on data that seeks to enter the database.

However, a database is not a static thing; it changes with its program. In other words, 
during the lifetime of a database, its topology is in constant flux. In order to make any 
changes to fundamental entities in a database or their relationships, the programmer 
will have to modify the schema. Tables and attributes can be modified, added or deleted
by making changes to the schema. Corresponding changes will also have to be made to the
Data model classes.  

Changes to a database are complicated by the fact that organizations usually have 
multiple instances of a database. For instance, a company may have a development 
environment where software is developed, a production environment where stable software 
runs and perhaps a demo environment for demonstrating products to clients. If the 
development team makes changes to the master database, how are they going to roll out 
changes to all instances of the database? Databases change often enough that their Schema 
needs version control. 

A schema defines the topology of a database at one point in time. Usually, the schema 
only contains the current topology of the database. However, it would be very useful to 
have snapshots of the schema every time it was changed. This is where SCHEMA MIGRATIONS 
(often called MIGRATIONS) come in. Schema snapshots are called MIGRATIONS and they serve 
as version control for a database's schema. 

To track changes to the schema, you can generate a migrations file. Migration files 
contain only delta values, i.e. only changes from the previous migration file. Your 
program usually has one active schema file, but many migrations files. You can use 
migrations to update to or rollback from a version of the database's schema. A migration 
file will  have a way to revert its own changes. It may also contain scripts to enact 
transformations and to insert default data into the database by populating tables and 
columns.

To ensure data integrity, it is a best practice to take a snapshot of a database's 
topology every time a change is made to the schema. When you first write the schema for a 
new database, you should generate a migration file that contains the entire schema. 
Subsequently, every time you change entities or their relationships by editing the 
schema, you will want to generate a new migration file that contains the new, updated 
topology of the database. Once again, do note that the new migration file only contains 
delta values, i.e. only changes from the previous migration file.

Migrations are constantly generated in the development stage of an application, because 
that's when the schema is changing the most. Large architectural changes and feature 
churn necessitate frequent alterations to the database's schema. Migrations are also 
needed when upgrading a database, switching to another database vendor, or moving a 
database into the cloud. Having a version controlled history of the database's schema 
(which captures the changes made to the database over time) gives you the ability to 
migrate the database to older or newer versions of the schema. Migrations are a 
convenient way to track your database's schema over time in a predictable and 
consistent way. By using migrations, you can update all instances of a database 
automatically and reliably to the schema version of your choice.
 
Typical location of migration files: Data/Migrations/ 


3.4.3.2 THE CONTENTS OF A MIGRATIONS FILE

Every migration file will contain an Up() and Down() method. The Up() method contains 
transformations to the schema of the Database. The Down() method just rolls back 
transformations in the Up() method i.e. it returns the schema to the state in the Up() 
method of the previous migration. The database schema should be unchanged if a call to 
Up() is followed by a Down(). If another, new migration file has been generated and 
needs to be applied to the database, call its Up() method. To revert these 
transformations, call its Down() method. 

The final set of transformations in a long chain of such changes is described in the 
Up() method of the last migration. The current state of a database's schema is the 
result of calling the Up() methods of all previous Migrations. 

Here is a answer from StackOverflow that expands on migrations[6]:

    DB Migrations make changes to your database to reflect the changes made to the Entity 
    Framework Model. These changes are added to the database with the Up method.
    
    When you want to rollback a change (e.g. by rolling back a changeset in TFS or Git), 
    the changes in the database have to be rolled back also because otherwise your Entity 
    Framework model is out-of-sync with the database. This is what the Down method is 
    for. It undo's all changes to the database that were done when the Up method was run 
    against the database.
    
    The Seed method gives you the ability to Insert, Update or Delete data which is 
    sometimes needed when you change the model of the database. So the Seed method is 
    optional and only necessary when you need to modify existing data or add new data to 
    make the model working.
    
    - Ric .Net    
    
Leaving aside the Seed method, which appears to from another framework, let's take a look 
at a migrations file. Taken from the file called, 
'20180128032156_CreateHotelHotelRoomRoomTypeBedTypeRoomReservationTables.cs', this is the 
Logical schema for the Hotel table: 


    public partial class CreateHotelHotelRoomRoomTypeBedTypeRoomReservationTables : Migration
    {
        protected override void Up(MigrationBuilder migrationBuilder)
        {
            migrationBuilder.CreateTable(
                name: "Hotels",
                columns: table => new
                {
                    Id = table.Column<long>(type: "int8", nullable: false)
                        .Annotation("Npgsql:ValueGenerationStrategy",
                                     NpgsqlValueGenerationStrategy.SerialColumn),
                    Address = table.Column<string>(type: "varchar(200)",
                                                   maxLength: 200,
                                                   nullable: false),
                    Name = table.Column<string>(type: "varchar(100)",
                                                maxLength: 100,
                                                nullable: false),
                    PhoneNumber = table.Column<string>(type: "varchar(100)",
                                                       maxLength: 100,
                                                       nullable: false)
                },
                constraints: table =>
                {
                    table.PrimaryKey("PK_Hotels", x => x.Id);
                });
        }
    }
    

3.4.3.3 GENERATING, APPLYING AND REMOVING MIGRATIONS
    
What happens when a migration is generated? When the command is given, it appears that 
Entity Framework Core combines the existing Schema with data from Model classes to 
generate a migration file. Generating a Migration will create a new file in the 
Migrations folder. It will contain a a user-named class that is a subclass of Entity 
Framework Core's Migration class. You may need to manually remove things from either 
method if it added more than the changes you made to the Schema. Make sure you read the 
migration file before applying it.

Migrations are generated or applied with CLI commands. Note that there are two sets of 
CLI tools provided by Microsoft, one that is Visual-Studio only and the other is cross-
platform tooling that comes with every installation of the .NET Core SDK. The

The commands given below are for the cross-platform CLI tools only. You can issue these
commands by switching to the 'Terminal' tab in JetBrains Rider. Check the Entity 
Framework Core docs for more commands.


    1. GENERATING MIGRATIONS: Creates a new migration. The migration name should describe 
       the changes you made to the data model in pascal case (E.g. "PascalCaseExample"). 
    
       Command: dotnet ef migrations add <Migration-Name>
       
       E.g. 1: dotnet ef migrations add InitialMigration
       E.g. 2: dotnet ef migrations add AddUserUserPreferenceUserInformationRoleUserRoleTables
    
       According to Microsoft Docs' article on Migrations[7], the following things happen
       when you execute this command: 
       
           Three files are added to your project under the Migrations directory:
           
           * 00000000000000_InitialCreate.cs--The main migrations file. Contains the 
             operations necessary to apply the migration (in Up()) and to revert it 
             (in Down()).
             
           * 00000000000000_InitialCreate.Designer.cs--The migrations metadata file. 
             Contains information used by EF.
             
           * MyContextModelSnapshot.cs--A snapshot of your current model. Used to 
             determine what changed when adding the next migration.
    
    
    2. UPDATING DATABASE TO THE LATEST MIGRATION: Runs the Up() method of all migrations. 
    
       Command: dotnet ef database update 


       VARIATION 1: UPDATE DATABASE TO A SPECIFIED MIGRATION
       
       Command: dotnet ef database update <Migration-Name>
       
       E.g. 1: dotnet ef database update 20180904195021_InitialCreate
       
    
    3. REVERT THE LAST MIGRATION: Rolls back the last applied migration.
    
       Command: dotnet ef migrations remove [-f]  [--force]
       
    4. LIST AVAILABLE MIGRATIONS.
    
       Command: dotnet ef migrations list
        

There are many more commands available. These commands are not restricted to migrations
either. There are commands to drop databases, query for DbContext information, scaffold a
DbContext from an existing database and more. 

    
3.4.3.4 MIGRATIONS: THE WORKFLOW 

This is the workflow for changing the schema and generating migrations:


In the Development Environment:

    1. When you need to add tables or modify relations in the database, edit the Schema 
       in Context.cs.
    2. Because the Schema has been modified, you need to generate a new migrations file.
    3. You run the migration file in the development environment to verify that it works 
       as intended. 


In the Production, Demo and other Environments:
    
    4. Deploy and run the migrations file in other environments to update these instances 
       of the database to the latest version of the schema.
    5. Verify that migration has gone smoothly in these environments as well.
    

****************************************************************************************************
****************************************************************************************************
    
4. DATABASE NORMALIZATION

4.1 THE CONSEQUENCES OF POOR DATABASE DESIGN 

To keep it short, our natural instincts for creating tables and storing data in a 
database typically lead to terrible database designs. Poorly designed databases are prone 
to data redundancies or database anomalies. Redundant data is self-explanatory, so what 
are database anomalies? An anomaly is an aberrant behaviour in a database that occurs due 
to poor design. There are three kinds of database anomalies:


    * INSERTION ANOMALY: is a situation that occurs when you cannot insert a new record 
      into a table because the table requires a piece of data that is unavailable to you. 
      
      E.g.: A database that cannot add a customer information to its Customers table 
      until the customer actually makes a purchase.
      
      
    * DELETION ANOMALY: is the opposite of the Insert anomaly. This is a situation where
      the deletion of one entity/record causes unintended deletion of another entity or
      record. Deletion anomalies result in unexpected data loss.
      
      E.g.: Using the previous example, a deletion anomaly occurs if we delete a customer 
      invoice, which due to poor design, causes the deletion of that customer's 
      information. As in the insertion example, this happened because information about 
      two entities, namely customers and invoices, was incorrectly combined into a single
      table. 
      
      
    * UPDATE ANOMALY: arises when the same information can be expressed on multiple rows 
      of a table. This makes it possible for the same information to exist in multiple
      records. If a table design permits this error, then over time, it will almost 
      certainly lead to data inconsistencies when one record is updated but the other one 
      is not.
      
      E.g.: Suppose you have a Student table where its possible to have a student's 
      information stored in more than one record. If one of these records is updated, 
      say the address is changed, you now have two records for one student, with 
      conflicting addresses. This is a clear sign of data redundancy design error in your 
      table.
      
      Your tables should have only a single source of truth for an attribute. Attributes 
      should be captured once and stored once in one field only. If needed, you can make 
      references to it in other tables. All references must point to this single, 
      original source.


The best way to avoid anomalies is to normalize your database. This is a reference to a 
practice called 'Database Normalization'. 


4.2 NORMALIZATION

Here is an introductory description of Normalization from Wikipedia's article[8]:


    Database normalization is the process of restructuring a relational database in 
    accordance with a series of so-called normal forms in order to reduce data redundancy 
    and improve data integrity. It was first proposed by Edgar F. Codd as an integral 
    part of his relational model.
    
    Normalization entails organizing the columns (attributes) and tables (relations) of a 
    database to ensure that their dependencies are properly enforced by database 
    integrity constraints. It is accomplished by applying some formal rules either by a 
    process of synthesis (creating a new database design) or decomposition (improving an 
    existing database design).


Here's an alternate definition[9]:


    Database Normalization is a technique of organizing the data in the database. 
    Normalization is a systematic approach of decomposing tables to eliminate data 
    redundancy (repetition) and undesirable characteristics like Insertion, Update and 
    Deletion Anomalies. It is a multi-step process that puts data into tabular form, 
    [and] removing duplicated data from the relation tables.
    
    Normalization is used for mainly two purposes,
    
    * Eliminating redundant data, AND
    * Ensuring data dependencies make sense i.e data is logically stored.
    

What are some of the advantages of normalizing your database?[10]:


    * Eliminate data redundancies (and therefore use less space)
    * Make it easier to make changes to data, and avoid anomalies when doing so
    * Make referential integrity constraints easier to enforce
    * Produce an easily comprehensible structure that closely resembles the situation the 
      data represents, and allows for growth
       
    
When you design your database, you will have to do something called 'Entity-Relationship 
Modelling'. This is the process of planning out tables in your database and their
relationships to other tables. ER Modelling is dominated by "big picture" consideration 
of your program's data needs. Based on your conclusions, you should iteratively identify 
the relevant entities (tables) in your database, their attributes (columns) and their 
relationships. 

Once you produced a tentative database design, you should have some idea of what tables 
you will have in your database, their names, their attributes and their relationships to 
other tables. Designing inter-table relationships means that you should have decided on 
primary keys for most of your tables.

Following the ER Modelling process, you will then apply the process of 'Database 
Normalization'. Normalization is applied at the "micro level"; you will focus on one 
entity (table) and consider its attributes and other characteristics. The general idea
behind normalization is that a table is about a specific topic and that only columns
related to the topic must be included in the table. Our natural instincts at table design 
typically lead to tables that contain duplicate data. The other big problem is that poor
table design results in tables that fail to take advantage of the database's ability to
check and impose data integrity constraints. This can lead to wrongly typed data, 
multi-field values and other types of data integrity problems.

You should normalize every database you design. You should do normalization as part of
the initial design and whenever you revisit and tweak your database's schema. The process
of normalizing your database often results in the creation of new tables.

Normalization will make your database easier and more reliable to work with. 
Normalization will reduce unwanted duplicate data in the database, thereby decreasing its
size and increasing integrity and robustness of the data stored therein. This will make 
it harder to create invalid states in the database or enter poor quality data into it. 
This should make it easier to maintain, edit and modify your database.  


4.2.1 PRE-REQUISITE CONCEPTS

4.2.1.1 DATA & DATA DEPENDENCIES IN DATABASES

The you store data in a database, you usually store it in a relation, which is another 
way of referring to a table. Take a look at the example table below, called 'Students':


    +-----------+-----------+----------+----------------+--------------------------+
    | StudentId | FirstName | LastName |  PhoneNumber   |          Email           |
    +-----------+-----------+----------+----------------+--------------------------+
    |    293843 | Harold    | Kumar    | (298) 239-3894 | haroldkumar@yahoo.com    |
    |    293494 | Jenny     | Spice    | (298) 830-9238 | j_luvbabyspice@yahoo.com |
    |    293939 | Jacob     | Yycomb   | (298) 832-2398 | jacob.yycomb@gmail.com   |
    |    394853 | Victoria  | Gillium  | (298) 382-3982 | vgill@gmx.com            |
    +-----------+-----------+----------+----------------+--------------------------+


The table above is designed to capture information about students. This is done with 
attributes, also called columns, which are the names of various properties that an entity 
might have. Columns are listed horizontally at the top of a table. The 'Students' table 
has 5 attributes: 'StudentId', 'FirstName', 'LastName', 'PhoneNumber', and 'Email'. Each
attribute captures certain piece of information about a student. All information 
pertaining to an attribute have to be stored in its column. For instance, you cannot 
store StudentId information in the 'PhoneNumber'; this a constraint that is enforced by 
the database. 

A Row or a Record, refers to a horizontal block that contains information for one student.
The first record in this table, that of Harold Kumar, contains information required by 
the five attributes. Similarly, this table is filled with records, each one corresponding 
to one student. Or at least, that should be the case, if there are no duplicates in this 
table. 

Within this table, attributes have certain relationships to each other. The process of 
database normalization requires you to understand and formalize these data relationships. 
Once you understand the nature of the relationships between attributes, you can re-design 
your tables by moving extraneous information to new tables. Then, you can structure your 
tables in such a way that the database can enforce data types, referential and integrity
constraints. By doing this, you can leverage the database to prevent data redundancies
and database anomalies. 

The first step to doing that is to understand the nature of dependencies in your data.


FUNCTIONAL DEPENDENCY

To normalize tables, you have to understand the concept of 'Dependency' and its many
variants. In the most general terms[11], "a dependency is a constraint that applies to or 
defines the relationship between attributes." This is the same thing as a 'Functional 
Dependency'. Usually, but not always, this is referring to the relationship between a 
Primary Key (PK) attribute and a second non-key attribute within a table. 

This is the definition of a functional dependency: 

For two arbitrary attributes, A and B, Attribute B is said to be functionally dependent 
on Attribute A, if at any moment in time there is exactly one instance of B for any given 
instance of A. This is a rather formal explanation, but it is again phrased another way: 
If Attribute A functionally determines Attribute B, then the value of an instance of A 
uniquely determines a value for B. 

Let's move on to a less formal description: If A determines B, then a value of A can 
uniquely identify a value of B. A determinant is an attribute that determines the value 
of another attribute. Therefore, A is a determinant of B and B is a dependent of A.  This 
relationship is visually represented like this:

    
    A -> B 


It is read like this: "A (functionally) determines B" OR "A derives B."
The Inverse: "B depends on A."

Let's take a concrete example[12]: In a table, suppose there are two columns with a 
functionally dependent relationship: 'Social Security Number' and 'EmployeeName'. The 
'EmployeeName' attribute is functionally dependent on the 'SSN' attribute because an SSN 
uniquely identifies an employee. Note that the reverse does not hold true; names do not 
uniquely identify a SSN. This makes sense because a Social Security Number is issued to 
every person in the country to uniquely identify them, even when there are multiple 
people with the same name. If there were other non-key attributes in the table, for 
instance 'EmployeeAddress' and 'BirthDate', the 'SSN' attribute could functionally 
determine them too.

Why is this important? 

In a StackOverflow answer, NealB explains[13]:


    Sets of functional dependencies may be used to synthesize relations (tables). The 
    definition of the first 3 normal forms, including Boyce Codd Normal Form (BCNF) is 
    stated in terms of how a given set of relations represent functional dependencies. 
    Fourth and fifth normal forms involve Multi-Valued dependencies (another kettle of 
    fish).
 
 
Normalization applies what are called 'Normal Forms' to tables. The second and third 
normal forms as well as the Boyce-Codd Normal Form (which is considered to be 3.5 Normal
Form) require understanding of functional dependencies. 

There are several types of dependencies, which are discussed below:

TRIVIAL FUNCTIONAL DEPENDENCIES

A trivial functional dependency is an obvious dependency. For instance, consider a 
primary key composed of two attributes, 'Social Security Number' and 'EmployeeName'. Such 
a key is called a composite key. By searching for both attributes, the composite key can 
uniquely identify the name of an employee in the 'EmployeeName' column. The relationship 
is expressed like this:


    {SSN, EmployeeName} -> EmployeeName
    
 
The composite key can uniquely identify a 'EmployeeName' because this attribute is a part
of the composite key. If it seems obvious, it is because it is. A trivial dependency 
occurs when the dependant attribute is a part of the determinant attribute. Generically,
it is expressed as follows:


    {A, B} -> B 



4.2.1.2 PRIME AND NON-PRIME ATTRIBUTES

A candidate key is any attribute or combination of attributes that can determine 
(uniquely identify) a record. There may be several candidate keys in a table, of which
one is usually chosen to a primary key. That they are candidates to be a primary key is 
the reason for their name. 

Attributes (columns) that are part of a candidate key are called PRIME ATTRIBUTES or KEY
ATTRIBUTES. The other keys in that table are called NON-PRIME ATTRIBUTES or NON-KEY
ATTRIBUTES.


4.2.2 THE NORMAL FORMS

The Normal Forms were developed by Edgar Codd, the father of relational databases. 
Normalization theory defines 6 major Normal Forms and a number of minor forms. The first
three Normal Forms are treated like an essential block. If you bring your database into
compliance with the first three forms, you will be protected against well over 90% of 
redundancy and data integrity issues in most databases.   

Therefore, you should apply the first three forms to all databases. Following that, you 
should consider the Boyce-Codd Normal Form (BCNF or 3.5NF), which is considered to be a 
minor extension to the Third Normal Form. The fourth and fifth forms come into play only 
very rarely. The sixth form seems to be treated like a theoretical oddity.  


4.2.2.0 UNNORMALIZED FORM (UNF or 0NF)

This is the state of a table or database before you have applied the Normal Forms. An 
unnormalized database will suffer from data redundancy and data integrity issues due to
their violation of the Normal Forms. 

The Forms are mainly properties of a relation. In database terminology, a relation is 
equivalent to a table. Therefore, you have to scrutinize every table in your database to
ensure that each of the forms is applied. 

    
4.2.2.1 FIRST NORMAL FORM (1NF)
   
These are the criteria of First Normal Form:


    * To eliminate anomalies and data redundancy, create a separate table for each set of
      related data. You should define a primary key in each table to uniquely identify 
      records. 
      
    * An attribute is a vertical column in a table. All columns in a table must have 
      unique names. All values in a column must be of the same domain (data type). 
      
    * In a relation, a field is the intersection of a row and a column. It is the most 
      fundamental unit of storage in a database. You can store a data fragment in it.
      This piece of data must be atomic, i.e. a piece of data that cannot or should not 
      be divided into smaller pieces. (For example of the latter case, you can divide 
      date values into separate year, month and date columns, but this is widely regarded 
      as a poor idea.) 
      
    * A field must, at most, contain only one piece of data. 1NF prohibits multi-value 
      fields. This applies to all fields in all tables. Do not use commas, semi-colons or 
      other delimiters to put multiple values into a field. Databases treat multi-value
      fields as a single value. 
      
      Applying 1NF lets the database track, store and display data in a tabular format. 
      Breaking 1NF will mean that the database is no longer tracking the extra values in
      multi-value fields. This will prevent the database from applying data integrity 
      constraints to them. This will lead to an increase in the amount of low-quality 
      or conflicting data in your database.
    
    * Eliminate repeating groups from tables. Repeating groups are columns with the same
      name, but different numbers at the end. They are a special case of multi-value 
      fields. They usually occur when you try to get rid of multi-value fields by 
      creating new columns. The best way to eliminate repeating groups is to study the 
      table, find and identify the set of data related to repeating groups and move them 
      into a new table. 
          
    
This principle is hard to understand in the abstract, so consider the table below. It is
an 'Employee' table, which contains a list of employees, their Ids, names, emails etc. 
Each employee may be issued corporate devices like laptops and phones, whose serials need 
to be tracked. This will be done in the last column, which is called 'DeviceSerial'.


    +------------+-----------+----------+-----------------------+--------------+
    | EmployeeId | FirstName | LastName |         Email         | DeviceSerial |
    +------------+-----------+----------+-----------------------+--------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D |
    |       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 |
    |       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8 |
    +------------+-----------+----------+-----------------------+--------------+


So far, this table lets the company track the devices they given to employees and as a
bonus, complies with 1NF. However, what happens if employees are given more than one 
device? The easiest solution to the problem is to add additional devices to the 
DeviceSerial column, separated by commas and other delimiters. This is what it would look
like:


+------------+-----------+----------+-----------------------+--------------------------------------------+
| EmployeeId | FirstName | LastName |         Email         |                DeviceSerial                |
+------------+-----------+----------+-----------------------+--------------------------------------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D, SD8GD8DFHFH0                 |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 | RIQO98L239DS | 0J78G8J8G7GJ |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8                               |
+------------+-----------+----------+-----------------------+--------------------------------------------+


However, this is something you should NEVER do in a database. It is a violation of 1NF. 
Relational Databases can handle many things, but they are not equipped to handle 
multi-value fields. Multiple values in a field get treated like a single value. Searching 
for data becomes extremely difficult in a database that fails 1NF compliance. The bigger 
problem is that this practice retards a database's full abilities to categorize and 
manage data. If you use multi-value fields, you are now responsible for enforcing data 
types, and other data integrity constraints for potentially millions of records. So, 
don't go there. Let your database manage the data and enforce types and constraints. All 
you have to do to offload this responsibility to the database is put a maximum of one 
value in a field.  

However, we still have multiple values to track. How else can you track them? The next 
obvious solution is to add additional columns for devices, so that you can track one 
device per column. See the table below:


+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
| EmployeeId | FirstName | LastName |         Email         | DeviceSerial1 | DeviceSerial2 | DeviceSerial3 |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D  | SD8GD8DFHFH0  |               |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4  | RIQO98L239DS  | 0J78G8J8G7GJ  |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8  |               |               |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+


Unfortunately, this is also something you should NEVER do, because this too violates 
First Normal Form. Creating multiple columns in the vein of DeviceSerial1, DeviceSerial2,
DeviceSerial3 etc., is called a REPEATING GROUP. The existence of repeating groups also
violates First Normal Form. The classic sign of a repeating group is identical column
names with different numbers tacked on the end of it to make unique names. 

Repeating groups are a sign of inflexible design. What happens when employees from the 
quality control department need to be given 10 different devices each? Repeating groups 
do not scale well. Giving an employee a new device should not necessitate changes to a 
database's schema. 

No, the correct response to such a problem is to create a new table that tracks devices,
with 'EmployeeId' as foreign key. You can move the 'DeviceSerial' column to this new 
table and add new ones, like 'DeviceType' and 'Description' to help the business better 
track its devices. After doing this, you can establish a one-to-many relationship between 
the 'Employee' and 'Devices' table, where the 'One' side is the 'Employee' table and the 
'Many' side being the 'Devices' table. This is the modified 'Employees' table may look 
like:

    
    +------------+-----------+----------+-----------------------+
    | EmployeeId | FirstName | LastName |         Email         |
    +------------+-----------+----------+-----------------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com |
    |       0002 | Jill      | Hill     | jhill@gmail.com       |
    |       0003 | Simon     | Says     | ss@outlook.com        |
    +------------+-----------+----------+-----------------------+


This is what the 'Devices' table might look like: 

    
    +--------------+------------+------------+-----------------------+
    | DeviceSerial | EmployeeId | DeviceType |      Description      |
    +--------------+------------+------------+-----------------------+
    | AD89AFLL35L4 |       0001 | Iphone     | 5th gen Apple iPhone. |
    | AD9F8AFAF98D |       0003 | Printer    | HP Printer #2839      |
    | SDF90AF87GG8 |       0003 | Laptop     | Dell Latitude 910     |
    | 0J78G8J8G7GJ |       0002 | Camera     | Canon XTS Pro 510     |
    +--------------+------------+------------+-----------------------+
    
    
You can use 'EmployeeId' to query the 'Devices' table and find a list of all devices that
a given employee had been loaned. With this design modification, there are no repeating 
groups in tables or repeating values in any field.

Note that the trouble with the original table was that table's design was too lenient:
You are to only put a set of related data in a table, but the original design allowed 
extraneous data in. Often, as was the case in this example, the typical solution to a 
normalization problem is the creation of a new table.
  
    
4.2.2.2 SECOND NORMAL FORM (2NF)

These are the requirements for Second Normal Form: 
      
    * The first requirement for Second Normal Form is that the table is in First Normal
      Form. Full compliance with 1NF is a mandatory requirement for full compliance with 
      2NF.
    
    * Second Normal Form is concerned with the relationship between Key columns and 
      Non-key columns. More specifically, 2NF is only a problem for tables that use a 
      composite primary key. A composite key is a primary key that is composed of values 
      from two or more columns. If you use a composite key, you must pay close attention 
      to its relationship to other non-key columns in that table. If a table has a single-
      attribute primary key, which is usually the case the majority of the time, then you 
      do not have to worry: the table is automatically compliant with 2NF. 
    
      If your table does use a composite key, 2NF demands that all non-key attributes 
      must be functionally dependent on the entirety of the composite key. As a composite 
      key is composed of two or more keys, it is possible for a non-key attribute to be 
      dependent on only some of the component keys of the composite key. 2NF will not 
      accept partial dependence; it demands that all non-key attributes must be fully 
      dependent on all constituent attributes of the composite primary key. 
    
    
Once again, this is hard to understand without a actual example, so consider the example
table below. This is a 'Shipments Received' table. The 'Shipment#' and 'Part#' columns 
together form a composite key for the table. This has been done because the 'Shipment#' 
column alone cannot uniquely identify a part; a Cooling Gasket can be part of multiple 
shipments. However, when 'Shipment#' is combined with 'Part#', you can use that to 
uniquely identify the other non-key columns. Or at least, that's what should happen if
2NF were in effect. However, while this table complies with 1NF, it fails to comply with 
Second Normal Form.   


    +-----------+--------+---------------+-----------------+------------+-------------+
    | Shipment# | Part#  | DateOfArrival |    PartName     | Quantity   | Price (CAD) |
    +-----------+--------+---------------+-----------------+------------+-------------+
    |       532 | 183490 | 2018/08/23    | Cooling Gasket  |         43 |         125 |
    |       329 | 109834 | 2018/03/11    | Sprocket Wrench |        118 |          35 |
    |       623 | 958943 | 2018/09/02    | Fizzy Whizzler  |         12 |        1500 |
    |       497 | 183490 | 2018/06/17    | Cooling Gasket  |         11 |         125 |
    |       648 | 958943 | 2018/11/13    | Fizzy Whizzler  |         25 |        1500 |
    +-----------+--------+---------------+-----------------+------------+-------------+


Why? Because two columns, 'PartName' and 'Price', are not functionally dependent on the
composite key. In fact, these columns fail to have a dependency on either 'Part#' or 
'Shipment#', so they are not even partially dependent on the composite key. You'll notice 
that multiple entries exist for 'Cooling Gasket' and 'Fizzy Whizzler' and their 
associated prices. Neither 'PartName' nor 'Price' can be uniquely determined by either of 
the component keys of the composite key, or the composite key as a whole. Thus, this 
table fails to be 2NF-compliant. If the offending non-key columns were dependent on 
either 'Part#' or 'Shipment#', they would be partially dependent on the composite key.
Partial dependence also fails 2NF: all non-key columns must depend on the entirety of the
composite key.  

Why is any of this important? This is important because at least a few columns, 'Part#', 
'PartName' and 'Price' respectively, are linked to each other, but that link is not being
managed by the database. What if someone updates a part number in this table, but fails 
to change its associated part name or its price? Now you face the question: Did you get a 
shipment of 'Sprocket Wrenches' or part number 34910 on a given date? Which is the 
incorrect value? Chaos will rule in the database, as part numbers and names change and 
start conflicting with each other. You do not want error-prone humans in charge of making 
sure values do not conflict in a database. A database could have millions of records! The 
point of implementing 2NF is to offload that responsibility on to the database itself. 

How can you fix this problem and bring this table into compliance with 2NF? Once again, 
the solution is to create a new table. You have to create a new 'Parts' table and move 
the 'PartName' and 'Prices' columns into it:


    +--------+-----------------+-------+
    | Part#  |    PartName     | Price |
    +--------+-----------------+-------+
    | 183490 | Cooling Gasket  |   125 |
    | 109834 | Sprocket Wrench |    35 |
    | 958943 | Fizzy Whizzler  |  1500 |
    +--------+-----------------+-------+
    
    
The 'Part#' attribute will act as a primary key for this field. In the 'Shipments 
Received' table, you can remove these fields and make the table smaller, like this: 


    +-----------+--------+---------------+----------+
    | Shipment# | Part#  | DateOfArrival | Quantity |
    +-----------+--------+---------------+----------+
    |       532 | 183490 | 2018/08/23    |       43 |
    |       329 | 109834 | 2018/03/11    |      118 |
    |       623 | 958943 | 2018/09/02    |       12 |
    |       497 | 183490 | 2018/06/17    |       11 |
    |       648 | 958943 | 2018/11/13    |       25 |
    +-----------+--------+---------------+----------+


Then you have to connect these two tables with a One-to-Many relationship. 'Shipments 
Received' would be 'Many' side and the 'Parts' table would be the 'One' side. This brings
the table into compliance with 1NF and as well as the requirements peculiar to 2NF, 
making it fully compliant with Second Normal Form.
    
    
4.2.2.3 THIRD NORMAL FORM (3NF)

These are the requirements of Third Normal Form:

    * The first requirement for Third Normal Form is that the table is in Second Normal
      Form. As 2NF requires compliance with 1NF, effecting 3NF first requires compliance
      with both First and Second Normal Form. Full compliance with 1NF and 2NF is a 
      mandatory pre-requisite for full compliance with 3NF.
      
    * 3NF is concerned with the relationship between Non-key attributes. Third Normal 
      Form calls for the elimination of all 'Transitive Dependencies' in your tables. 
      Oppel defines this term on page 203[1]: "An attribute that depends on another 
      attribute that is not the primary key of the relation is said to be transitively 
      dependent." 
      
      Put another way, 3NF demands that no non-key attribute (columns that are NOT part 
      of the primary key) must be functionally dependant on any other non-key field in 
      the table. Restated again: One or more non-key column(s) should not be the 
      determinant (i.e. able to uniquely identify) of values in another non-key column in 
      the same table. Expressed yet another way: 3NF requires that all non-key attributes
      must be dependent on only the primary key. 


Let's go to an example. See the 'OrderItem' table below. It tracks customer orders, 
product ordered, quantity and other such values. This table complies with 1NF and 2NF,
but 3NF. Why? See if you can spot the issue: 


    +--------+---------+-----------+----------+------------+-------+
    |   Id   | OrderId | ProductId | Quantity | Unit Price | Total |
    +--------+---------+-----------+----------+------------+-------+
    | 257939 |  398489 | BC32-5    |        7 |         12 |    84 |
    | 257940 |  398490 | NB82-9    |       15 |         57 |   855 |
    | 257941 |  398491 | OH29-1    |      150 |          5 |   750 |
    | 257942 |  398491 | VP10-8    |       15 |         25 |   375 |
    +--------+---------+-----------+----------+------------+-------+


The problem lies with the last three columns. The 'Total' column depends on the 
'Quantity' and 'Unit Price' columns: its values are a product of the latter two columns.
A non-key field is dependant on two non-key fields, violating 3NF. It is a very common 
mistake to put easily derivable information in a new column, but you should NOT do this.

If the values of one non-key can be ascertained from another field(s), it may lead to 
hard-to-resolve conflicts. If an order is updated and its quantity or unit price changed, 
the 'Total' column does not automatically get updated. If you fail to update the total, 
and notice the discrepancy at a later time, you face a serious problem: which of the 
values is wrong? Is it the 'Quantity' value, 'Unit Price' or the 'Total'? This is the 
reason why 3NF exists; to prevent your database from devolving into a chaotic mess of 
conflicting values.

You can resolve this problem by removing the transitively dependent column from the 
table. But if the information in the 'Total' column is valuable to you, can you keep it
somehow? Yes, you can. Many databases offer the option of defining a read-only 'Computed' 
or 'Calculated' column that is not really a part of the table. However, if you've defined
such a column, it will show up when you open the table. Its values cannot be changed 
directly as they are automatically calculated by the database from the two columns it 
depends on. In this way, you can both comply with 3NF and retain a computed column that 
cannot be corrupted by human error. 


4.2.2.4 BOYCE-CODD NORMAL FORM (BCNF or 3.5NF)

BCNF has two requirements:

    * The relation must be in Third Normal Form. As the Forms are cumulative, this means
      that 1NF, 2NF and 3NF must be applied to your table before you can apply BCNF. 
      
    * BCNF is an extension of Third Normal Form and was created to strengthen it. This is
      due to certain rare anomalies related to 3NF that were not addressed by it. BCNF 
      closes these loopholes. The techopedia article on BCNF explains the circumstances 
      that led to the development of this extension Form[14]: "3NF states that all data 
      in a table must depend only on that table’s primary key, and not on any other field
      in the table. At first glance it would seem that BCNF and 3NF are the same thing. 
      However, in some rare cases it does happen that a 3NF table is not BCNF-compliant. 
      This may happen in tables with two or more overlapping composite candidate keys." 
    
      Oppel provides more detail about what BCNF is designed to prevent on page 206[1]: 
      "It addresses anomalies that occur when a non-key attribute is a determinant of an 
      attribute that is part of the primary key (that is, when an attribute that is part 
      of the primary key is functionally dependent on a non-key attribute)." These are 
      anomalies that occur in tables with multiple candidate keys. 
  
      Oppel explains BCNF's prime criterion on page 206[1]: "No determinants exist that 
      are not either the primary key or a candidate key for the table. That is, a non-key 
      attribute may not uniquely identify (determine) any other attribute, including one 
      that participates in the primary key."  
      
      In the Second Normal Form, we have addressed cases where non-prime attributes 
      depend upon prime attributes, either in whole (functional dependency) or in part
      (partial dependency). In the Third Normal Form, we have addressed situations that
      arise when non-prime attributes depended upon other non-prime attributes. The one
      possibility that the first three Normal Forms have not addressed is what happens if
      a non-prime attribute determines a prime attribute? BCNF addresses this situation. 
      
      BCNF demands that, for any dependency where A derives B (A -> B), A must be a 
      candidate key. Rephrased: BCNF states that if B is a prime attribute, then A cannot 
      be a non-prime attribute. 
      
      In other words, BCNF insists that every determinant in a table be a candidate key. 
      A candidate key is the most minimum set of attributes in a table that can be used 
      to uniquely identify a record. In an article[15], Agnieszka Kozubek puts it this 
      way: "Informally the Boyce-Codd normal form is expressed as “Each attribute must 
      represent a fact about the key, the whole key, and nothing but the key.”" 

    
If ever there was a concept in need of an example, it is this one. BCNF is a bit more 
impenetrable than the first three Normal Forms, but it is still understandable. The 
example below is taken from MariaDb's Docs[16]. It depicts a 'Student Enrollment' table. 
You should make the following assumptions about the attributes in it:


    * Each instructor takes only one course
    * Each course can have one or more instructors
    * Each student only has one instructor per course
    * Each student can take one or more courses 


    +-----------------+----------------------+----------------+
    |     Student     |        Course        |   Instructor   |
    +-----------------+----------------------+----------------+
    | Julian Mives    | Geology 101          | Chris Chao     |
    | Pradeep Connect | Computer Science 203 | John Ike       |
    | Ahmed Kathra    | Philosophy 427       | Richard Mbappe |
    | Golan Goldberg  | Philosophy 427       | Richard Mbappe |
    +-----------------+----------------------+----------------+


For now, this table uses a composite primary key composed of two attributes: 'Student'
and 'Course'. The table complies with 1NF. It complies with 2NF because the 'Student' and 
'Course' attributes both determine the 'Instructor' column. Therefore, the 'Instructor' 
column is fully dependent on both attributes of the composite key. The table also easily
complies with 3NF's demand that no non-key attribute be dependent on any other non-key
attribute: there's only one non-key field, 'Instructor'! So what's the problem? 

Look at the third and fourth rows. There are two students taking Philosophy 427, which 
results in the instructor's name (Richard Mbappe) being stored twice. This is a data 
redundancy problem that the current design of our table fails to eliminate. This happened 
because 'Instructor' determines 'Course', which stated generically is: a non-prime 
attribute determines a prime attribute. In other words, 'Instructor' determines 'Course', 
but 'Instructor' is not a super key. Thus, this table fails to enforce BCNF. 

The solution is to move the 'Course' attribute to a separate table, along with its key.
This leaves only two attributes in the original table, as you can see below: 
 

    +-----------------+----------------+
    |     Student     |   Instructor   |
    +-----------------+----------------+
    | Julian Mives    | Chris Chao     |
    | Pradeep Connect | John Ike       |
    | Ahmed Kathra    | Richard Mbappe |
    | Golan Goldberg  | Richard Mbappe |
    +-----------------+----------------+


This new table is called the 'Student-Instructor' table. After removing the 'Course' 
attribute, you cannot effectively search for records with a single-attribute key. So you 
need to combine both 'Student' and 'Instructor' into a composite key for the table. This 
way, you will be able to uniquely identify records in this table.

The other table is the 'Instructor-Course' table. As the 'Instructor' attribute 
determined 'Course' in the original table, I am making it the primary key in this table.


    +----------------+----------------------+
    |   Instructor   |        Course        |
    +----------------+----------------------+
    | Chris Chao     | Geology 101          |
    | John Ike       | Computer Science 203 |
    | Richard Mbappe | Philosophy 427       |
    +----------------+----------------------+


As you can see, this eliminates the Instructor's name being stored redundantly. 
Decomposing the table into two separate tables has once again solved a normalization
problem.
 

****************************************************************************************************
****************************************************************************************************


SOURCES

01: Databases - A Beginner's Guide (Andy Oppel, 2009)
02: https://db-engines.com/en/article/RDBMS
03: https://www.calebcurry.com/beginner-database-terms/
04: https://www.techopedia.com/definition/25227/net-data-provider
05: https://stackoverflow.com/questions/25093452/difference-between-data-model-and-database-schema-in-dbms
06: https://stackoverflow.com/questions/36650268/the-difference-between-the-up-and-down-methods-in-the-migration-file
07: https://docs.microsoft.com/en-us/ef/core/managing-schemas/migrations/
08: https://en.wikipedia.org/wiki/Database_normalization
09: https://www.studytonight.com/dbms/database-normalization.php
10: https://mariadb.com/kb/en/library/database-normalization-overview/
11: https://www.lifewire.com/database-dependencies-1019727
12: https://www.techopedia.com/definition/19504/functional-dependency
13: https://stackoverflow.com/questions/4199444/functional-dependency-and-normalization
14: https://www.techopedia.com/definition/5642/boyce-codd-normal-form-bcnf
15: https://www.vertabelo.com/blog/technical-articles/boyce-codd-normal-form-bcnf
16: https://mariadb.com/kb/en/library/database-normalization-boyce-codd-normal-form/