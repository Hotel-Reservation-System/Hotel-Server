1. INTRODUCTION TO DATABASE DESIGN CONCEPTS

In this article, I'll talk about how to design a database and the tables in it. Do not be 
fooled by similarities between a spreadsheet and a database table: What you see is only a 
visual representation of data. For instance, beneath the hood, a database identifies 
relationships between pieces of data and enforces constraints between them prevent data
anomalies from making their way in. A spreadsheet program can do some of these things, 
but it is not really designed for this purpose. 

This is just one of the many things that a good database can do. However, you have to 
work with the database to design tables that are resistant to data-related problems. This
article will cover the following topics:


    2.  The Consequences of Poor Database Design
    
    3.  Database Design: The Process
    3.1 Revision: The Parts of a Table  
    3.2 Entity-Relationship Modelling
      


2. THE CONSEQUENCES OF POOR DATABASE DESIGN 

Poorly designed databases are prone to data redundancies or database anomalies. Redundant 
data is self-explanatory, but what are database anomalies? An anomaly is an aberrant 
behaviour in a database that occurs due to poor design. There are three kinds of database 
anomalies:


    * INSERTION ANOMALY: is a situation that occurs when you cannot insert a new record 
      into a table because the table requires a piece of data that is unavailable to you. 
      
      E.g.: A database that cannot add a customer information to its Customers table 
      until the customer actually makes a purchase.
      
      
    * DELETION ANOMALY: is the opposite of the Insert anomaly. This is a situation where
      the deletion of one entity/record causes unintended deletion of another entity or
      record. Deletion anomalies result in unexpected data loss.
      
      E.g.: Using the previous example, a deletion anomaly occurs if we delete a customer 
      invoice, which due to poor design, causes the deletion of that customer's 
      information. As in the insertion example, this happened because information about 
      two entities, namely customers and invoices, was incorrectly combined into a single
      table. 
      
      
    * UPDATE ANOMALY: arises when the same information can be expressed on multiple rows 
      of a table. This leads to a situation where the same information can exist in 
      multiple records. 
      
      Update anomalies can also refer to a situation where you have a chain of two or 
      more columns with inter-dependencies. In such a situation, updating a column may 
      require you to manually update other columns that depend on its values. If you 
      update the main column but fail to do that for the dependant column, you may get 
      data inconsistencies. 
      
      If a table design permits this anomaly, then over time, it will almost certainly 
      lead to data inconsistencies when one record or field is updated but the other one 
      is not.
      
      E.g.: Suppose you have a Student table where its possible to have a student's 
      information stored in more than one record. If one of these records is updated, 
      say the address is changed, you now have two records for one student, with 
      conflicting addresses. This is a clear sign of design error in your table.
      
      Your tables should have only a single source of truth for an attribute. Attributes 
      should be captured once and stored once in one field only. If needed, you can make 
      references to it in other tables. All references must point to this single, 
      original source.


How does one end up with such anomalies in a database? Well, those of us with experience 
using Microsoft Excel or other tools like it might be inclined at organizing a database 
in the "spreadsheet design" style. However, an RDBMS is not a spreadsheet program, despite
superficial similarities. Therefore, following this approach will only result in a 
database with the data quality of Excel spreadsheets found in any standard business 
setting. Most of these spreadsheets are rife with data anomalies, duplication and data 
inconsistency problems. Spreadsheets are not very good are solving the fundamental 
problems that databases are trying to solve. 

The other big problem with taking a "spreadsheet" approach to database design is that 
this results in tables that fail to take advantage of the database's ability to impose 
data integrity constraints. This can lead to a database design that does not respect the 
'Four Integrities': Entity, Domain, Referential and User-Defined. Even worse than that, 
an unnormalized database does not enforce inter-column and inter-table relationships. If 
you fail to leverage the full powers of an RDBMS, you might as well use a spreadsheet 
program to track your data instead of an database!

No, if you want to use a database effectively, you have to leave the spreadsheet 
mentality behind and learn some new skills. Because a database that is poorly designed 
will not guard against the data-related problems I have discussed. Such databases will be 
filled with low-quality, inconsistent data. Trying to update it will lead to errors and 
maintenance will be ongoing chore. 

The best way to prevent anomalies is to design your database correctly via 'Entity-
Relationship Modelling'. After doing that, you have to apply the process of 'Database 
Normalization' to improve its formal internal structure. The rest of the article is 
concerned with these two process and others related to them. 


3. DATABASE DESIGN: THE PROCESS

3.1 REVISION: THE PARTS OF A TABLE

I'll start this section with a short refresher on the component parts of a tables. When 
you store data in a database, you usually store it in a relation, which is another way of 
referring to a table. Take a look at the example table below, called 'Students':


    +-----------+-----------+----------+----------------+--------------------------+
    | StudentId | FirstName | LastName |  PhoneNumber   |          Email           |
    +-----------+-----------+----------+----------------+--------------------------+
    |    293843 | Harold    | Kumar    | (298) 239-3894 | haroldkumar@yahoo.com    |
    |    293494 | Jenny     | Spice    | (298) 830-9238 | j_luvbabyspice@yahoo.com |
    |    293939 | Jacob     | Yycomb   | (298) 832-2398 | jacob.yycomb@gmail.com   |
    |    394853 | Victoria  | Gillium  | (298) 382-3982 | vgill@gmx.com            |
    +-----------+-----------+----------+----------------+--------------------------+


The table above is designed to capture information about students. This is done with 
attributes, also called columns, which are the names of various properties that an entity 
might have. Columns are listed horizontally at the top of a table. The 'Students' table 
has 5 attributes: 'StudentId', 'FirstName', 'LastName', 'PhoneNumber', and 'Email'. Each
attribute captures certain piece of information about a student. All information 
pertaining to an attribute have to be stored in its column. For instance, you cannot 
store StudentId information in the 'PhoneNumber'; this a constraint that is enforced by 
the database. 

In RDBMSes, a Row or a Record, refers to a horizontal block of named elements that 
contain information on a single entity. In this table, each record contains information 
on a student. The first record in this table, that of Harold Kumar, contains information 
required by the five attributes at the top. Similarly, this table is filled with other 
unique student records. Or at least, that should be the case, if there are no duplicate
records in it. Each record has the same structure; it contains information required by 
the columns. 
 
ASIDE: Sometimes, records are referred to as tuples. A tuple is a formal term that is 
distinct concept but it is related to a record (or a row).

The intersection of a column and a row is a Field. This is where an individual piece of 
data is to be stored. In the example table, you should see meaningful pieces of data in 
all of the fields.  


3.2 ENTITY-RELATIONSHIP MODELLING

When you sit down to design a new database, the first thing you will have to do is 
something called 'Entity-Relationship Modelling'. ER Modelling is dominated by a "big 
picture" consideration of your program's data needs. Bearing in mind these requirements, 
ER-Modelling is the process of determining the entities/relations (tables) in your 
database, their attributes (columns) and and their relationships to other entities 
(One-to-One, One-to-Many, Many-to-Many) relationships.

At this time, you should also be considering how to protect data integrity by implementing
the 'Four Integrities'. Modern RDBMSes are designed to help you easily implement these 
safeguards. Practically speaking, what does that mean? You can enforce these constraints
as follows: 


    1. Entity Integrity Constraints: are enforced by using primary keys to ensure that no 
       two rows in your table have identical values for all columns. At the very least, 
       a table must have at least one attribute with unique values. This role is usually 
       played by primary key columns, which must contain unique values for every record. 
       With the help of a primary key, you can uniquely identify any record in the table.
       
       When you design a table, you must give it a primary key (or a composite key).   
       
       
    2. Domain Integrity Constraints: are enforced by defining a set of constraints for an
       attribute. When you design the attributes in a table, you have the responsibility 
       of setting their constraints. For instance, you can set data type for a column and 
       other limitations if you so choose, such as formatting or range restrictions on 
       the values. Another thing you can do is you can direct an attribute to accept NULL 
       values. 
       
       
    3. Referential Integrity Constraints: When you want to create inter-table 
       relationships, you need to have primary keys in the principal table and foreign 
       keys in the dependent table. If you declare and register these keys with the RDBMS
       when you create tables, you can have it enforce referential integrity constraints. 
       This constraint will check to see whether every foreign key value in the dependent 
       table has a corresponding primary key in the principal table. 
       
    4. User-Defined Constraints: This constraint is optional. However, if you wish to 
       have legal requirements or business process rules codified into constraints, you
       can also do it via CHECK constraints and TRIGGERS. An RDBMS will enforce these
       constraints along with the other three, further improving data quality.


At the completion of the ER-Modelling process for your database, you should have 
identified tables and their columns. You should also identified the nature of 
relationships between two tables. And to enforce integrity constraints, you should have 
declared a primary or composite key in every table and a foreign key whenever two tables 
have to be connected. Within tables, you should have carefully considered and assigned 
data types and restrictions to every attribute. 


3.3 INTRODUCTION TO NORMALIZATION: WHY DO YOU NEED IT? 

Here is an introductory description of Normalization from Wikipedia's article[1]:


    Database normalization is the process of restructuring a relational database in 
    Database normalization is the process of restructuring a relational database in 
    accordance with a series of so-called normal forms in order to reduce data redundancy 
    and improve data integrity. It was first proposed by Edgar F. Codd as an integral 
    part of his relational model.
    
    Normalization entails organizing the columns (attributes) and tables (relations) of a 
    database to ensure that their dependencies are properly enforced by database 
    integrity constraints. It is accomplished by applying some formal rules either by a 
    process of synthesis (creating a new database design) or decomposition (improving an 
    existing database design).


Here's an alternate definition[2]:


    Database Normalization is a technique of organizing the data in the database. 
    Normalization is a systematic approach of decomposing tables to eliminate data 
    redundancy (repetition) and undesirable characteristics like Insertion, Update and 
    Deletion Anomalies. It is a multi-step process that puts data into tabular form, 
    [and] removing duplicated data from the relation tables.
    
    Normalization is used for mainly two purposes,
    
    * Eliminating redundant data, AND
    * Ensuring data dependencies make sense i.e data is logically stored.
    

What are some of the advantages of normalizing your database?[3]:


    * Eliminate data redundancies (and therefore use less space)
    * Make it easier to make changes to data, and avoid anomalies when doing so
    * Make referential integrity constraints easier to enforce
    * Produce an easily comprehensible structure that closely resembles the situation the 
      data represents, and allows for growth
       
       
Before I talk about Normalization, let's back up a little bit. In the previous section, 
you should already have arrived at a preliminary design for your database. However, you 
are still not fully taking advantage of an RDBMS's capabilities to manage inter-attribute 
relationships. What that does mean mean? 

An attribute's values may have one of several relationships to another attribute. For 
instance, if you have a 'Social Security Number' column in an properly designed 
'Employees' table, it will map on to one and only one value in 'EmployeeName', 
'DateOfBirth' and perhaps other columns. This is a very useful property, because you can
use it to uniquely identify and pull up employee records. There are a number of useful
properties and relationships like this one that can occur between data elements in a 
table. 

If you can identify every inter-attribute relationship in every table in your database, 
you can formalize these relationships and get the RDBMS to track them. If you can manage 
this, you have successfully offloaded a very complex, tedious and error-prone task to 
your database engine. In a very large database, which may contain tens of millions of 
records, there is no hope of manually managing all of the inter-attribute relationships. 
There are so many ways to make mistakes that lead to data anomalies, redundancies, 
inconsistencies and even data loss, that no human could possibly succeed. Thus, this is
a task best left to an RDBMS program. That is the goal of Database Normalization: to 
impose a logical structure on your database so that the RDBMS engine can be put in charge 
of managing it. 

Normalization is applied at the "micro level"; you will focus on one entity (table) and 
consider its attributes and other characteristics. The general idea behind Normalization 
is that a table is about a specific topic and that only columns which are related to it 
must be included in it. It is the process of applying formal rules to tables and most 
especially, their columns. The end goal is to identify to create a correct set of 
inter-connections between columns while also removing columns that are extraneous to that
table.

In order to do this, you have to identify the nature of inter-attribute dependencies in a 
table. First, are any of the columns in the present table not directly related to its 
main topic? Secondly, you have examine the relationship of each of the attributes to the
primary key and to each other, applying so-called 'Normal Forms' to bring the attributes 
into a correct relationship with each other. 

By doing this, you are moving from merely a spreadsheet style collection of information, 
to Structured Data. Structured data has well-defined attributes which give it a clearly 
defined form. For instance, in a table, every record must have values for mandatory 
columns. Every attribute must have the correct relationship to other attributes, in 
accordance with the Normal Forms. By doing these things, you are imposing structure on 
on what is just a collection of data. By determining the nature of relationships between 
data elements and ensuring that they meet certain formally defined standards, you no 
longer have to enforce inter-attribute relationship constraints by hand. As I mentioned
before, you can offload the responsibility of tracking these relationships on to the 
RDBMS engine. 

This should lead to a much more robust database design, one in which you are doing as 
little work as possible, and yet giving the RDBMS engine all the tools it needs to do 
everything it can to mitigate data-related issues which plague databases. This is the 
difference between a spreadsheet program and a database. A spreadsheet is just a 
collection of data into rows and columns with little help from the program in imposing 
order and enforcing constraints, especially between columns.

Normalization will make your database easier and more reliable to work with. It will 
reduce unwanted duplicate data in the database, thereby decreasing your database's size. 
It will render many classes of anomalies impossible, increasing the integrity, validity
and robustness of the data stored therein. This will make it harder to create invalid 
states in the database or enter poor quality data into it. This should make it easier 
to modify and maintain your database.

For all these reasons, you should normalize every database you design. You should do 
normalization as part of the initial design and whenever you revisit and tweak your 
database's schema. Note that the process of normalizing your database often results in 
the creation of new tables. (This happens when you realize that some attributes just do 
not belong in a table; therefore, you move them to a new table.) 


3.4 NORMALIZATION: THE PROCESS
   
So how do you normalize your database? What does the process look like? This depends on
whether you have an existing unnormalized/partially normalized database, or if you are 
starting to design a fresh, new database. Applying Normalization to the former is called
Decomposition while doing it to the latter is called Synthesis. The same principles apply 
to both situations.
   
Before you can start normalization, you should have completed the 'ER-Modelling' phase. 
You should have a provisional database design. 

In order to successfully apply Normalization on a database, you have to understand these
concepts:


    1. DEPENDENCY THEORY, especially as it relates to Functional Dependencies
    2. THE NORMAL FORMS, which are a series of requirements imposed on inter-attribute
       relationship.


4. DEPENDENCY THEORY



4.2.1.1 PRIME AND NON-PRIME ATTRIBUTES

A candidate key is any attribute or combination of attributes that can determine 
(uniquely identify) a record. Keys that can do this are candidates to become the primary 
key of a table. This is where the name 'Candidate Key' comes from. In a table, there may 
be several candidate keys, from which you have to select one as primary key.

Attributes (columns) that are part of a candidate key are called PRIME ATTRIBUTES or KEY
ATTRIBUTES. The other attributes from that table are called NON-PRIME ATTRIBUTES or 
NON-KEY ATTRIBUTES.


4.2.1.2 DATA RELATIONSHIPS IN DATABASES: INTER-ATTRIBUTE DEPENDENCIES

Within a table, the data in one attribute(s) may have certain relationships to data in 
other attribute(s). The process of database normalization requires you to understand and formalize 
these data relationships. The study of this subject is called DEPENDENCY THEORY. Its 
Wikipedia article explains what it is about[]: "Dependency theory is a sub-field of 
database theory which studies implication and optimization problems related to logical 
constraints, commonly called dependencies, on databases. The best known class of such 
dependencies are functional dependencies, which form the foundation of keys on database 
relations. Another important class of dependencies are the multivalued dependencies."

Once you understand the nature of the relationships between attributes, you can re-design 
your tables by moving extraneous information into new tables. Then, you can structure 
your tables in such a way that the database can enforce data types, referential and 
integrity constraints. By doing this, you can leverage the database to prevent data 
redundancies and database anomalies. 

The first step to doing that is to understand the nature of dependencies in your data.


When it comes to normalization, there are several major kinds of dependencies: 
Functional, Multi-Valued and Join. This article will only consider Functional 
Dependencies and its sub-types, because they are necessary to understand the Normal Forms
covered in this article: the first three Normal Forms and the Boyce-Codd Normal Form.
Multi-Valued and Join dependencies cover later Normal Forms. 


FUNCTIONAL DEPENDENCY

To normalize tables, you have to understand the concept of 'Dependency' and its many
variants. In the most general terms[11], "a dependency is a constraint that applies to or 
defines the relationship between attributes." This is the same thing as a 'Functional 
Dependency'. A Functional Dependency is a relationship between two sets of attributes. 
Usually, but not always, this is referring to the relationship between a Primary Key (PK) 
attribute set and a second set of non-prime attribute(s) within a table. 

This is the definition of a functional dependency: 

For two arbitrary attributes, A and B, Attribute B is said to be functionally dependent 
on Attribute A, if at any moment in time there is exactly one instance of B for any given 
instance of A. This is a rather formal explanation, but here it is again phrased another 
way: If Attribute A functionally determines Attribute B, then the value of an instance of 
A uniquely determines a value for B. 

Let's move on to a less formal description: A determinant is an attribute that determines 
the value of another attribute. If A determines B, then a value of A can uniquely 
identify a value of B. Therefore, A is said to be a determinant of B and B is said to be 
a dependent of A. This relationship is visually represented like this:

    
    A -> B 


The left hand side of this Functional Dependency diagram contains the determinant set and
the right hand side is the determined set. It is read like this from left to right: "A 
(functionally) determines B" OR "A derives B". The Inverse: "B depends on A."

Let's take a concrete example[12]: In a table, suppose there are two columns with a 
functionally dependent relationship: 'Social Security Number' and 'EmployeeName'.  Let's 
look at the FD diagram:


     Social Security Number -> EmployeeName, EmployeeAddress, BirthDate 
 

The 'EmployeeName' attribute is functionally dependent on the 'SSN' attribute because an 
SSN uniquely identifies an employee. Note that the reverse does not hold true; names do 
not uniquely identify a SSN. This makes sense because a Social Security Number is issued 
to every person in the country to uniquely identify them, even when there are multiple 
people with the same name. If there were other non-key attributes in the table, for 
instance 'EmployeeAddress' and 'BirthDate', the 'SSN' attribute could functionally 
determine them too.

Why is the concept of functional dependence important? 

In a StackOverflow answer, NealB explains[13]:


    Sets of functional dependencies may be used to synthesize relations (tables). The 
    definition of the first 3 normal forms, including Boyce Codd Normal Form (BCNF) is 
    stated in terms of how a given set of relations represent functional dependencies. 
    Fourth and fifth normal forms involve Multi-Valued dependencies (another kettle of 
    fish).
 

Normalization applies what are called 'Normal Forms' to tables. The second and third 
normal forms as well as the Boyce-Codd Normal Form (which is considered to be Normal Form
3.5) require an understanding of functional dependencies. You will have to find and 
write down the dependencies in every table to determine how close they are to complying
with the Normal Forms.

There are several sub-types of functional dependencies, which are discussed below:


TRIVIAL FUNCTIONAL DEPENDENCY

A trivial functional dependency is an obvious dependency. It occurs when the determined 
set is a subset of its determining set. Formally, it is called the 'Axiom of Reflexivity'.

For instance, consider a primary key composed of two attributes, 'Social Security Number' 
and 'EmployeeName'. Such a key is called a composite key. By searching for records that 
match the values of both attributes, the composite key can uniquely identify the name of 
an employee in the 'EmployeeName' column. The nature of the relationship is expressed 
like this:


    {SSN, EmployeeName} -> EmployeeName
    
 
This is where Trivial Dependency occurs: The composite key can uniquely identify an 
'EmployeeName' because this attribute is a part of the composite key. If this seems 
obvious, it is because it is. A trivial dependency occurs because 'EmployeeName' is part
of both the determining and determined sets. 
  
  Generically, it is expressed as follows:


    {A, B} -> B 
    

PARTIAL FUNCTIONAL DEPENDENCY

Partial dependency occurs when the determinant set is only partially dependant on the
determining set. Partial dependency is formally called the 'Axiom of Augmentation'.

Consider this example table: Store(StoreId, Product, Price). First, identify the prime 
and non-prime attributes in this table. To do that, you have to figure out the candidate
key(s). Neither 'StoreId' nor 'Product' can uniquely locate a record, so we have to 
create a composite key by combining these two attributes together. There is only one
attribute left, 'Price', and thus, it is a non-prime attribute. 

Thus, this is the FD diagram for this table:


    StoreId, Product -> Price


'StoreId' and 'Product' combined form the determining set, which determines the value for
'Price', the determined set. However, if you pay attention, you will note that 'Price' is
only partially dependant on the determinant set. Why? Because 'StoreId' does not play a 
part in determining the 'Price', only 'Product' does. That makes 'StoreId' extraneous to 
the determinant set. Despite the inclusion of 'StoreId' in the determining set, the true 
nature of functional dependency in this table looks like this:


    Product -> Price


Partial Dependency violates Second Normal Form. It is something that you must strive to 
eliminate from your database, because it will lead to data inconsistency errors. 


FULL FUNCTIONAL DEPENDENCY 

Before discussing the Full Functional Dependency, I want to talk about the concept of a
'Minimum Viable Determinant Set'. When you create a determinant set, you can add 
extraneous attributes to it, as the partial dependency example demonstrated. This does
absolutely nothing useful, but it can serve as a source of confusion about nature of 
dependencies in your table as well as data inconsistencies and redundancies. Remember, 
partial dependency is not to be embraced, but eliminated. 

Therefore, when you are creating candidate keys, it is necessary to reduce the set of 
attributes to the bare minimum required to uniquely identify records. If you remove any 
attributes from this minimum set, you will lose the ability to uniquely identify records, 
which is to say, you will break functional dependency. I call this the 'Minimum Viable 
Determinant Set' (MVDS).

'Full Functional Dependency' occurs when the conditions for functional dependency are met 
AND the determinant set is an MVDS. In this situation, the dependant set is functionally 
dependant on the entirety of the determinant set, not merely on a subset of it. To 
rephrase: Full dependency describes a situation where dependent attribute(s) in a relation
is dependant on ALL the component attributes of the determinant attribute(s). The operative 
word here is "All".

Consider an 'Employee' relation with three attributes, 'EmployeeId', 'EmployeeName' and 
'Location'. It can be represented like this: Employee(EmployeeId, EmployeeName, Location). 
'EmployeeId' is the primary key and the other two columns are non-key attributes. In this 
example, the second and third attributes are fully dependent on the 'EmployeeId' 
attribute. The relationship looks like this:


    EmployeeId  ->  EmployeeName, Location 
    

Because 'EmployeeId' uniquely identifies employees, it functionally determines a value 
for 'EmployeeName' and 'Location'. 'EmployeeName' and 'Location' each are fully dependant 
on the entirety of the determinant set, which in this case is a single attribute primary 
key. 

This is a simple case because the determinant set has only one attribute, but more 
complex cases arise when the determinant set is composed of many keys. Indeed, violations
of Second Normal Form occur only when you have multi-attribute primary keys. Thus, when 
you have a composite primary key, you have to ensure that every dependent attribute 
depends on every attribute in the composite key in order to have Full Functional 
Dependence.


TRANSITIVE DEPENDENCY

Before discussing Transitive Dependence, let's first look at the 'Axiom of Transitivity' 
that is core to this idea. This axiom states the following:


    If A -> B, and B -> C, then A -> C
    
    
The term 'transitive' in mathematics and logic, simply says that if some relation (such
as equality, greater than, less than or functional dependency) exists between the first
and second elements, and also exists between the second and third elements, this same 
relation will also hold between the first and third elements. Take a moment to absorb 
this concept before proceeding. 

In a table, transitive Dependence occurs when a primary key does not directly identify 
an attribute. Instead, the primary key determines a non-prime attribute, which in turn 
determines the value of a second non-prime attribute. Therefore, when transitive 
dependence occurs, there is an indirect relationship that causes a functional dependency. 
The Functional Diagram looks like this:


    A -> B -> C

    
Where A is a prime attribute, and B and C are non-prime attributes. This concept is a 
bit tricky, so let's jump into an example. Take a look at the relation below, which was
taken from Wikipedia: 


        +----------------------+------+-----------------+------------------------+
        |      Tournament      | Year |     Winner      | Winner's Date of Birth |
        +----------------------+------+-----------------+------------------------+
        | Indiana Invitational | 1998 | Al Frederickson | 21 July 1975           |
        | Cleveland Open       | 1999 | Bob Albertson   | 28 September 1968      |
        | Des Moines Masters   | 1999 | Al Frederickson | 21 July 1975           |
        | Indiana Invitational | 1999 | Chip Masterson  | 14 March 1977          |
        +----------------------+------+-----------------+------------------------+


The primary key is composed of 'Tournament' and 'Year', making 'Winner' and 'Winner's 
Date of Birth' non-prime attributes. The problem is this table'a attributes have a 
transitive functional dependence. The composite key determines the 'Winner' attribute, 
which in turn determines the 'Winner's Date of Birth'.  

Transitive Dependency is an undesirable trait in a table; it violates Third Normal Form. 
But why is it a problem? For one thing, there are duplicate values in the last column. 
If the 'Winner' column of any record is changed, you have to update the 'Winner's DOB' 
column and vice versa. Moreover, this dependency is not being managed by the database, 
so the user is now in charge of it. Whenever you have a human in charge of cross-checking
potentially millions of records to exclude data redundancy errors, you know how this 
situation is going to end: the person will fail and the database will fill up with data
inconsistencies. This, in turn, will lead to update anomalies. 

A database can only manage direct functional dependencies, not indirect ones. Therefore,
transitive dependencies are not allowed in databases. If you eliminate transitive 
dependencies, you move the onus for ensuring data consistency on to the database. 

In this table, you can eliminate this problem by creating a new 'Winners' table and 
moving the 'Winner's DOB' attribute into it. You can use the add a 'Names' column and 
populate it with the names of the winners. 


4.2.2 THE NORMAL FORMS

The Normal Forms were developed by Edgar Codd, the father of relational databases. 
Normalization theory defines 6 major Normal Forms and a number of minor forms. The first
three Normal Forms are treated like an essential block. If you bring your database into
compliance with the first three forms, you will be protected against well over 90% of 
redundancy and data integrity issues in most databases.   

Therefore, you should apply the first three forms to all databases. Following that, you 
should consider the Boyce-Codd Normal Form (BCNF or 3.5NF), which is considered to be a 
minor extension to the Third Normal Form. The fourth and fifth forms come into play only 
very rarely. The sixth form seems to be treated like a theoretical oddity.  


4.2.2.0 UNNORMALIZED FORM (UNF or 0NF)

This is the state of a table or database before you have applied the Normal Forms. An 
unnormalized database will suffer from data redundancy and data integrity issues due to
their violation of the Normal Forms. 

The Forms are mainly properties of a relation. In database terminology, a relation is 
equivalent to a table. Therefore, you have to scrutinize every table in your database to
ensure that each of the forms is applied. 

    
4.2.2.1 FIRST NORMAL FORM (1NF)
   
These are the criteria of First Normal Form:


    * To eliminate anomalies and data redundancy, create a separate table for each set of
      related data. You should define a primary key in each table to uniquely identify 
      records. This is done to enforce 'Entity Integrity' in the database. 
      
    * An attribute is a vertical column in a table. All columns in a table must have 
      unique names. All values in a column must be of the same domain (data type). This 
      will ensure 'Domain Integrity' in the database. 
      
    * In a relation, a field is the intersection of a row and a column. It is the most 
      fundamental unit of storage in a database. You can store a data fragment in it.
      This piece of data must be atomic, i.e. a piece of data that cannot or should not 
      be divided into smaller pieces. (For example of the latter case, you can divide 
      date values into separate year, month and date columns, but this is widely regarded 
      as a poor idea.) 
      
    * A field must, at most, contain only one piece of data. 1NF prohibits multi-value 
      fields. This applies to all fields in all tables. Do not use commas, semi-colons or 
      other delimiters to put multiple values into a field. Databases treat multi-value
      fields as a single value. 
      
      Applying 1NF lets the database track, store and display data in a tabular format. 
      Violating 1NF will mean that the database is no longer tracking the extra values in
      multi-value fields. This will prevent the database from applying domain integrity 
      constraints to them. This will lead to an increase in the amount of low-quality 
      or conflicting data in your database.
    
    * Eliminate repeating groups from tables. Repeating groups are columns with the same
      name, but different numbers at the end. They are a special case of multi-value 
      fields. They usually occur when you try to get rid of multi-value fields by 
      creating new columns. The best way to eliminate repeating groups is to study the 
      table, find and identify the set of data related to repeating groups and move them 
      into a new table. 
          
    
This principle is hard to understand in the abstract, so consider the table below. It is
an 'Employee' table, which contains a list of employees, their Ids, names, emails etc. 
Each employee may be issued corporate devices like laptops and phones, whose serials need 
to be tracked. This will be done in the last column, which is called 'DeviceSerial'.


    +------------+-----------+----------+-----------------------+--------------+
    | EmployeeId | FirstName | LastName |         Email         | DeviceSerial |
    +------------+-----------+----------+-----------------------+--------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D |
    |       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 |
    |       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8 |
    +------------+-----------+----------+-----------------------+--------------+


So far, this table lets the company track the devices they given to employees and as a
bonus, complies with 1NF. However, what happens if employees are given more than one 
device? The easiest solution to the problem is to add additional devices to the 
DeviceSerial column, separated by commas and other delimiters. This is what it would look
like:


+------------+-----------+----------+-----------------------+--------------------------------------------+
| EmployeeId | FirstName | LastName |         Email         |                DeviceSerial                |
+------------+-----------+----------+-----------------------+--------------------------------------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D, SD8GD8DFHFH0                 |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 | RIQO98L239DS | 0J78G8J8G7GJ |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8                               |
+------------+-----------+----------+-----------------------+--------------------------------------------+


However, this is something you should NEVER do in a database. It is a violation of 1NF. 
Relational Databases can handle many things, but they are not equipped to handle 
multi-value fields. Multiple values in a field get treated like a single value. Searching 
for data becomes extremely difficult in a database that fails 1NF compliance. The bigger 
problem is that this practice retards a database's full abilities to categorize and 
manage data. If you use multi-value fields, you are now responsible for enforcing data 
types, and other data integrity constraints for potentially millions of records. So, 
don't go there. Let your database manage the data and enforce types and constraints. All 
you have to do to offload this responsibility to the database is put a maximum of one 
value in a field.  

However, we still have multiple values to track. How else can you track them? The next 
obvious solution is to add additional columns for devices, so that you can track one 
device per column. See the table below:


+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
| EmployeeId | FirstName | LastName |         Email         | DeviceSerial1 | DeviceSerial2 | DeviceSerial3 |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D  | SD8GD8DFHFH0  |               |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4  | RIQO98L239DS  | 0J78G8J8G7GJ  |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8  |               |               |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+


Unfortunately, this is also something you should NEVER do, because this too violates 
First Normal Form. Creating multiple columns in the vein of DeviceSerial1, DeviceSerial2,
DeviceSerial3 etc., is called a REPEATING GROUP. The existence of repeating groups also
violates First Normal Form. The classic sign of a repeating group is identical column
names with different numbers tacked on the end of it to make unique names. 

Repeating groups are a sign of inflexible design. What happens when employees from the 
quality control department need to be given 10 different devices each? Repeating groups 
do not scale well. Giving an employee a new device should not necessitate changes to a 
database's schema. 

No, the correct response to such a problem is to create a new table that tracks devices,
with 'EmployeeId' as foreign key. You can move the 'DeviceSerial' column to this new 
table and add new ones, like 'DeviceType' and 'Description' to help the business better 
track its devices. After doing this, you can establish a one-to-many relationship between 
the 'Employee' and 'Devices' table, where the 'One' side is the 'Employee' table and the 
'Many' side being the 'Devices' table. This is the modified 'Employees' table may look 
like:

    
    +------------+-----------+----------+-----------------------+
    | EmployeeId | FirstName | LastName |         Email         |
    +------------+-----------+----------+-----------------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com |
    |       0002 | Jill      | Hill     | jhill@gmail.com       |
    |       0003 | Simon     | Says     | ss@outlook.com        |
    +------------+-----------+----------+-----------------------+


This is what the 'Devices' table might look like: 

    
    +--------------+------------+------------+-----------------------+
    | DeviceSerial | EmployeeId | DeviceType |      Description      |
    +--------------+------------+------------+-----------------------+
    | AD89AFLL35L4 |       0001 | Iphone     | 5th gen Apple iPhone. |
    | AD9F8AFAF98D |       0003 | Printer    | HP Printer #2839      |
    | SDF90AF87GG8 |       0003 | Laptop     | Dell Latitude 910     |
    | 0J78G8J8G7GJ |       0002 | Camera     | Canon XTS Pro 510     |
    +--------------+------------+------------+-----------------------+
    
    
You can use 'EmployeeId' to query the 'Devices' table and find a list of all devices that
a given employee had been loaned. With this design modification, there are no repeating 
groups in tables or repeating values in any field.

Note that the trouble with the original table was that table's design was too lenient:
You are to only put a set of related data in a table, but the original design allowed 
extraneous data in. Often, as was the case in this example, the typical solution to a 
normalization problem is the creation of a new table.
  
    
4.2.2.2 SECOND NORMAL FORM (2NF)

These are the requirements for Second Normal Form: 
      
    * The first requirement for Second Normal Form is that the table is in First Normal
      Form. Full compliance with 1NF is a mandatory requirement for full compliance with 
      2NF.
    
    * Second Normal Form is concerned with the relationship between Key columns and 
      Non-key columns. More specifically, 2NF is only a problem for tables that use a 
      composite primary key. A composite key is a primary key that is composed of values 
      from two or more columns. If you use a composite key, you must pay close attention 
      to its relationship to other non-key columns in that table. If a table has a single-
      attribute primary key, which is usually the case the majority of the time, then you 
      do not have to worry: the table is automatically compliant with 2NF. 
    
      If your table does use a composite key, 2NF demands that all non-key attributes 
      must be functionally dependent on the entirety of the composite key. As a composite 
      key is composed of two or more keys, it is possible for a non-key attribute to be 
      dependent on only some of the component keys of the composite key. 2NF will not 
      accept partial functional dependence; it demands that all non-key attributes must 
      be fully dependent on all constituent attributes of the composite primary key. 
    
    
Once again, this is hard to understand without a actual example, so consider the example
table below. This is a 'Shipments Received' table. The 'Shipment#' and 'Part#' columns 
together form a composite key for the table. This has been done because the 'Shipment#' 
column alone cannot uniquely identify a part; a Cooling Gasket can be part of multiple 
shipments. However, when 'Shipment#' is combined with 'Part#', you can use that to 
uniquely identify the other non-key columns. Or at least, that's what should happen if
2NF were in effect. However, while this table complies with 1NF, it fails to comply with 
Second Normal Form.


    +-----------+--------+---------------+-----------------+------------+-------------+
    | Shipment# | Part#  | DateOfArrival |    PartName     | Quantity   | Price (CAD) |
    +-----------+--------+---------------+-----------------+------------+-------------+
    |       532 | 183490 | 2018/08/23    | Cooling Gasket  |         43 |         125 |
    |       329 | 109834 | 2018/03/11    | Sprocket Wrench |        118 |          35 |
    |       623 | 958943 | 2018/09/02    | Fizzy Whizzler  |         12 |        1500 |
    |       497 | 183490 | 2018/06/17    | Cooling Gasket  |         11 |         125 |
    |       648 | 958943 | 2018/11/13    | Fizzy Whizzler  |         25 |        1500 |
    +-----------+--------+---------------+-----------------+------------+-------------+


Why? Because two columns, 'PartName' and 'Price', are not functionally dependent on the
composite key. In fact, these columns fail to have a dependency on either 'Part#' or 
'Shipment#', so they are not even partially dependent on the composite key. You'll notice 
that multiple entries exist for 'Cooling Gasket' and 'Fizzy Whizzler' and their 
associated prices. Neither 'PartName' nor 'Price' can be uniquely determined by either of 
the component keys of the composite key, or the composite key as a whole. Thus, this 
table fails to be 2NF-compliant. If the offending non-key columns were dependent on 
either 'Part#' or 'Shipment#', they would be partially dependent on the composite key.
Partial dependence also fails 2NF: all non-key columns must depend on the entirety of the
composite key.  

Why is any of this important? This is important because at least a few columns, 'Part#', 
'PartName' and 'Price' respectively, are linked to each other, but that link is not being
managed by the database. What if someone updates a part number in this table, but fails 
to change its associated part name or its price? Now you face the question: Did you get a 
shipment of 'Sprocket Wrenches' or part number 34910 on a given date? Which is the 
incorrect value? Chaos will rule in the database, as part numbers and names change and 
start conflicting with each other. You do not want error-prone humans in charge of making 
sure values do not conflict in a database. A database could have millions of records! The 
point of implementing 2NF is to offload that responsibility on to the database itself. 

How can you fix this problem and bring this table into compliance with 2NF? Once again, 
the solution is to create a new table. You have to create a new 'Parts' table and move 
the 'PartName' and 'Prices' columns into it:


    +--------+-----------------+-------+
    | Part#  |    PartName     | Price |
    +--------+-----------------+-------+
    | 183490 | Cooling Gasket  |   125 |
    | 109834 | Sprocket Wrench |    35 |
    | 958943 | Fizzy Whizzler  |  1500 |
    +--------+-----------------+-------+
    
    
The 'Part#' attribute will act as a primary key for this field. In the 'Shipments 
Received' table, you can remove these fields and make the table smaller, like this: 


    +-----------+--------+---------------+----------+
    | Shipment# | Part#  | DateOfArrival | Quantity |
    +-----------+--------+---------------+----------+
    |       532 | 183490 | 2018/08/23    |       43 |
    |       329 | 109834 | 2018/03/11    |      118 |
    |       623 | 958943 | 2018/09/02    |       12 |
    |       497 | 183490 | 2018/06/17    |       11 |
    |       648 | 958943 | 2018/11/13    |       25 |
    +-----------+--------+---------------+----------+


Then you have to connect these two tables with a One-to-Many relationship. 'Shipments 
Received' would be 'Many' side and the 'Parts' table would be the 'One' side. This brings
the table into compliance with 1NF and as well as the requirements peculiar to 2NF, 
making it fully compliant with Second Normal Form.
    
    
4.2.2.3 THIRD NORMAL FORM (3NF)

These are the requirements of Third Normal Form:

    * The first requirement for Third Normal Form is that the table is in Second Normal
      Form. As 2NF requires compliance with 1NF, effecting 3NF first requires compliance
      with both First and Second Normal Form. Full compliance with 1NF and 2NF is a 
      mandatory pre-requisite for full compliance with 3NF.
      
    * 3NF is concerned with the relationship between Non-key attributes. Third Normal 
      Form tries to implement a general principle: "All attributes should depend on the 
      key, whole key and nothing but the key."[Branko Dimitrijevic :https://stackoverflow.com/questions/9950367/what-is-wrong-with-a-transitive-dependency] 
      
      Transitive Dependencies violate this principle; which is why 3NF calls for the 
      elimination of all 'Transitive Dependencies' in your tables. Oppel defines this 
      term on page 203[1]: "An attribute that depends on another attribute that is not 
      the primary key of the relation is said to be transitively dependent." 
      
      Put another way, 3NF demands that no non-key attribute (columns that are NOT part 
      of the primary key) must be functionally dependant on any other non-key field in 
      the table. Restated again: One or more non-key column(s) should not be the 
      determinant (i.e. able to uniquely identify) of values in another non-key column in 
      the same table. Expressed yet another way: 3NF requires that all non-key attributes
      must be dependent on only the primary key. 


Let's go to an example. See the 'OrderItem' table below. It tracks customer orders, 
product ordered, quantity and other such values. This table complies with 1NF and 2NF,
but 3NF. Why? See if you can spot the issue: 


    +--------+---------+-----------+----------+------------+-------+
    |   Id   | OrderId | ProductId | Quantity | Unit Price | Total |
    +--------+---------+-----------+----------+------------+-------+
    | 257939 |  398489 | BC32-5    |        7 |         12 |    84 |
    | 257940 |  398490 | NB82-9    |       15 |         57 |   855 |
    | 257941 |  398491 | OH29-1    |      150 |          5 |   750 |
    | 257942 |  398491 | VP10-8    |       15 |         25 |   375 |
    +--------+---------+-----------+----------+------------+-------+


The problem lies with the last three columns. The 'Total' column depends on the 
'Quantity' and 'Unit Price' columns: its values are a product of the latter two columns.
A non-key field is dependant on two non-key fields, violating 3NF. It is a very common 
mistake to put easily derivable information in a new column, but you should NOT do this.

If the values of one non-key can be ascertained from another field(s), it may lead to 
hard-to-resolve conflicts. If an order is updated and its quantity or unit price changed, 
the 'Total' column does not automatically get updated. If you fail to update the total, 
and notice the discrepancy at a later time, you face a serious problem: which of the 
values is wrong? Is it the 'Quantity' value, 'Unit Price' or the 'Total'? This is the 
reason why 3NF exists; to prevent your database from devolving into a chaotic mess of 
conflicting values.

You can resolve this problem by removing the transitively dependent column from the 
table. But if the information in the 'Total' column is valuable to you, can you keep it
somehow? Yes, you can. Many databases offer the option of defining a read-only 'Computed' 
or 'Calculated' column that is not really a part of the table. However, if you've defined
such a column, it will show up when you open the table. Its values cannot be changed 
directly as they are automatically calculated by the database from the two columns it 
depends on. In this way, you can both comply with 3NF and retain a computed column that 
cannot be corrupted by human error. 


4.2.2.4 BOYCE-CODD NORMAL FORM (BCNF or 3.5NF)

BCNF has two requirements:

    * The relation must be in Third Normal Form. As the Forms are cumulative, this means
      that 1NF, 2NF and 3NF must be applied to your table before you can apply BCNF. 
      
    * BCNF is an extension of Third Normal Form and was created to strengthen it. This is
      due to certain rare anomalies related to 3NF that were not addressed by it. BCNF 
      closes these loopholes. The techopedia article on BCNF explains the circumstances 
      that led to the development of this extension Form[14]: "3NF states that all data 
      in a table must depend only on that table’s primary key, and not on any other field
      in the table. At first glance it would seem that BCNF and 3NF are the same thing. 
      However, in some rare cases it does happen that a 3NF table is not BCNF-compliant. 
      This may happen in tables with two or more overlapping composite candidate keys." 
    
      Oppel provides more detail about what BCNF is designed to prevent on page 206[1]: 
      "It addresses anomalies that occur when a non-key attribute is a determinant of an 
      attribute that is part of the primary key (that is, when an attribute that is part 
      of the primary key is functionally dependent on a non-key attribute)." These are 
      anomalies that occur in tables with multiple candidate keys. 
  
      Oppel explains BCNF's prime criterion on page 206[1]: "No determinants exist that 
      are not either the primary key or a candidate key for the table. That is, a non-key 
      attribute may not uniquely identify (determine) any other attribute, including one 
      that participates in the primary key."  
      
      In the Second Normal Form, we have addressed cases where non-prime attributes 
      depend upon prime attributes, either in whole (functional dependency) or in part
      (partial dependency). In the Third Normal Form, we have addressed situations that
      arise when non-prime attributes depended upon other non-prime attributes. The one
      possibility that the first three Normal Forms have not addressed is what happens if
      a non-prime attribute determines a prime attribute? BCNF addresses this situation. 
      
      BCNF demands that, for any dependency where A derives B (A -> B), A must be a 
      candidate key. Rephrased: BCNF states that if B is a prime attribute, then A cannot 
      be a non-prime attribute. 
      
      In other words, BCNF insists that every determinant in a table be a candidate key. 
      A candidate key is the most minimum set of attributes in a table that can be used 
      to uniquely identify a record. In an article[15], Agnieszka Kozubek puts it this 
      way: "Informally the Boyce-Codd normal form is expressed as “Each attribute must 
      represent a fact about the key, the whole key, and nothing but the key.”" 

    
If ever there was a concept in need of an example, it is this one. BCNF is a bit more 
impenetrable than the first three Normal Forms, but it is still understandable. The 
example below is taken from MariaDb's Docs[16]. It depicts a 'Student Enrollment' table. 
You should make the following assumptions about the attributes in it:


    * Each instructor takes only one course
    * Each course can have one or more instructors
    * Each student only has one instructor per course
    * Each student can take one or more courses 


    +-----------------+----------------------+----------------+
    |     Student     |        Course        |   Instructor   |
    +-----------------+----------------------+----------------+
    | Julian Mives    | Geology 101          | Chris Chao     |
    | Pradeep Connect | Computer Science 203 | John Ike       |
    | Ahmed Kathra    | Philosophy 427       | Richard Mbappe |
    | Golan Goldberg  | Philosophy 427       | Richard Mbappe |
    +-----------------+----------------------+----------------+


For now, this table uses a composite primary key composed of two attributes: 'Student'
and 'Course'. The table complies with 1NF. It complies with 2NF because the 'Student' and 
'Course' attributes both determine the 'Instructor' column. Therefore, the 'Instructor' 
column is fully dependent on both attributes of the composite key. The table also easily
complies with 3NF's demand that no non-key attribute be dependent on any other non-key
attribute: there's only one non-key field, 'Instructor'! So what's the problem? 

Look at the third and fourth rows. There are two students taking Philosophy 427, which 
results in the instructor's name (Richard Mbappe) being stored twice. This is a data 
redundancy problem that the current design of our table fails to eliminate. This happened 
because 'Instructor' determines 'Course', which stated generically is: a non-prime 
attribute determines a prime attribute. In other words, 'Instructor' determines 'Course', 
but 'Instructor' is not a super key. Thus, this table fails to enforce BCNF. 

The solution is to move the 'Course' attribute to a separate table, along with its key.
This leaves only two attributes in the original table, as you can see below: 
 

    +-----------------+----------------+
    |     Student     |   Instructor   |
    +-----------------+----------------+
    | Julian Mives    | Chris Chao     |
    | Pradeep Connect | John Ike       |
    | Ahmed Kathra    | Richard Mbappe |
    | Golan Goldberg  | Richard Mbappe |
    +-----------------+----------------+


This new table is called the 'Student-Instructor' table. After removing the 'Course' 
attribute, you cannot effectively search for records with a single-attribute key. So you 
need to combine both 'Student' and 'Instructor' into a composite key for the table. This 
way, you will be able to uniquely identify records in this table.

The other table is the 'Instructor-Course' table. As the 'Instructor' attribute 
determined 'Course' in the original table, I am making it the primary key in this table.


    +----------------+----------------------+
    |   Instructor   |        Course        |
    +----------------+----------------------+
    | Chris Chao     | Geology 101          |
    | John Ike       | Computer Science 203 |
    | Richard Mbappe | Philosophy 427       |
    +----------------+----------------------+


As you can see, this eliminates the Instructor's name being stored redundantly. 
Decomposing the table into two separate tables has once again solved a normalization
problem.
 

****************************************************************************************************
****************************************************************************************************


SOURCES

01: https://en.wikipedia.org/wiki/Database_normalization
02: https://www.studytonight.com/dbms/database-normalization.php
03: https://mariadb.com/kb/en/library/database-normalization-overview/
https://en.wikipedia.org/wiki/Dependency_theory_(database_theory)
11: https://www.lifewire.com/database-dependencies-1019727
12: https://www.techopedia.com/definition/19504/functional-dependency
13: https://stackoverflow.com/questions/4199444/functional-dependency-and-normalization
14: https://www.techopedia.com/definition/5642/boyce-codd-normal-form-bcnf
15: https://www.vertabelo.com/blog/technical-articles/boyce-codd-normal-form-bcnf
16: https://mariadb.com/kb/en/library/database-normalization-boyce-codd-normal-form/