1. INTRODUCTION TO DATABASE DESIGN CONCEPTS

In this article, I'll talk about how to design a database and the tables in it. Do not be 
fooled by similarities between a spreadsheet and a database table: What you see is only a 
visual representation of data. For instance, beneath the hood, a database identifies 
relationships between pieces of data and enforces constraints between them prevent data
anomalies from making their way in. A spreadsheet program can do some of these things, 
but it is not really designed for this purpose. 

This is just one of the many things that a good database can do. However, you have to 
work with the database to design tables that are resistant to data-related problems. This
article will cover the following topics:


    2.    The Consequences of Poor Database Design
    
    3.    Database Design: The Process
    3.1   Revision: The Parts of a Table
    3.2   Entity-Relationship Modelling
    3.3   Introduction to Database Normalization: Why Do You Need It?
    3.4   Database Normalization: The Process
      
    4.    Dependencies
    4.1   Dependency Theory
    4.2   Prime and Non-Prime Attributes 
    4.3   Functional Dependency
    4.3.1 Trivial Functional Dependency
    4.3.2 Partial Functional Dependency
    4.3.3 Full Functional Dependency
    4.3.4 Transitive Dependency
    
    5.    The Normal Forms
    5.1   Unnormalized Form
    5.2   First Normal Form
    5.3   Second Normal Form
    5.4   Third Normal Form
    5.5   Boyce-Codd Normal Form
    5.6   Denormalization
    
    
Section 2 will explore the consequences of poorly designed databases. Section 3 will go 
through the various stages of designing a database, like Entity-Relationship Modelling 
and Database Normalization. Section 4 and 5 will explain Dependencies and the Normal Forms 
respectively, which are two theory-heavy sets of concepts that are central to Database
Normalization.


****************************************************************************************************
****************************************************************************************************


2. THE CONSEQUENCES OF POOR DATABASE DESIGN 

Poorly designed databases are prone to data redundancies or database anomalies. Redundant 
data is self-explanatory, but what are database anomalies? An anomaly is an aberrant 
behaviour in a database that occurs due to poor design. There are three kinds of database 
anomalies:


    * INSERTION ANOMALY: is a situation that occurs when you cannot insert a new record 
      into a table because the table requires a piece of data that is unavailable to you. 
      
      E.g.: A database that cannot add a customer information to its Customers table 
      until the customer actually makes a purchase.
      
      
    * DELETION ANOMALY: is the opposite of the Insert anomaly. This is a situation where
      the deletion of one entity/record causes unintended deletion of another entity or
      record. Deletion anomalies result in unexpected data loss.
      
      E.g.: Using the previous example, a deletion anomaly occurs if we delete a customer 
      invoice, which due to poor design, causes the deletion of that customer's 
      information. As in the insertion example, this happened because information about 
      two entities, namely customers and invoices, was incorrectly combined into a single
      table. 
      
      
    * UPDATE ANOMALY: arises when the same information can be expressed on multiple rows 
      of a table. This leads to a situation where the same information can exist in 
      multiple records. 
      
      Update anomalies can also refer to a situation where you have a chain of two or 
      more columns with inter-dependencies. In such a situation, updating a column may 
      require you to manually update other columns that depend on its values. If you 
      update the main column but fail to do that for the dependant column, you may get 
      data inconsistencies. 
      
      If a table design permits this anomaly, then over time, it will almost certainly 
      lead to data inconsistencies when one record or field is updated but the other one 
      is not.
      
      E.g.: Suppose you have a Student table where its possible to have a student's 
      information stored in more than one record. If one of these records is updated, 
      say the address is changed, you now have two records for one student, with 
      conflicting addresses. This is a clear sign of design error in your table.
      
      Your tables should have only a single source of truth for an attribute. Attributes 
      should be captured once and stored once in one field only. If needed, you can make 
      references to it in other tables. All references must point to this single, 
      original source.


How does one end up with such anomalies in a database? Well, those of us with experience 
using Microsoft Excel or other tools like it might be inclined at organizing a database 
in the "spreadsheet design" style. However, an RDBMS is not a spreadsheet program, despite
superficial similarities. Therefore, following this approach will only result in a 
database with the data quality of Excel spreadsheets found in any standard business 
setting. Most of these spreadsheets are rife with data anomalies, duplication and data 
inconsistency problems. Spreadsheets are not very good are solving the fundamental 
problems that databases are trying to solve. 

The other big problem with taking a "spreadsheet" approach to database design is that 
this results in tables that fail to take advantage of the database's ability to impose 
data integrity constraints. This can lead to a database design that does not respect the 
'Four Integrities': Entity, Domain, Referential and User-Defined. Even worse than that, 
an unnormalized database does not enforce inter-column and inter-table relationships. If 
you fail to leverage the full powers of an RDBMS, you might as well use a spreadsheet 
program to track your data instead of an database!

No, if you want to use a database effectively, you have to leave the spreadsheet 
mentality behind and learn some new skills. Because a database that is poorly designed 
will not guard against the data-related problems I have discussed. Such databases will be 
filled with low-quality, inconsistent data. Trying to update it will lead to errors and 
maintenance will be ongoing chore. 

The best way to prevent anomalies is to design your database correctly via 'Entity-
Relationship Modelling'. After doing that, you have to apply the process of 'Database 
Normalization' to improve its formal internal structure. The rest of the article is 
concerned with these two process and others related to them. 


****************************************************************************************************
****************************************************************************************************


3. DATABASE DESIGN: THE PROCESS

3.1 REVISION: THE PARTS OF A TABLE

I'll start this section with a short refresher on the component parts of a tables. When 
you store data in a database, you usually store it in a relation, which is another way of 
referring to a table. Take a look at the example table below, called 'Students':


    +-----------+-----------+----------+----------------+--------------------------+
    | StudentId | FirstName | LastName |  PhoneNumber   |          Email           |
    +-----------+-----------+----------+----------------+--------------------------+
    |    293843 | Harold    | Kumar    | (298) 239-3894 | haroldkumar@yahoo.com    |
    |    293494 | Jenny     | Spice    | (298) 830-9238 | j_luvbabyspice@yahoo.com |
    |    293939 | Jacob     | Yycomb   | (298) 832-2398 | jacob.yycomb@gmail.com   |
    |    394853 | Victoria  | Gillium  | (298) 382-3982 | vgill@gmx.com            |
    +-----------+-----------+----------+----------------+--------------------------+


The table above is designed to capture information about students. This is done with 
attributes, also called columns, which are the names of various properties that an entity 
might have. Columns are listed horizontally at the top of a table. The 'Students' table 
has 5 attributes: 'StudentId', 'FirstName', 'LastName', 'PhoneNumber', and 'Email'. Each
attribute captures certain piece of information about a student. All information 
pertaining to an attribute have to be stored in its column. For instance, you cannot 
store StudentId information in the 'PhoneNumber'; this a constraint that is enforced by 
the database. 

In RDBMSes, a Row or a Record, refers to a horizontal block of named elements that 
contain information on a single entity. In this table, each record contains information 
on a student. The first record in this table, that of Harold Kumar, contains information 
required by the five attributes at the top. Similarly, this table is filled with other 
unique student records. Or at least, that should be the case, if there are no duplicate
records in it. Each record has the same structure; it contains information required by 
the columns. 
 
ASIDE: Sometimes, records are referred to as tuples. A tuple is a formal term that is 
distinct concept but it is related to a record (or a row).

The intersection of a column and a row is a Field. This is where an individual piece of 
data is to be stored. In the example table, you should see meaningful pieces of data in 
all of the fields.  

Finally, there is a way to represent a relation and its attributes. There is the generic
format:


    TableName(Attribute1, Attribute2, Attribute3, Attribute4...)
    

Therefore, the table above can expressed as: 


    Students(StudentID, FirstName, LastName, PhoneNumber, Email)  


3.2 ENTITY-RELATIONSHIP MODELLING

When you sit down to design a new database, the first thing you will have to do is 
something called 'Entity-Relationship Modelling'. ER Modelling is dominated by a "big 
picture" consideration of your program's data needs. Bearing in mind these requirements, 
ER-Modelling is the process of determining the entities/relations (tables) in your 
database, their attributes (columns) and and their relationships to other entities 
(One-to-One, One-to-Many, Many-to-Many) relationships.

At this time, you should also be considering how to protect data integrity by implementing
the 'Four Integrities'. Modern RDBMSes are designed to help you easily implement these 
safeguards. Practically speaking, what does that mean? You can enforce these constraints
as follows: 


    1. Entity Integrity Constraints: are enforced by using primary keys to ensure that no 
       two rows in your table have identical values for all columns. At the very least, 
       a table must have at least one (set of) attributes with unique values. This role 
       is usually played by primary key columns, which must contain unique values for 
       every record. With the help of a primary key, you should be able to uniquely 
       identify any record in the table.
       
       When you design a table, you must give it a primary key (or a composite key). 
       
       
    2. Domain Integrity Constraints: are enforced by defining a set of constraints for an
       attribute. When adding columns to a table, remember to give them unique names.   
       You have the responsibility of setting constraints on the data stored within a 
       column. One constraint that you must impose is the column's data type. This is 
       critical to Domain Integrity.  
       
       You can also set other limitations if you so choose, such as formatting or range 
       restrictions on the values. Another thing you can do is you can direct an 
       attribute to accept NULL values. 
       
       
    3. Referential Integrity Constraints: When you want to create inter-table 
       relationships, you need to have primary keys in the principal table and foreign 
       keys in the dependent table. If you declare and register these keys with the RDBMS
       when you create tables, you can have it enforce referential integrity constraints. 
       This constraint will check to see whether every foreign key value in the dependent 
       table has a corresponding primary key in the principal table. 
       
    4. User-Defined Constraints: This constraint is optional. However, if you wish to 
       have legal requirements or business process rules codified into constraints, you
       can also do it via CHECK constraints and TRIGGERS. An RDBMS will enforce these
       constraints along with the other three, further improving data quality.


At the completion of the ER-Modelling process for your database, you should have 
identified tables and their columns. You should also identified the nature of 
relationships between two tables. And to enforce integrity constraints, you should have 
declared a primary or composite key in every table and a foreign key whenever two tables 
have to be connected. Within tables, you should have carefully considered and assigned 
data types and restrictions to every attribute. 


3.3 INTRODUCTION TO DATABASE NORMALIZATION: WHY DO YOU NEED IT? 

Here is an introductory description of Normalization from Wikipedia's article[1]:


    Database normalization is the process of restructuring a relational database in 
    Database normalization is the process of restructuring a relational database in 
    accordance with a series of so-called normal forms in order to reduce data redundancy 
    and improve data integrity. It was first proposed by Edgar F. Codd as an integral 
    part of his relational model.
    
    Normalization entails organizing the columns (attributes) and tables (relations) of a 
    database to ensure that their dependencies are properly enforced by database 
    integrity constraints. It is accomplished by applying some formal rules either by a 
    process of synthesis (creating a new database design) or decomposition (improving an 
    existing database design).


Here's an alternate definition[2]:


    Database Normalization is a technique of organizing the data in the database. 
    Normalization is a systematic approach of decomposing tables to eliminate data 
    redundancy (repetition) and undesirable characteristics like Insertion, Update and 
    Deletion Anomalies. It is a multi-step process that puts data into tabular form, 
    [and] removing duplicated data from the relation tables.
    
    Normalization is used for mainly two purposes,
    
    * Eliminating redundant data, AND
    * Ensuring data dependencies make sense i.e data is logically stored.
    

What are some of the advantages of normalizing your database?[3]:


    * Eliminate data redundancies (and therefore use less space)
    * Make it easier to make changes to data, and avoid anomalies when doing so
    * Make referential integrity constraints easier to enforce
    * Produce an easily comprehensible structure that closely resembles the situation the 
      data represents, and allows for growth
       
       
Before I talk about Normalization, let's back up a little bit. In the previous section, 
you should already have arrived at a preliminary design for your database. However, you 
are still not fully taking advantage of an RDBMS's capabilities to manage inter-attribute 
relationships. What that does mean mean? 

An attribute's values may have one of several relationships to another attribute. For 
instance, if you have a 'Social Security Number' column in an properly designed 
'Employees' table, it will map on to one and only one value in 'EmployeeName', 
'DateOfBirth' and perhaps other columns. This is a logical constraint that exists between
two attributes, and the general term for such inter-attribute relationships is 
'Dependency'. In this case, the existence of this dependency is a very useful property: 
You can use the 'SSN' column to search the table and uniquely identify employee records. 
There are a number of useful dependencies like this one that can occur between columns 
in a table. 

The trouble is that a medium-to-large size database can have thousands of tables, each 
with dozens of attributes. It could contain tens of millions of records. If you did not 
normalize this database during design, you are stuck with a poor-quality database that 
is prone to serious data integrity, consistency and other issues. You should expect to 
constantly run into data anomalies, which have now become your responsibility to 
mitigate. You will have to manually ensure, on a ongoing basis, that none of the data 
entered into the database is of low quality. You will have to make sure that every time 
you change a value in a table, that you change all the values that depend on it. You will 
have to skirt every pitfall and avoid every anomaly - all by hand. 

Let me tell you at the outset that there is no hope of manually preventing anomalies in a 
database of this size. There are so many ways to make mistakes that lead to data 
redundancies, inconsistencies and even data loss, that no human could possibly succeed at 
this task in a database of this size. In fact, manually keeping data-related problems out 
of a much smaller database is just as doomed a task. In short, if you cannot think of a 
soul-crashing responsibility that is doomed to failure, trying to manually maintain data 
quality in an unnormalized database is a good example.  

The wiser course of action is to design such problems out of existence. In the 'ER-
Modelling' phase, you used the RBDMS engine's built-in capabilities to enforce the 
'Four Integrities' throughout the database on a continual basis. In Normalization, you 
are trying to do something similar, though you should note that the RDBMS cannot help you 
enforce the management of dependencies as much as it could the various integrities. This, 
then, is the general goal of Database Normalization: to use formal methods to study your 
database, re-design tables and attributes until your database has a structure that 
resists anomalous data issues.

By doing this, you are moving from merely a spreadsheet style collection of information, 
to Structured Data. Structured data has well-defined attributes which give it a clearly 
defined form. For instance, in a table, every record must have values for mandatory 
columns. Every attribute must have the correct relationship to other attributes, in 
accordance with the formal rules. By doing these things, you are imposing structure on 
on what is just a heap of data.   

Normalization should lead to a much more robust database design, one in which you are 
doing as little work to maintain data quality as possible. A solid database design should 
pro-actively prevent the majority of data-quality issues which plague databases. 
Normalization will make your database easier and more reliable to work with. It will 
reduce unwanted duplicate data in the database, thereby decreasing your database's size. 
It will render many classes of anomalies impossible, increasing the integrity, validity
and robustness of the data stored therein. This will make it harder to create invalid 
states in the database or enter poor quality data into it. This should make it easier 
to query, modify and maintain your database.

For all these reasons, you should normalize every database you design. You should do 
normalization as part of the initial design and whenever you revisit and tweak your 
database's schema. 


3.4 DATABASE NORMALIZATION: THE PROCESS

So how do you normalize your database? What does the process look like? This depends on
whether you have an existing unnormalized/partially normalized database, or if you are 
starting to design a fresh, new database. Applying Normalization to the former is called
Decomposition while doing it to the latter is called Synthesis. The same principles apply 
to both situations.
   
Before you can start normalization, you should have completed the 'ER-Modelling' phase. 
You should have a provisional database design in hand. 

Normalization is applied at the "micro level". The general idea behind Normalization is 
that a table is about a specific topic and that only those columns which are related to 
this topic must be included within it. Therefore, during the Normalization process, you 
will focus on one entity (table) and study its attributes, looking to find attributes 
that just do not belong in a table. You will be asking this question: Are any of the 
columns in the present table not directly related to its main topic?

In order to do this, you need to understand the nature of dependencies that may occur 
between columns. To determine if a column belongs in a table, you will study its 
relationships to other attributes. Both "good" and "bad" kinds of dependencies can 
exist between attributes. "Good" dependencies are a sign that the two sets of attributes 
in question belong to the table and are in good standing with respect to each other. 
"Bad" dependencies indicate that columns don't have an acceptable relationship to other
attributes, suggesting that one or more of these columns are extraneous to the table.
When you find extraneous attributes, you should move them out into a new table. That's
right: the solution to a normalization issues is usually the creation of a new table. 

The goal is to identify and keep the set of columns that have "good" dependencies, which
contribute to promoting data integrity and validity in your database, while getting rid 
of negative dependencies that detract from this goal. The result should be that every 
table only contains the attributes it needs. Each attribute will only positive 
dependencies on other attributes, as all the columns with negative ones should have been 
moved to other tables or deleted completely. Remember that by doing this, you will get a 
database design that is very resistant to many classes of data integrity issues. 

In order to successfully apply Normalization on a database, you have to understand these
concepts and their many sub-concepts:


    1. DEPENDENCY THEORY, especially as it relates to Functional Dependencies
    2. THE NORMAL FORMS, which are a series of requirements imposed on inter-attribute
       relationships.
       

The first topic explains attribute dependencies. You need to understand the nature of 
dependencies and recognize them so that you can work out whether they are "good" or "bad". 
The 'Normal Forms' are a set of formal rules that will help you remove columns with "bad"
dependencies. There are half of dozen of them, and implementing each one will bring you
one step closer to soundly designed table. Apply them to every table in the database and 
you will have a robust database design.


****************************************************************************************************
****************************************************************************************************


4. DEPENDENCIES

4.1 DEPENDENCY THEORY

As I just explained, within a table, the data in one attribute(s) may have certain 
relationships to data from other attribute(s), which are called dependencies. The 
process of database normalization requires you to understand dependencies and root out 
the "bad" ones by removing associated columns from the table. To do this, you must 
understand the nature of the attribute relationships.

Dependency theory is the field that deals with such relationships. The Wikipedia article
provides a somewhat opaque definition[4]:


    Dependency theory is a subfield of database theory which studies implication and 
    optimization problems related to logical constraints, commonly called dependencies, 
    on databases. The best known class of such dependencies are functional dependencies, 
    which form the foundation of keys on database relations. Another important class of 
    dependencies are the multivalued dependencies.
    
    
So there exist logical constraints between two attributes or more, which are called 
DEPENDENCIES. There are several major kinds of dependencies: Functional, Multi-Valued and 
Join. This article will only consider the most common type of dependency, which is 
Functional Dependency and its sub-types. That is because this concept is important to the 
types of Normal Forms which are covered later in this article: The first three Normal 
Forms and the Boyce-Codd Normal Form. Multi-Valued and Join dependencies apply to later 
Normal Forms. 


4.2. PRIME AND NON-PRIME ATTRIBUTES

Before getting into 'Functional Dependency', I need to bring up the topic of Prime and
Non-prime attributes. 

A candidate key is any attribute or combination of attributes that can uniquely identify 
a record. Keys that can do this are candidates to become the primary key of a table. This 
is where the name 'Candidate Key' comes from. In a table, there may be several candidate 
keys, from which you have to select one as primary key.

Attributes (columns) that are part of any candidate key are called PRIME ATTRIBUTES or 
KEY ATTRIBUTES. The other attributes from that table are called NON-PRIME ATTRIBUTES or 
NON-KEY ATTRIBUTES.


4.3 FUNCTIONAL DEPENDENCY

In the most general terms[5], "a dependency is a constraint that applies to or defines 
the relationship between attributes." As I said before, there are several kinds of 
dependencies, but this section and the rest of the article will concern itself only with
'Functional Dependency' and its variations. A Functional Dependency is a type of 
relationship that exists between two sets of attributes. Usually, but not always, this 
relationship occurs between a Primary Key (PK) attribute set and a second set of 
non-prime attribute(s) within a table. 

Let's start with an simple and informal definition:

Assume that there are two columns in a table. If you can use the value from one column to 
uniquely identify the a value in the second column, then a functional dependency exists 
between these two columns. 

I'll re-iterate the example of Employee table. Consider this relation: 


    Employees(Social Security Number, FirstName, LastName)
    

So let's be very clear about the relationship between 'Social Security Number' and
'FirstName'. A functional dependency exists between these two attributes. The 'Social 
Security Number' attribute in this table has the special ability to uniquely identify 
values in the other two columns. This should make some sense; in any country, there are 
often multiple people with exactly the same name. Social Security Numbers are designed to 
be unique - one SSN identifies exactly one person. Any attribute which can uniquely 
identify the value of another attribute is called a DETERMINANT.

In this relationship, then, the 'Social Security Number' column is said to be the 
Determinant attribute, while the 'FirstName' column is said to be DEPENDANT attribute. 
'FirstName' can also be called the DETERMINED attribute. The same relationship exists 
between 'Social Security Number' and 'LastName', with the former once again being the 
Determinant while the latter is Dependant. The SSN column is said to determine the other 
two columns, and in return, the other two are said to be dependant on SSN. 

This relationship is expressed like this: 


    Social Security Number -> FirstName
    Social Security Number -> LastName


This is what it looks like generically: 


    A -> B 


The left hand side of this Functional Dependency diagram contains the determinant set and
the right hand side is the determined set. It is read like this from left to right: "A 
(functionally) determines B" OR "A derives B". The Inverse: "B depends on A." Functional 
dependencies occur when there is only one value in the determined set for a given value 
of the determinant set. 

Do note, that in real tables, keys can be comprised of multiple columns, which is where 
you might become confused. So, you can have a set of columns that form the 
determinant/primary key/candidate key.

Columns or collections of columns that can serve as determinants in that table are 
excellent candidates to become Primary key. Thus, determinant columns are either 
candidate keys or the primary key. As I noted before, columns that are part  
of the part of a candidate key or a primary key are called prime attributes and the 
columns that are not part of such keys are called non-prime attributes. 

Let's look at the formal definition of a functional dependency: 

For two arbitrary sets of attributes, A and B, set B is said to be functionally dependent 
on set A, if at any moment in time there is exactly one instance of B for any given 
instance of A. This is a rather formal explanation, but here it is again phrased another 
way: If Attribute set A functionally determines Attribute set B, then the value of an 
instance of A uniquely determines a value for B. 

And a less formal, generic description:  If A determines B, then a value of A can 
uniquely identify a value of B. Therefore, A is said to be a determinant of B and B is 
said to be a dependent of A. 

Why is the concept of functional dependence important? 

In a StackOverflow answer, NealB explains[6]:


    Sets of functional dependencies may be used to synthesize relations (tables). The 
    definition of the first 3 normal forms, including Boyce Codd Normal Form (BCNF) is 
    stated in terms of how a given set of relations represent functional dependencies. 
    Fourth and fifth normal forms involve Multi-Valued dependencies (another kettle of 
    fish).


Normalization applies what are called 'Normal Forms' to tables. The second and third 
normal forms as well as the Boyce-Codd Normal Form (which is considered to be an extension
of Third Normal Form) require an understanding of functional dependencies. 

During Normalization, one of the first things you will have to do when studying a table 
is to identify the prime and non-prime attributes. Then you will study the nature of the 
dependencies that exist between them. There are certain kinds of functional dependencies
that are undesirable in a table, indicating the those columns must be removed to an other
table. 

For this reason, you will need to know what sub-type of functional dependency exists 
between two sets of columns. There are several sub-types of functional dependencies, 
which are discussed in the sections that follow.


4.3.1 TRIVIAL FUNCTIONAL DEPENDENCY

A trivial functional dependency is an obvious dependency. It occurs when the determined 
set is a subset of its determining set. Formally, it is called the 'Axiom of Reflexivity'.

For instance, consider a primary key composed of two attributes, 'Social Security Number' 
and 'EmployeeName'. Such a key is called a composite key. By searching the 'EmployeeName'
column for records that match the values of both attributes, the composite key can 
uniquely identify the name of an employee in the 'EmployeeName' column. This is the 
Functional Dependency diagram for this relationship:


    {SSN, EmployeeName} -> EmployeeName
    
 
This is where Trivial Dependency occurs: The composite key can uniquely identify an 
'EmployeeName' because this attribute is a part of the composite key. If you already
know the name of you employee you are searching for, it can play a part in finding it. 
This follows from common sense. So if this seems obvious, it is because it is. A trivial 
dependency occurs because 'EmployeeName' is part of both the determining and determined 
sets. 
  
  Generically, it is expressed as follows:


    {A, B} -> B 
    

4.3.2 PARTIAL FUNCTIONAL DEPENDENCY

Partial dependency occurs when the determinant set is only partially dependant on the
determining set. Partial dependency is formally called the 'Axiom of Augmentation'.

Consider this example table: Store(StoreId, Product, Price). First, identify the prime 
and non-prime attributes in this table. To do that, you have to figure out the candidate
key(s). Neither 'StoreId' nor 'Product' can uniquely locate a record, so we have to 
create a composite key by combining these two attributes together. 'Price' is the 
dependant attribute and thus, it is a non-prime attribute. 

Thus, this is the FD diagram for this table:


    StoreId, Product -> Price


'StoreId' and 'Product' combined form the determining set, which determines the value for
'Price', the determined set. However, if you pay attention, you will note that 'Price' is
only partially dependant on the determinant set. Why? Because in this example, the 
'StoreId' does not play a part in determining the 'Price', only 'Product' does. That 
makes 'StoreId' extraneous to the determinant set. Despite the inclusion of 'StoreId' in 
the determining set, the true nature of functional dependency in this table looks like 
this:


    Product -> Price


Partial Dependency is a "bad" dependency. The existence of partial functional dependency
between two columns violates Second Normal Form. It is something that you must strive to 
eliminate from your database, or it will lead to data inconsistency errors. 


4.3.3. FULL FUNCTIONAL DEPENDENCY 

Before discussing the Full Functional Dependency, I want to talk about the concept of a
'Minimum Viable Determinant Set'. When you create a determinant set, you can add 
extraneous attributes to it, as the partial dependency example demonstrated. This does
absolutely nothing useful, but it can serve as a source of confusion about nature of 
dependencies in your table as well as open the door to data inconsistencies and 
redundancies. Remember, partial dependency is not to be embraced, but eliminated. 

Therefore, when you are creating candidate keys, it is necessary to reduce the set of 
attributes to the bare minimum required to uniquely identify records. If you remove any 
more attributes from this minimum set, you will lose the ability to uniquely identify 
records, which is to say, you will break functional dependency. I call this the 'Minimum 
Viable Determinant Set' (MVDS).

'Full Functional Dependency' occurs when the conditions for functional dependency are met 
AND the determinant set is an MVDS. In this situation, the dependant set is functionally 
dependant on the entirety of the determinant set, not merely on a subset of it. To 
rephrase: Full dependency describes a situation where dependent attribute(s) in a relation
is dependant on ALL the component attributes of the determinant attribute(s). The operative 
word here is "All". 'Full Functional Dependency' is a "good" sub-type of dependency.

Consider an 'Employee' relation with three attributes, 'EmployeeId', 'EmployeeName' and 
'Location'. It can be represented like this: Employee(EmployeeId, EmployeeName, Location). 
'EmployeeId' is the primary key and the other two columns are non-key attributes. In this 
example, the second and third attributes are fully dependent on the 'EmployeeId' 
attribute. The relationship looks like this:


    EmployeeId  ->  EmployeeName, Location 
    

Because 'EmployeeId' uniquely identifies employees, it functionally determines a value 
for 'EmployeeName' and 'Location'. 'EmployeeName' and 'Location' each are fully dependant 
on the entirety of the determinant set, which in this case is a single attribute primary 
key. 

This is a simple case because the determinant set has only one attribute, but more 
complex cases arise when the determinant set is composed of many keys. Indeed, violations
of Second Normal Form occur only when you have multi-attribute primary keys. Thus, when 
you have a composite primary key, you have to ensure that every dependent attribute 
depends on every attribute in the composite key. By doing this, you eliminate partial
dependencies and bring Full Functional Dependency to the attributes in your table. This
should bring it into compliance with Second Normal Form.


4.3.4 TRANSITIVE DEPENDENCY

Before discussing Transitive Dependence, let's first look at the 'Axiom of Transitivity' 
that is core to this idea. This axiom states the following:


    If A -> B, and B -> C, then A -> C
    
    
The term 'transitive' in mathematics and logic, simply says that if some relationship 
(such as equality, greater than, less than or functional dependency) exists between the 
first and second elements, and also exists between the second and third elements, this 
same relation will also hold between the first and third elements. Take a moment to 
absorb this concept before proceeding. 

In a table, Transitive Dependence occurs when a primary key cannot directly identify 
an attribute. Instead, the primary key determines a non-prime attribute, which in turn 
determines the value of a second non-prime attribute. Therefore, when transitive 
dependence occurs, there is an indirect relationship that causes a functional dependency. 
The Functional Diagram looks like this:


    A -> B -> C

    
Where A is a prime attribute, and B and C are non-prime attributes. This concept is a 
bit tricky, so let's jump into an example. Take a look at the relation below, which was
taken from Wikipedia: 


        +----------------------+------+-----------------+------------------------+
        |      Tournament      | Year |     Winner      | Winner's Date of Birth |
        +----------------------+------+-----------------+------------------------+
        | Indiana Invitational | 1998 | Al Frederickson | 21 July 1975           |
        | Cleveland Open       | 1999 | Bob Albertson   | 28 September 1968      |
        | Des Moines Masters   | 1999 | Al Frederickson | 21 July 1975           |
        | Indiana Invitational | 1999 | Chip Masterson  | 14 March 1977          |
        +----------------------+------+-----------------+------------------------+


The primary key is a composite one, made up of 'Tournament' and 'Year', making 'Winner' 
and 'Winner's Date of Birth' non-prime attributes. The problem with this table is that
the composite key has a transitive dependency relationship to the other two attributes.
The composite key determines the 'Winner' attribute, which in turn determines the 
'Winner's Date of Birth'.  

Why is this a problem? Well, oddly enough, when you have three attributes that are in a 
state of Transitive Dependance, the relationship stops being Transitive. More 
specifically, this happens when the middle attribute is a non-prime attribute. In the 
example above, due to the fact that 'Winner' is non-prime, the composite key does not 
determine the 'Winner's Date of Birth' column. The Functional Dependence relationship 
has broken, and sure enough, if you check the last column, you will find duplicate values 
in it. 

If the 'Winner' column of any record is changed, you have to manually update the 
'Winner's DOB' column and vice versa. If you fail to ensure that the values have been 
simultaneously updated, you will get data inconsistencies on top of data duplication 
issues. As I have said before, if you have a human in charge of cross-checking records to 
prevent anomalies, you know how this situation is going to end: the person will fail and 
the database will fill up with rubbish. This, in turn, will lead to update anomalies.

Therefore, transitive dependencies are "bad" - they violate Third Normal Form. In a well-
designed table, you have non-prime attributes directly depending on prime attributes. 
Three-part dependencies seem lead to anomalies. For this reason, a good database design 
needs to eliminate indirect dependencies on non-prime columns. In this table, you can 
eliminate this problem by creating a new 'Winners' table and moving the 'Winner's DOB' 
attribute into it. You can use the add a 'Names' column and populate it with the names 
of the winners. 


****************************************************************************************************
****************************************************************************************************


5. THE NORMAL FORMS

The concept of Normal Forms was developed by Edgar Codd, the father of relational 
databases. Normalization theory defines 6 major Normal Forms and a number of minor forms. 
The first three Normal Forms are treated like an essential block. If you bring your 
database into compliance with the first three forms plus the Boyce-Codd Normal Form, you 
will have prevented well over 90% of data anomalies in most databases.   

Therefore, you should apply the first three forms to all databases. Following that, you 
should consider the Boyce-Codd Normal Form (BCNF or 3.5NF), which is considered to be a 
minor extension to the Third Normal Form. The fourth and fifth forms come into play only 
when you have multi-value dependencies, which is rather rare. The sixth form seems to be 
treated like a theoretical oddity.  

The Forms are mainly properties of a relation. Therefore, you have to scrutinize every 
table in your database and apply the forms to each one in order to give your database a
good design.


5.1 UNNORMALIZED FORM (UNF or 0NF)

This is the state of a database before you have Normalized it. An unnormalized database 
will suffer from data redundancy, inconsistency, integrity and validity issues due to 
their violation of the Normal Forms. 

As you start the Normalization process, remember these guidelines: 


    * A table is about one topic. Create a table for each set of related data. Do not 
      forget to give it a meaningful name. 
      
    * Moving on, Normalization is about removing columns that do not belong in a table. 
      When you analyze a table and find unnecessary dependencies and columns, remove the 
      troublesome columns to a new table or re-evaluate their existence.

    
5.2 FIRST NORMAL FORM (1NF)
   
These are the criteria of First Normal Form:

      
    * In a relation, a field is the intersection of a row and a column. It is the most 
      fundamental unit of storage in a database. You can store a data fragment in it.
      This piece of data must be atomic, i.e. a piece of data that cannot or should not 
      be divided into smaller pieces. (For example of the latter case, you can divide 
      date values into separate year, month and date columns, but this is widely regarded 
      as a poor idea.) 
      
    * A field must, at most, contain only one piece of data. 1NF prohibits multi-value 
      fields. This applies to all fields in all tables. Do not use commas, semi-colons or 
      other delimiters to put multiple values into a field. Databases treat multi-value
      fields as a single value. 
      
      Applying 1NF lets the database track, store and display data in a tabular format. 
      Violating 1NF will mean that the database is no longer tracking the extra values in
      multi-value fields. This will prevent the database from applying domain integrity 
      constraints to them. This will lead to an increase in the amount of low-quality 
      or inconsistent data in your database.
    
    * Eliminate repeating groups from tables. Repeating groups are columns with the same
      name, but different numbers at the end. They are a special case of multi-value 
      fields. They usually occur when you try to get rid of multi-value fields by 
      creating new columns. The best way to eliminate repeating groups is to study the 
      table, find and identify the set of data related to repeating groups and move them 
      into a new table. 
          
    
This principle is hard to understand in the abstract, so consider the table below[7]. It 
is an 'Employee' table, which contains a list of employees, their Ids, names, emails etc. 
Each employee may be issued corporate devices like laptops and phones, whose serials need 
to be tracked. This will be done in the last column, which is called 'DeviceSerial'.


    +------------+-----------+----------+-----------------------+--------------+
    | EmployeeId | FirstName | LastName |         Email         | DeviceSerial |
    +------------+-----------+----------+-----------------------+--------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D |
    |       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 |
    |       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8 |
    +------------+-----------+----------+-----------------------+--------------+


So far, this table lets the company track the devices they given to employees and as a
bonus, complies with 1NF. However, what happens if employees are given more than one 
device? The easiest solution to the problem is to add additional devices to the 
DeviceSerial column, separated by commas and other delimiters. This is what it would look
like:


+------------+-----------+----------+-----------------------+--------------------------------------------+
| EmployeeId | FirstName | LastName |         Email         |                DeviceSerial                |
+------------+-----------+----------+-----------------------+--------------------------------------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D, SD8GD8DFHFH0                 |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4 | RIQO98L239DS | 0J78G8J8G7GJ |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8                               |
+------------+-----------+----------+-----------------------+--------------------------------------------+


However, this is something you should NEVER do in a database. It is a violation of 1NF. 
Relational Databases can handle many things, but they are not equipped to handle 
multi-value fields. Multiple values in a field get treated like a single value. Searching 
for data becomes extremely difficult in a database that fails 1NF compliance. The bigger 
problem is that this practice retards a database's full abilities to categorize and 
manage data. If you use multi-value fields, you are now responsible for enforcing data 
types, and other data integrity constraints for potentially millions of records. So, 
don't go there. Let your database manage the data and enforce types and constraints. All 
you have to do to offload this responsibility to the database is put a maximum of one 
value in a field.  

However, we still have multiple values to track. How else can you track them? The next 
obvious solution is to add additional columns for devices, so that you can track one 
device per column. See the table below:


+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
| EmployeeId | FirstName | LastName |         Email         | DeviceSerial1 | DeviceSerial2 | DeviceSerial3 |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+
|       0001 | Jack      | Hill     | jack_hill@hotmail.com | AD9F8AFAF98D  | SD8GD8DFHFH0  |               |
|       0002 | Jill      | Hill     | jhill@gmail.com       | AD89AFLL35L4  | RIQO98L239DS  | 0J78G8J8G7GJ  |
|       0003 | Simon     | Says     | ss@outlook.com        | SDF90AF87GG8  |               |               |
+------------+-----------+----------+-----------------------+---------------+---------------+---------------+


Creating multiple columns in the vein of DeviceSerial1, DeviceSerial2, DeviceSerial3 etc., 
is called a REPEATING GROUP. The existence of repeating groups also violates First Normal 
Form. Therefore, this is also something you should NEVER do. The classic sign of a 
repeating group is identical column names with different numbers tacked on the end of it 
to make unique names. 

Repeating groups are a sign of inflexible design; they do not scale well. What happens 
when employees from the quality control department need to be given 10 different devices 
each? Giving an employee a new device should not necessitate changes to a database's 
schema. 

No, the correct response to such a problem is to create a new table that tracks devices,
with 'EmployeeId' as foreign key. You can move the 'DeviceSerial' column to this new 
table and add new ones, like 'DeviceType' and 'Description' to help the business better 
track its devices. After doing this, you can establish a one-to-many relationship between 
the 'Employee' and 'Devices' table, where the 'One' side is the 'Employee' table and the 
'Many' side being the 'Devices' table. This is the modified 'Employees' table may look 
like:

    
    +------------+-----------+----------+-----------------------+
    | EmployeeId | FirstName | LastName |         Email         |
    +------------+-----------+----------+-----------------------+
    |       0001 | Jack      | Hill     | jack_hill@hotmail.com |
    |       0002 | Jill      | Hill     | jhill@gmail.com       |
    |       0003 | Simon     | Says     | ss@outlook.com        |
    +------------+-----------+----------+-----------------------+


This is what the 'Devices' table might look like: 

    
    +--------------+------------+------------+-----------------------+
    | DeviceSerial | EmployeeId | DeviceType |      Description      |
    +--------------+------------+------------+-----------------------+
    | AD89AFLL35L4 |       0001 | Iphone     | 5th gen Apple iPhone. |
    | AD9F8AFAF98D |       0003 | Printer    | HP Printer #2839      |
    | SDF90AF87GG8 |       0003 | Laptop     | Dell Latitude 910     |
    | 0J78G8J8G7GJ |       0002 | Camera     | Canon XTS Pro 510     |
    +--------------+------------+------------+-----------------------+
    
    
You can use 'EmployeeId' to query the 'Devices' table and find a list of all devices that
a given employee had been loaned. With this design modification, there are no repeating 
groups in tables or multipe values in any field.

Note that the trouble with the original table was that table's design was too broad: 
extraneous columns were stuck in there. The 'Employees' table should not contain data 
from the 'Devices' table. Often, as was the case in this example, the typical solution to 
a normalization problem is the creation of a new table.
  
    
5.3 SECOND NORMAL FORM (2NF)

These are the requirements for Second Normal Form: 
      
    * The first requirement for Second Normal Form is that the table is in First Normal
      Form. Full compliance with 1NF is a mandatory requirement for full compliance with 
      2NF.
    
    * Second Normal Form is concerned with the relationship between Key columns and 
      Non-key columns. More specifically, 2NF is only a problem for tables that use a 
      composite primary key. A composite key is a primary key that is composed of values 
      from two or more columns. It appears that composite keys are used when there are no 
      candidate keys in a table, i.e., no single attribute that can determine the values 
      of the other non-prime attributes. If you use a composite key, you must pay close
      attention to its relationship to other non-key columns in that table. If a table 
      has a single-attribute primary key, which is the case most of the time, then you do 
      not have to worry: the table is automatically compliant with 2NF. 
    
      But what happens when you have a composite key? As a composite key is composed of 
      two or more attributes, it is possible for some of the non-prime attributes in a 
      table to be dependent on only some of the component attributes of the composite 
      key. This happens when the composite key is not a Minimum Viable Determinant Set. 
      The composite key has extraneous columns in it, which causes Partial Functional 
      Dependence between the composite key and (some of) the non-prime elements in
      the table. This is a violation of 2NF. 
      
      If your table does use a composite key, 2NF demands that all non-prime attributes
      must be dependant on the entire key. Stated another way, non-key attributes must be 
      fully functionally dependent on all constituent attributes of the composite key. 
      
      Therefore, to effect 2NF, you will have to ensure that all non-prime attributes in 
      a table are in a state of Full Functional Dependence on the composite key. To do
      this, remove all extraneous columns from the composite key and make it an MVDS. 
      
      Then, check the relationship between a non-prime column and the all the columns in 
      the composite key to see if is fully dependant on all of them. Repeat this process 
      for all the non-prime attributes. 
    
    
Once again, this is hard to understand without a actual example, so consider the example
table below[7]:

    
        +--------+-----------+------------------+------+----------+-----------+
        | Course |   Date    |   CourseTitle    | Room | Capacity | Available |
        +--------+-----------+------------------+------+----------+-----------+
        | SQL101 | 3/1/2013  | SQL Fundamentals | 4A   |       12 |         4 |
        | DB202  | 3/1/2013  | Database Design  | 7B   |       14 |         7 |
        | SQL101 | 4/14/2013 | SQL Fundamentals | 7B   |       14 |        10 |
        | SQL101 | 5/28/2013 | SQL Fundamentals | 12A  |        8 |         8 |
        | CS200  | 4/15/2012 | C Programming    | 4A   |       12 |        11 |
        +--------+-----------+------------------+------+----------+-----------+


This is an 'Events' table for a school. It uses a composite primary key that was created 
by combining the 'Course' and 'Date' columns. This was done because the 'Course' column 
alone cannot uniquely identify a course due to the fact that the another class for this 
course can run on another date. However, if you combine the first two columns, this 
combination of values can help you locate a specific course. Thus, this combination of 
attributes can act as a primary key. 

As Second Normal Form requires us to pay close attention to non-prime keys if you have a 
composite key, lets take a closer look. Is there a problem with this table? Yes. The 
'Room', 'Capacity' and 'Available' columns depend on both parts of the composite key, but 
this is not true of the 'CourseTitle' column. You can determine the 'CourseTitle' from 
just the first part of the composite key. Thus, this is a case of Partial Dependence, 
which clearly violates 2NF.  

Why is this a problem? Well, a Partial Dependency indicates potential for data redundancy
and inconsistency. Whenever you have 'SQL101' for the first column, that course will 
always refer to 'SQL Fundamentals'. Why have two columns saying the same thing? What 
happens if someone updated one of these columns but not the other? What if you had a 
record claim 'SQL101' as the Course ID and 'Advanced SQL' for the 'CourseTitle'? Having 
a Partial Dependency created a redundant column in this table, which opens the door to 
data inconsistencies. Multiply the number of the tables in your database by number of 
records in each table to get a feeling for how many flawed records you get if you decline
to implement 2NF. You do not want to be in charge of finding and correcting all these 
records by hand. As you can see, this is not just an academic point, it indicates a 
structural weakness in the design of your table. The best thing you can do is design this 
table in such a way that these problems cannot manifest.  

How do you do that? By applying 2NF: All non-key columns must depend on the entirety of 
the composite key. Again, the solution to a normalization process is the creation of a
new table. You should remove the offending column to a new, distinct 'Courses' table:


            +----------+------------------+-----+
            | CourseId |      Title       | ... |
            +----------+------------------+-----+
            | SQL101   | SQL Fundamentals | ... |
            | DB202    | Database Design  | ... |
            | CS200    | C Programming    | ... |
            +----------+------------------+-----+

 
'CourseId' is the primary key for this table. This way, each course title is associated 
with a unique course ID. You can connect this table to 'Events' table with a One-to-Many
relationship, with 'Courses' being the 'One' side and 'Events' being the 'Many' side. In
the 'Events' table, 'CourseId' will act as a foreign key. This is what the 'Events' table 
will now look like: 


        +--------+-----------+------+----------+-----------+
        | Course |   Date    | Room | Capacity | Available |
        +--------+-----------+------+----------+-----------+
        | SQL101 | 3/1/2013  | 4A   |       12 |         4 |
        | DB202  | 3/1/2013  | 7B   |        7 |         7 |
        | SQL101 | 4/14/2013 | 7B   |       14 |        10 |
        | SQL101 | 5/28/2013 | 12A  |        8 |         8 |
        | CS200  | 4/15/2012 | 4A   |       12 |        11 |
        +--------+-----------+------+----------+-----------+
    
    
With the removal of 'CourseTitle', the last three columns are now fully functionally 
dependant on both the first and the second columns. This brings the table into compliance 
with 1NF and as well as the requirements peculiar to 2NF, making it fully compliant with 
Second Normal Form. 
       
    
5.4 THIRD NORMAL FORM (3NF)

These are the requirements of Third Normal Form:

    * The first requirement for Third Normal Form is that the table is in Second Normal
      Form. As 2NF requires compliance with 1NF, effecting 3NF first requires compliance
      with both First and Second Normal Form. Full compliance with 1NF and 2NF is a 
      mandatory pre-requisite for full compliance with 3NF.
      
    * 3NF is concerned with the relationship between Non-key attributes. Branko 
      Dimitrijevic explains that Third Normal Form tries to enforce this general 
      principle[8]: "All attributes should depend on the key, whole key and nothing but 
      the key." Transitive Dependencies are a kind of "bad" dependency that flouts this
      principle. Oppel defines this term on page 203[1]: "An attribute that depends on 
      another attribute that is not the primary key of the relation is said to be 
      transitively dependent." As a Transitive Dependency involves a non-prime attribute 
      determining another non-prime attribute, such dependencies violate Dimitrijevic's 
      principle. For this reason, 3NF calls for the elimination of all Transitive 
      Dependencies in your tables. 
      
      This means that functional dependencies between non-prime attributes are prohibited. 
      3NF accepts functional dependencies only between the primary key and a non-prime 
      attribute. This is the order of things according to Third Normal Form: Every table 
      ought to have a primary key which on all the other attributes are functionally 
      dependant. There should be no dependencies between non-prime columns. The presence 
      of a transitive dependency means that the dependant non-prime column does not 
      belong in that table.  
      

Let's go back to the example from Second Normal Form. Here is the 'Events' table[7]:


            +--------+-----------+------+----------+-----------+
            | Course |   Date    | Room | Capacity | Available |
            +--------+-----------+------+----------+-----------+
            | SQL101 | 3/1/2013  | 4A   |       12 |         4 |
            | DB202  | 3/1/2013  | 7B   |        7 |         7 |
            | SQL101 | 4/14/2013 | 7B   |       14 |        10 |
            | SQL101 | 5/28/2013 | 12A  |        8 |         8 |
            | CS200  | 4/15/2012 | 4A   |       12 |        11 |
            +--------+-----------+------+----------+-----------+


This table complies with 1NF and 2NF, but not 3NF. Why? See if you can spot the issue.
The problem is the relationship between 'Room' and 'Capacity'. 'Room' determines the 
value of 'Capacity', but it is not a prime attribute. This is a violation of 3NF because
a non-prime column is determining another non-prime attribute. You can see one of the 
problems immediately: the 'Room' column has duplicates. These columns just do not belong
in this table, so let's create a new 'Room' table:


                +------+----------+
                | Room | Capacity |
                +------+----------+
                | 4A   |       12 |
                | 7B   |       14 |
                | 12A  |        8 |
                +------+----------+                 
                

You can connect it to the 'Events' table and with a One-to-Many relationship and so on. 
This is the new 'Events' table: 


            +--------+-----------+-----------+
            | Course |   Date    | Available |
            +--------+-----------+-----------+
            | SQL101 | 3/1/2013  |         4 |
            | DB202  | 3/1/2013  |         7 |
            | SQL101 | 4/14/2013 |        10 |
            | SQL101 | 5/28/2013 |         8 |
            | CS200  | 4/15/2012 |        11 |
            +--------+-----------+-----------+


Note how it shed so many of its attributes as the Normal Forms were applied to it. Let's 
move on to an example that will highlight a common design mistake that leads to 
Transitive Dependencies. Look at the 'OrderItem' table below, which calculates parts of
an invoice[7]: 


        +-------+---------+-----------+----------+-----------+--------+
        |  ID   | OrderID | ProductID | Quantity | UnitPrice | Total  |
        +-------+---------+-----------+----------+-----------+--------+
        | 10011 |     454 | A43       |        4 |     10.00 |  40.00 |
        | 10012 |     455 | B56       |        2 |      5.00 |  10.00 |
        | 10013 |     456 | A22       |        1 |      9.99 |   9.99 |
        | 10014 |     456 | A43       |       55 |     10.00 | 550.00 |
        | 10015 |     457 | C77       |       10 |     75.00 | 750.00 |
        +-------+---------+-----------+----------+-----------+--------+
         

The problem lies with the last three columns. The 'Total' column depends on the 
'Quantity' and 'UnitPrice' columns because its values are a product of the other two 
columns. A non-key field is dependant on two non-key fields, once again violating 3NF. It 
is a very common mistake to put easily derivable information in a new column, but you 
should NOT do this.

If the values of one non-key can be ascertained from another field(s), it may lead to 
hard-to-resolve conflicts. If an order is updated and its quantity or unit price changed, 
the 'Total' column does not automatically get updated. If you fail to update the total, 
and notice the discrepancy at a later time, you face a serious data inconsistency problem: 
Which of the values is wrong? Is it the 'Quantity' value, 'UnitPrice' or the 'Total'? 
This is the reason that 3NF exists - to prevent your database from devolving into a 
chaotic mess of conflicting values.

You can resolve this problem by removing the transitively dependent column from the 
table. But if the information in the 'Total' column is valuable to you, can you keep it
somehow? Yes, you can. Many databases offer the option of defining a read-only 'Computed' 
or 'Calculated' column that is not really a part of the table. However, if you've defined
such a column, it will show up when you open the table. Its values cannot be changed 
directly as they are automatically calculated by the database from the two columns it 
depends on. In this way, you can both comply with 3NF and retain a computed column that 
cannot be corrupted by human error. 


5.5 BOYCE-CODD NORMAL FORM (BCNF or 3.5NF)

BCNF has two requirements:

    * The relation must be in Third Normal Form. As the Forms are cumulative, this means
      that 1NF, 2NF and 3NF must be applied to your table before you can apply BCNF. 
      
    * BCNF is an extension of Third Normal Form and was created to strengthen it. This is
      due to certain rare anomalies that 3NF does not prevent. BCNF closes these 
      loopholes. The techopedia article on BCNF explains the circumstances that led to the 
      development of this extension Form[9]: "3NF states that all data in a table must 
      depend only on that tables primary key, and not on any other field in the table. 
      At first glance it would seem that BCNF and 3NF are the same thing. However, in 
      some rare cases it does happen that a 3NF table is not BCNF-compliant. This may 
      happen in tables with two or more overlapping composite candidate keys." 
    
      Oppel provides more detail about what BCNF is designed to prevent on page 206[1]: 
      "It addresses anomalies that occur when a non-key attribute is a determinant of an 
      attribute that is part of the primary key (that is, when an attribute that is part 
      of the primary key is functionally dependent on a non-key attribute)." These are 
      anomalies that occur in tables with multiple candidate keys. 
  
      Oppel explains BCNF's prime criterion on page 206[1]: "No determinants exist that 
      are not either the primary key or a candidate key for the table. That is, a non-key 
      attribute may not uniquely identify (determine) any other attribute, including one 
      that participates in the primary key."  
      
      In the Second Normal Form, we have addressed cases where non-prime attributes 
      depend upon prime attributes. In the Third Normal Form, we have addressed situations 
      that arise when non-prime attributes depended upon other non-prime attributes. The 
      one possibility that the first three Normal Forms have not addressed is what happens 
      if a non-prime attribute determines a prime attribute? BCNF addresses this 
      situation. 
      
      BCNF demands that, for any dependency where A derives B (A -> B), A must be a 
      candidate key. Rephrased: BCNF states that if B is a prime attribute, then A cannot 
      be a non-prime attribute. 
      
      In other words, BCNF insists that every determinant in a table be a candidate key. 
      A candidate key is the most minimum set of attributes in a table that can be used 
      to uniquely identify a record. In an article[10], Agnieszka Kozubek puts it this 
      way: "Informally the Boyce-Codd normal form is expressed as Each attribute must 
      represent a fact about the key, the whole key, and nothing but the key." 

    
If ever there was a concept in need of an example, it is this one. BCNF is a bit more 
impenetrable than the first three Normal Forms, but it is still understandable. The 
example below is taken from MariaDb's Docs[11]. It depicts a 'Student Enrollment' table. 
You should make the following assumptions about the attributes in it:


    * Each instructor takes only one course
    * Each course can have one or more instructors
    * Each student only has one instructor per course
    * Each student can take one or more courses 


    +-----------------+----------------------+----------------+
    |     Student     |        Course        |   Instructor   |
    +-----------------+----------------------+----------------+
    | Julian Mives    | Geology 101          | Chris Chao     |
    | Pradeep Connect | Computer Science 203 | John Ike       |
    | Ahmed Kathra    | Philosophy 427       | Richard Mbappe |
    | Golan Goldberg  | Philosophy 427       | Richard Mbappe |
    +-----------------+----------------------+----------------+


For now, this table uses a composite primary key composed of two attributes: 'Student'
and 'Course'. The table complies with 1NF. It complies with 2NF because the 'Student' and 
'Course' attributes both determine the 'Instructor' column. Therefore, the 'Instructor' 
column is fully dependent on both attributes of the composite key. The table also easily
complies with 3NF's demand that no non-key attribute be dependent on any other non-key
attribute: there's only one non-key field, 'Instructor'! So what's the problem? 

Look at the third and fourth rows. There are two students taking Philosophy 427, which 
results in the instructor's name (Richard Mbappe) being stored twice. This is a data 
redundancy problem that the current design of our table fails to eliminate. This happened 
because 'Instructor' determines 'Course', which stated generically is: a non-prime 
attribute determines a prime attribute. In other words, 'Instructor' determines 'Course', 
but 'Instructor' is not a super key. Thus, this table fails to enforce BCNF. 

The solution is to move the 'Course' attribute to a separate table, along with its key.
This leaves only two attributes in the original table, as you can see below: 
 

    +-----------------+----------------+
    |     Student     |   Instructor   |
    +-----------------+----------------+
    | Julian Mives    | Chris Chao     |
    | Pradeep Connect | John Ike       |
    | Ahmed Kathra    | Richard Mbappe |
    | Golan Goldberg  | Richard Mbappe |
    +-----------------+----------------+


This new table is called the 'Student-Instructor' table. After removing the 'Course' 
attribute, you cannot effectively search for records with a single-attribute key. So you 
need to combine both 'Student' and 'Instructor' into a composite key for the table. This 
way, you will be able to uniquely identify records in this table.

The other table is the 'Instructor-Course' table. As the 'Instructor' attribute 
determined 'Course' in the original table, I am making it the primary key in this table.


    +----------------+----------------------+
    |   Instructor   |        Course        |
    +----------------+----------------------+
    | Chris Chao     | Geology 101          |
    | John Ike       | Computer Science 203 |
    | Richard Mbappe | Philosophy 427       |
    +----------------+----------------------+


As you can see, this eliminates the Instructor's name being stored redundantly. 
Decomposing a table into two separate tables has once again solved a normalization
problem.
 
 
5.6 DENORMALIZATION

Bringing your database into compliance with the first three Normal Forms is the typical
expectation in production environments. At this point, something needs to be made clear:
Normalization is a means to an end and that end is creating a database that is resistant
to data anomalies. However, sometimes applying the Normal Forms is either more trouble
than its worth or causes unwanted degradation of some other database metric. This is 
especially true with regards to database performance; Normalization often results in the 
creation of more tables, which usually diminishes a database's performance. You now face 
a trade-off: increase the database's data quality (by enforcing the Normal Forms) or 
optimize the database for performance, scalability or some other metric (by reversing 
Normalization in certain places). 

The act of re-designing a database's tables, trading away some data integrity, increasing 
data redundancy and potentially the incidence of anomalies, in return for something else 
is called DENORMALIZATION. Note that Denormalization does mean not doing Normalization; 
it is done after successfully normalizing your database, when the circumstances require 
you to make such a tradeoff.   

For example here is an example table[7]: 


    Employee(EmployeeId, FirstName, LastName, Email, Email2, Phone, Phone2 )
    
    
Technically, the presence of Repeating Groups means that this table violates 1NF. However,
if you know that you need just one additional column for Email and Phone each, it may 
make sense to break 1NF to add these columns. This makes things more convenient and may
increase performance, at minimum cost in data integrity. Reversing one of the Normal 
Form's protocols or deliberately choosing to override them for a good reason is called a
Denormalization Decision. In this example, this decision only makes sense because you 
have to add one column each. If you need to store many email addresses and phone numbers,
denormalizing this table would not be a good idea. 

There are also situations that are deceptive; some tables seem ripe for Normalization or
Denormalization, but really, are not. Simon Allardice warns that tables that contain 
address information are an example of this[7]. For example, you may think that you can 
determine the 'City' and 'State'/'Province' columns of a address from just the 'ZipCode'
or 'PostalCode' column. In fact, it looks like a Transitive Dependency. However, there 
are some problems with this line of thought. While you can usually determine the city or 
province from a postal code, this is not always so. Reality is not that straightforward:
there could be multiple towns in one postal code or postal codes that overlap two 
provinces.

The point of this example is to remind you that you need to really understand your data
before you make a decision to Normalize or Denormalize. Corner cases and nuances abound
in real world data, and you have to account for this when designing your tables. It's 
important to be informed when making structural changes to your database.     


****************************************************************************************************
****************************************************************************************************


SOURCES

01: https://en.wikipedia.org/wiki/Database_normalization
02: https://www.studytonight.com/dbms/database-normalization.php
03: https://mariadb.com/kb/en/library/database-normalization-overview/
04: https://en.wikipedia.org/wiki/Dependency_theory_(database_theory)
05: https://www.lifewire.com/database-dependencies-1019727
06: https://stackoverflow.com/questions/4199444/functional-dependency-and-normalization
07: Programming Foundations: Databases (Lynda.com, Simon Allardice, 2015)
08: https://stackoverflow.com/questions/9950367/what-is-wrong-with-a-transitive-dependency
09: https://www.techopedia.com/definition/5642/boyce-codd-normal-form-bcnf
10: https://www.vertabelo.com/blog/technical-articles/boyce-codd-normal-form-bcnf
11: https://mariadb.com/kb/en/library/database-normalization-boyce-codd-normal-form/